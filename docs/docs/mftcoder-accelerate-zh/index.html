<!DOCTYPE html>
<html lang="en-CN">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="介绍主要功能">
<meta name="author" content="codefuse-ai">

<title>MFTCoder: Accelerate &#43; DeepSpeed/FSDP 框架篇 · CodeFuse-AI</title>

<link rel="canonical" href="/docs/mftcoder-accelerate-zh/">









<link rel="stylesheet" href="/scss/base.css" integrity="">



<link rel="stylesheet" href="/scss/theme/default.css" integrity="">



<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/img/icon/favicon.ico">
<link rel="icon" href="/img/icon/icon-16.png" sizes="16x16" type="image/png">
<link rel="icon" href="/img/icon/icon-32.png" sizes="32x32" type="image/png">
<link rel="apple-touch-icon" href="/img/icon/icon-180.png" sizes="180x180">
<meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-xxxx"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-xxxx');
</script>


  
  

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.3/css/all.css">
  </head>
  <body>
    <header id="site-header">
  
  <div id="site-header-brand">
    <a href="/">CodeFuse-AI</a>
  </div>

  
  <div id="site-header-controls">
    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="theme selector">
        <i class="icon icon-brightness"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        <li role="menuitem"><button class="color-scheme" data-value="light"><i class="icon icon-light-mode"></i> Light</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="dark"><i class="icon icon-dark-mode"></i> Dark &nbsp;</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="night"><i class="icon icon-night-mode"></i> Night</button></li>
      </ul>
    </div>

    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="language selector">
        <i class="icon icon-translate"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        
          
          <li role="menuitem"><a href="/docs/mftcoder-accelerate/">English</a></li>
          
          <li role="menuitem"><a href="/docs/mftcoder-accelerate-zh/">中文</a></li>
          
        
      </ul>
    </div>
  </div>

  
  <div id="site-header-menu">
    <nav>
      <ul>
        
        
        
          
          <li><a href="/" ><i class='icon icon-home'></i>Home</a></li>
        
          
          <li><a href="/docs"  class="active"><i class='icon icon-book'></i>Overview</a></li>
        
          
          <li><a href="/muagent" ><i class='icon icon-book'></i>MuAgent</a></li>
        
          
          <li><a href="/contribution" ><i class='icon icon-book'></i>Contribution</a></li>
        
      </ul>
    </nav>
  </div>

  
  <div id="site-header-search"></div>
</header>
    
<div id="site-main-content-wrapper">
  
  
    <aside id="sidebar">
  <span class="btn-close"><i class="icon icon-close"></i></span>

  <div class="sticky"><strong class="sidebar-section">📖 CodeFuse-AI 整体介绍</strong>
          
          <a class="sidebar-link " href="/docs/%E6%A6%82%E8%A7%88/">
            
              概览
            
          </a>

        <strong class="sidebar-section">📖 CodeFuse-AI 模块</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-query-zh/">
            
              CodeFuse-Query
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-zh/">
            
              MFTCoder
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-mft-vlm-zh/">
            
              CodeFuse-MFT-VLM
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/test-agent-zh/">
            
              Test-Agent
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-zh/">
            
              CodeFuse-ModelCache
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-chatbot-zh/">
            
              CodeFuse-ChatBot
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-eval-zh/">
            
              CodeFuse-DevOps-Eval
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-zh/">
            
              CodeFuse-DevOps-Model
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-evalution-zh/">
            
              CodeFuse-Evalution
            
          </a>

        <strong class="sidebar-section">CodeFuse-Query</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-query-introduction-zh/">
            
              基本介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-godellanguage-zh/">
            
              查询语言介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-toolchain-zh/">
            
              VSCode插件
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-usercase-zh/">
            
              用户案例
            
          </a>

        <strong class="sidebar-section">MFTCoder</strong>
          
          <a class="sidebar-link " href="/docs/mftcoder-introduction-zh/">
            
              基本介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-quickstart-zh/">
            
              快速使用
            
          </a>

        
          
          <a class="sidebar-link current" href="/docs/mftcoder-accelerate-zh/">
            
              Accelerate &#43; DeepSpeed/FSDP 框架篇
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-atorch-zh/">
            
              Atorch框架篇
            
          </a>

        <strong class="sidebar-section">CodeFuse-MFT-VLM</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-mft-vlm-quickstart-zh/">
            
              快速使用
            
          </a>

        <strong class="sidebar-section">🌱 Test Agent</strong>
          
          <a class="sidebar-link " href="/docs/test-agent-quickstart-zh/">
            
              快速开始
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-ModelCache</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-feature-zh/">
            
              功能特性
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-config-zh/">
            
              最佳配置
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-release-zh/">
            
              版本记录
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-ChatBot</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-chatbot-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/">
            
              启动明细
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/">
            
              本地私有化&amp;大模型接口接入
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/">
            
              ChatBot 技术路线
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-DevOps-Model</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-train-zh/">
            
              训练解析
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-quickstart-zh/">
            
              快速使用
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-DevOps-Eval</strong>
          
          <a class="sidebar-link " href="/docs/%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D/">
            
              数据介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-eval-quickstart-zh/">
            
              快速开始
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-evalution</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-evalution-quickstart-zh/">
            
              快速开始
            
          </a>

        
  </div>
</aside>
  
  <main>
    <article id="article">
      <nav id="article-nav">
  <button id="article-nav-menu-btn"><i class="icon icon-menu"></i> On this section</button>
  
</nav>
      <header id="article-header">
  <h1>MFTCoder: Accelerate + DeepSpeed/FSDP 框架篇</h1>
</header>
      <div id="article-content">
        
        
        <p><a href="https://huggingface.co/codefuse-ai"><img src="https://img.shields.io/badge/%F0%9F%A4%97-Huggingface%20Repo-green.svg" alt="Generic badge" target="_blank"></a>
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE" target="_blank">
<img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a></p>
<p>[<strong>中文</strong>] <a href="/docs/mftcoder-accelerate">[English]</a></p>
<h2 id="1-更新">1. 更新</h2>
<p>🔥 MFTCoder-accelerate 新增支持accelerate + FSDP框架， 支持全量微调和LoRA;</p>
<p>🔥 MFTCoder-accelerate 支持最新更多主流开源模型: mistral, mixtral-8x7b(Mixture of Experts), deepseek, chatglm3；</p>
<p>🔥 MFTCoder-accelerate 新增self-paced Loss, 用于收敛均衡；</p>
<p>🔥 MFTCoder-accelerate 支持使用accelerate + DeepSpeed框架下支持 全量参数/QLoRA/LoRA微调；</p>
<p>🔥 MFTCoder-accelerate 在训练中支持了多任务微调MFT， 可以同时平衡多个任务的训练，训练的模型支持多任务推理；</p>
<p>🔥 MFTCoder-accelerate 在训练中支持多种模型基座： codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwen等</p>
<h2 id="2-数据格式">2. 数据格式</h2>
<h3 id="21-训练数据格式">2.1 训练数据格式</h3>
<p>训练数据为jsonl格式，每一行的数据格式如下，其中chat_rounds字段是必需的，可以根据实际需求添加或删除其他字段。
可以参考项目中的xxx.jsonl文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;data_name&#34;</span><span class="p">:</span><span class="s2">&#34;code-helper&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;chat_rounds&#34;</span><span class="p">:[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;你是一个智能代码助手，可以回复用户与代码相关的问题&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;human&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;写一个快速排序&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;bot&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;以下是一个快速排序算法xxxxxx&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;human&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;解释一下这段代码&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;bot&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;好的，这段代码xxx&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="22-推理数据格式">2.2 推理数据格式</h3>
<p>推理数据格式为模型在训练数据格式下拼接的字符串形式，它也是推理时输入prompt拼接的方式：</p>
<pre tabindex="0"><code>&#34;&#34;&#34;
&lt;s&gt;system
这是System指令
&lt;s&gt;human
这是第1轮用户输入的问题
&lt;s&gt;bot
这是第1轮模型生成的内容{EOS_TOKEN}
&lt;s&gt;human
这是第2轮用户输入的问题
&lt;s&gt;bot
这是第2轮模型生成的内容{EOS_TOKEN}
...
...
...
&lt;s&gt;human
这是第n轮用户输入的问题
&lt;s&gt;bot
{模型现在要生成的内容}{EOS_TOKEN}
&#34;&#34;&#34;
</code></pre><h2 id="3-模型训练">3. 模型训练</h2>
<p>目前支持全量参数(Full-parameters)指令微调、QLoRA指令微调，LoRA指令微调。
一些优秀的代码预训练模型权重，理论上，HuggingFace上开源的模型，均可使用本项目进行训练：</p>
<p>🤗 <a href="https://huggingface.co/codellama/CodeLlama-34b-Python-hf" target="_blank">最新代码预训练SOTA，CodeLlama</a> ：code-llama-34b， code-llama-34b-python, 新的SOTA基座。</p>
<p>🤗 <a href="https://huggingface.co/bigcode/starcoder" target="_blank">10B级别最佳代码预训练模型Starcoder</a> wizardCoder-15B, PanGu-coder2等前SOTA的基座模型。</p>
<p>🤗 <a href="https://huggingface.co/Qwen/Qwen-7B" target="_blank">多语言能手Qwen-7b</a> ：适用于多语言任务，也适用中文任务。进行指令微调时。</p>
<p><strong>mftcoder_accelerate文件结构</strong></p>
<pre tabindex="0"><code>mftcoder_accelerate
       |
       src
          configs
          |
          data
          |
          model
          |
          *pefts*
          |
          tokenizer
          |
          utils
       |
       evals
</code></pre><p>我们将训练中使用的各种组件抽取出来，以便后续的扩展和优化， 详见<code>src</code>目录下的实现。</p>
<p>训练入口文件是<code>mftcoder_accelerate/src/pefts/mft_accelerate.py</code></p>
<p>参数配置存储在<code>mftcoder_accelerate/src/configs</code>目录下，方便统一管理和更改。</p>
<p><strong><em>所以，在你开启训练之前，请进入src目录</em></strong></p>
<pre tabindex="0"><code>cd mftcoder_accelerate/src
</code></pre><h3 id="31-数据tokenization">3.1 数据tokenization</h3>
<p>训练时，我们将多轮对话拼接成如下格式（也是上文中的推理数据格式），然后进行tokenize。
其中，默认情况下：</p>
<p><code>&lt;s&gt;human\n</code>作为human/user的起始符，<code>&lt;s&gt;bot\n</code>作为bot/assistant的起始符，<code>{EOS_TOKEN}</code> 表示eos_token。
其中eos_token可以根据不同模型修改替换。不同角色的起始符可以配置，用来实现不同的对话/问答模版。</p>
<pre tabindex="0"><code>&#34;&lt;s&gt;human\n{input1}&lt;s&gt;bot\n{target1}{EOS_TOKEN}&lt;s&gt;human\n{input2}&lt;s&gt;bot\n{target2}{EOS_TOKEN}\n&#34;
</code></pre><p>在计算loss时，我们通过loss mask的方式，input部分的loss不参与参数更新，只有“target{EOS_TOKEN}”部分的loss参与参数更新。
这种方式充分利用了模型并行计算的优势，训练更加高效，同时也充分利用了decoder-only模型从左到右attention的特性，一次性将多轮对话中的每个target部分都参与了训练，训练更充分高效。</p>
<h3 id="32-loraqlora微调">3.2 LoRA/QLoRA微调</h3>
<h4 id="loraqlora微调简介">LoRA/QLoRA微调简介</h4>
<p>关于LoRA的详细介绍可参考论文：<a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></p>
<p>关于QLoRA的详细介绍可参考论文：<a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank">QLORA: Efficient Finetuning of Quantized LLMs</a></p>
<p>QLoRA通过4-bit的nf4量化，且加入更多adapter，在大幅减少显存消耗的同时，尽可能逼近全量参数微调的效果。
QLoRA论文指出，该方法可以在一张V100上对33B的模型进行微调，并且性能逼近全量参数微调。</p>
<p>执行如下命令即可进行 Lora/QLora/全量 微调：</p>
<h4 id="launch-via-deepspeed">Launch via Deepspeed</h4>
<p>DeepSpeed配置在accelerate_ds_config.yaml中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">accelerate launch --config_file accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type <span class="s2">&#34;DeepSpeed&#34;</span> 
</span></span></code></pre></div><p>或者</p>
<p>DeepSpeed配置在脚本中通过命令行输入。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sh ds_single_launch.sh
</span></span></code></pre></div><h4 id="launch-via-fsdp">Launch via FSDP</h4>
<p>FSDP配置在accelerate_fsdp_config.yaml中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">accelerate launch --config_file accelerate_fsdp_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type <span class="s2">&#34;FSDP&#34;</span>
</span></span></code></pre></div><p>或者</p>
<p>FSDP配置在脚本中通过命令行输入。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sh fsdp_single_launch.sh
</span></span></code></pre></div><h4 id="训练参数">训练参数</h4>
<p><em><strong>训练需要的参数配置在<code>configs/*_train_config</code>中，主要参数说明如下：</strong></em></p>
<ul>
<li><strong>load_raw_dataset</strong>: 需要保持true，后续会支持其它模式数据，当前仅支持jsonl输入</li>
<li><strong>data_paths</strong>: &ldquo;[path1,path2,path3]&rdquo; 输入数据地址，字符串，开头结尾用[]，中间用<code>,</code>间隔不同path，每个path是一个目录，目录的最后一级名字作为任务名称，下面包含1到多个jsonl数据</li>
<li><strong>output_dir</strong>：训练输出目录，存储checkpoint(全量训练时)、lora_adaptor（Lora或者Qlora时）等</li>
<li><strong>tb_dir</strong>: 存储tensorboard等</li>
<li><strong>model_type</strong>: &ldquo;mixtral|mistral|deepseek|llama|starcoder|chatglm2|qwen|gpt_neox&rdquo;</li>
<li><strong>attn_implementation</strong>: &ldquo;flash_attention_2&rdquo; 或者 &ldquo;eager&rdquo;</li>
<li><strong>peft_type</strong>: lora或者qlora或者null(全量微调)</li>
<li><strong>lora_rank</strong>: lora rank</li>
<li><strong>lora_alpha</strong>: lora alpha</li>
<li><strong>lora_dropout</strong>: lora dropout</li>
<li><strong>target_modules</strong>: List[str], lora目标模块，如果null，会使用默认，参考model_mapping.py</li>
<li><strong>quantization</strong>: 是否量化，&ldquo;4bit&rdquo;, &ldquo;8bit&rdquo; 或者null， qlora推荐4bit量化</li>
<li><strong>pretrained_model_path</strong>：预训练模型的本地目录，或者在huggingface上的模型名称。</li>
<li><strong>weighted_loss_mode</strong>: 多任务loss加权模式， &ldquo;case3&quot;是当前推荐。</li>
<li><strong>padding_mode</strong>: 数据的样本组织方式， &ldquo;padding&quot;是将每个原始样本填充到seq_length, &ldquo;pack&quot;是将尽量多的样本打包到每个seq_length的序列中。</li>
<li><strong>num_train_epochs</strong>：训练的轮次。如果数据量足够大，一般建议只训1-2个epoch。</li>
<li><strong>per_device_train_batch_size</strong>：每张显卡train的batch size。</li>
<li><strong>per_device_eval_batch_size</strong>：每张显卡eval的batch size。</li>
<li><strong>gradient_accumulation_steps</strong>：梯度累计步数。global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_steps。</li>
<li><strong>learning_rate</strong>：学习率。全量参数微调的时候，建议小一些，1e-5或5e-6。qlora中的学习率设置更大一些，一般为1e-4、2e-4。</li>
<li><strong>min_lr</strong>: 最低学习率， 一般是learning_rate的十分之一</li>
<li><strong>seq_length</strong>：训练时的最大长度。按照自己的设备进行设置，越长需要占用越多显存。</li>
<li><strong>log_interval</strong>：每隔多少步统计一次train loss。</li>
<li><strong>checkpointing_steps</strong>：每隔多少步保存一个模型。</li>
<li><strong>evaluation_steps</strong>：每隔多少步在验证集上evaluate一次。</li>
<li><strong>early_stopping</strong> ： 是否执行early_stop</li>
<li><strong>early_stopping_stall_num</strong>： 多少个eval point不继续收敛，则停止训练</li>
<li><strong>lr_scheduler_type</strong>：学习率变化策略。常用&quot;cosine&rdquo;</li>
<li><strong>warmup_steps</strong>：warm up步数。学习率经过多少步，增长到指定的数值。</li>
<li><strong>seed</strong>：随机种子，用于复现实验结果。</li>
<li><strong>saving_limit</strong>：整数，ckpt存储数量上限， 全量训练必须设置。默认null即不限制数量。</li>
<li><strong>role_markers</strong>: null，即使用{&ldquo;system&rdquo;: &ldquo;&lt;s&gt;system\n&rdquo;, &ldquo;user&rdquo;: &ldquo;&lt;s&gt;human\n&rdquo;, &ldquo;assistant&rdquo;: &ldquo;&lt;s&gt;bot\n&rdquo;}。 你可以自定义 &ldquo;system&rdquo;, &ldquo;user&rdquo; and &ldquo;assistant&quot;的模板， 用于定制自己的问答或者对话模板，比如 {&ldquo;system&rdquo;: &ldquo;### System:\n&rdquo;, &ldquo;user&rdquo;: &ldquo;### Instruction:\n&rdquo;, &ldquo;assistant&rdquo;: &ldquo;### Response:\n&rdquo;}</li>
</ul>
<h2 id="4-模型使用">4. 模型使用</h2>
<h3 id="41-权重合并">4.1 权重合并</h3>
<p>如果使用LoRA或者QLoRA进行训练，本项目仅保存adapter的权重和配置文件，需要将adapter权重与base model进行合并。
可以使用如下merge_base_and_lora_to_hf.py脚本。</p>
<pre tabindex="0"><code>python pefts/merge_base_and_lora_to_hf.py \
    --base_model_or_path model_path \
    --adaptor_path lora_adapter_path \
    --model_type model_type \
    --merged_output_path output_path
</code></pre><h3 id="42-模型推理">4.2 模型推理</h3>
<p>我们提供了单轮对话和多轮对话的如下脚本，该脚本可同时兼容大部分huggingface格式的模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">AutoTokenizer</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&#34;codefuse-ai/CodeFuse-Deepseek-33B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&#34;&lt;｜end▁of▁sentence｜&gt;&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">HUMAN_ROLE_START_TAG</span> <span class="o">=</span> <span class="s2">&#34;&lt;s&gt;human</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">BOT_ROLE_START_TAG</span> <span class="o">=</span> <span class="s2">&#34;&lt;s&gt;bot</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;write a python function of quick sort.&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">HUMAN_ROLE_START_TAG</span><span class="si">}{</span><span class="n">text</span><span class="si">}{</span><span class="n">BOT_ROLE_START_TAG</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;attention_mask&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gen_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[:,</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">gen_text</span><span class="p">)</span>
</span></span></code></pre></div><p>生成脚本中的top_p、temperature、repetition_penalty、do_sample等参数对模型的生成效果影响较大，可按照自己的使用场景进行调试修改。
实践中，在代码生成场景中，如果采样模式，do_sample=True, top_p=0.95, temperature=0.1是pass@1指标的不错选择；
如果非采样模式， do_sample=False, beam_num=1或者3是不错的选择，其中beam_num=1即为greedy decoding。</p>
<h2 id="5-faq">5. FAQ</h2>
<h4 id="问题1oom如何解决">问题1：OOM如何解决？</h4>
<p>如果发生OOM，可以缩小per_device_train_batch_size、seq_length等参数来缓解。由于面对的模型普遍较大（6b， 13b， 34b， 70b等）我们已经默认使用gradient_checkpointing技术，可以大幅降低显存占用，但训练速度会稍慢一些。</p>
<h4 id="问题2安装包错误">问题2：安装包错误</h4>
<p>参考init_env.sh和requirements.txt</p>
<h4 id="问题3如何指定使用某些卡训练">问题3：如何指定使用某些卡训练？</h4>
<p>通过如下方式，即可指定使用0和1号卡进行训练:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0,1 accelerate launch --config_file pefts/accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type <span class="s2">&#34;deepspeed&#34;</span>
</span></span></code></pre></div><h4 id="问题4关于flash-attention-该如何配置训练">问题4：关于Flash Attention, 该如何配置训练？</h4>
<p>首先，我们强烈建议您安装Flash Attention 2(FA2)，（&gt;=2.1.0, 2.3.6功能更齐全）。</p>
<p>训练参数中&quot;attn_implementation&rdquo; 设置成 &ldquo;eager&rdquo; 可以用naive attention，也就是未经加速的attention。</p>
<p>训练参数中&quot;attn_implementation&rdquo; 设置成 &ldquo;flash_attention_2&rdquo; 可以用FA2，速度快，省显存。</p>
<p>如果你可以自行安装环境并使用torch&gt;=2.1.1，可以尝试设置参数&quot;attn_implementation&quot;为 &ldquo;sdpa&rdquo;。这样会尝试使用transformers兼容的torch.nn.functional.scaled_dot_product_attention。支持的模型还不全面。</p>
<h4 id="问题5推荐的分布式框架是怎样的">问题5：推荐的分布式框架是怎样的？</h4>
<p>对于LoRA/QLoRA, 我们推荐使用DeepSpeed作为底层分布式框架，它具有易用性和兼容性好的特点，并且速度很快。
FSDP 不支持QLoRA, 因为bitsandbytes暂不支持FSDP。</p>
<p>对于全量微调，我们推荐使用FSDP， 因为它在全量训练时可以发挥fully sharding的优势，达到更快的训练速度。</p>
<h4 id="问题6当前支持的模型中有什么区别">问题6：当前支持的模型中，有什么区别</h4>
<p>国产大模型比如chatglm2， chatglm3， baichuan2， qwen， aquila2等，使用的是和模型共同发布的modeling_xxx.py.
其它被transformers官方支持的大模型，由于已经升级支持flash attention等，所以全面切换到官方的modeling支持训练，之前的自定义modeling会被deprecated</p>

      </div>
      








  
  
  
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

      
    
        

        
          
        

        
        
      


<footer id="article-footer">
  <time id="article-last-updated" datetime="2024-04-09"><i class="icon icon-calendar"></i>&nbsp;Last updated: 2024-04-09</time>

  
    <a id="article-prev-link" href="/docs/mftcoder-quickstart-zh/"><i class="icon icon-prev icon-colored"></i> Prev</a>
  

  
    <a id="article-next-link"  href="/docs/mftcoder-atorch-zh/">Next <i class="icon icon-next icon-colored"></i></a>
  
</footer>
    </article>
    <aside id="toc">
  <span class="btn-close"><i class="icon icon-close"></i></span>
  <div class="sticky">
    <strong><i class="icon icon-toc"></i> On this page</strong>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-更新">1. 更新</a></li>
    <li><a href="#2-数据格式">2. 数据格式</a>
      <ul>
        <li><a href="#21-训练数据格式">2.1 训练数据格式</a></li>
        <li><a href="#22-推理数据格式">2.2 推理数据格式</a></li>
      </ul>
    </li>
    <li><a href="#3-模型训练">3. 模型训练</a>
      <ul>
        <li><a href="#31-数据tokenization">3.1 数据tokenization</a></li>
        <li><a href="#32-loraqlora微调">3.2 LoRA/QLoRA微调</a></li>
      </ul>
    </li>
    <li><a href="#4-模型使用">4. 模型使用</a>
      <ul>
        <li><a href="#41-权重合并">4.1 权重合并</a></li>
        <li><a href="#42-模型推理">4.2 模型推理</a></li>
      </ul>
    </li>
    <li><a href="#5-faq">5. FAQ</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</aside>
  </main>
</div>

    <footer id="site-footer">
  
  <div id="site-footer-copyright">
    <a href="https://github.com/codefuse-ai" target="_blank">
      <i class="icon icon-copyright"></i> 2023-2024 codefuse-ai
    </a>
  </div>

  
  <div id="site-footer-social">

    

    

    
    <a href="https://github.com/codefuse-ai" target="_blank" aria-label="github url">
      <i class="icon icon-github icon-colored"></i>
    </a>
    

    

  </div>

  
  <div id="site-footer-fund">

    

    

  </div>

  
  <div id="site-footer-love">
    Made with <i class="icon icon-love icon-colored"></i> &nbsp;from&nbsp;<a href="https://docura.github.io/" target="_blank">Docura</a>
  </div>
</footer>
    






<script type="text/javascript" src="/js/base.min.js"></script>




<script type="application/javascript">
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('/sw.js?2024-04-09')
            .catch(function(err) {console.error('ServiceWorker registration failed: ', err);});
    }
</script>
  </body>
</html>