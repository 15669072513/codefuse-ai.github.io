<!DOCTYPE html>
<html lang="en-CN">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="介绍主要功能">
<meta name="author" content="codefuse-ai">

<title>MFTCoder训练: Atorch框架篇 · CodeFuse-AI</title>

<link rel="canonical" href="/docs/mftcoder-atorch-zh/">









<link rel="stylesheet" href="/scss/base.css" integrity="">



<link rel="stylesheet" href="/scss/theme/default.css" integrity="">



<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/img/icon/favicon.ico">
<link rel="icon" href="/img/icon/icon-16.png" sizes="16x16" type="image/png">
<link rel="icon" href="/img/icon/icon-32.png" sizes="32x32" type="image/png">
<link rel="apple-touch-icon" href="/img/icon/icon-180.png" sizes="180x180">
<meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-xxxx"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-xxxx');
</script>


  
  

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.3/css/all.css">
  </head>
  <body>
    <header id="site-header">
  
  <div id="site-header-brand">
    <a href="/">CodeFuse-AI</a>
  </div>

  
  <div id="site-header-controls">
    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="theme selector">
        <i class="icon icon-brightness"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        <li role="menuitem"><button class="color-scheme" data-value="light"><i class="icon icon-light-mode"></i> Light</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="dark"><i class="icon icon-dark-mode"></i> Dark &nbsp;</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="night"><i class="icon icon-night-mode"></i> Night</button></li>
      </ul>
    </div>

    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="language selector">
        <i class="icon icon-translate"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        
          
          <li role="menuitem"><a href="/docs/mftcoder-atorch/">English</a></li>
          
          <li role="menuitem"><a href="/docs/mftcoder-atorch-zh/">中文</a></li>
          
        
      </ul>
    </div>
  </div>

  
  <div id="site-header-menu">
    <nav>
      <ul>
        
        
        
          
          <li><a href="/" ><i class='icon icon-home'></i>Home</a></li>
        
          
          <li><a href="/docs"  class="active"><i class='icon icon-book'></i>Overview</a></li>
        
          
          <li><a href="/muagent" ><i class='icon icon-book'></i>MuAgent</a></li>
        
          
          <li><a href="/contribution" ><i class='icon icon-book'></i>Contribution</a></li>
        
      </ul>
    </nav>
  </div>

  
  <div id="site-header-search"></div>
</header>
    
<div id="site-main-content-wrapper">
  
  
    <aside id="sidebar">
  <span class="btn-close"><i class="icon icon-close"></i></span>

  <div class="sticky"><strong class="sidebar-section">📖 CodeFuse-AI 整体介绍</strong>
          
          <a class="sidebar-link " href="/docs/%E6%A6%82%E8%A7%88/">
            
              概览
            
          </a>

        <strong class="sidebar-section">📖 CodeFuse-AI 模块</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-query-zh/">
            
              CodeFuse-Query
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-zh/">
            
              MFTCoder
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-mft-vlm-zh/">
            
              CodeFuse-MFT-VLM
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/test-agent-zh/">
            
              Test-Agent
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-zh/">
            
              CodeFuse-ModelCache
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-chatbot-zh/">
            
              CodeFuse-ChatBot
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-eval-zh/">
            
              CodeFuse-DevOps-Eval
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-zh/">
            
              CodeFuse-DevOps-Model
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-evalution-zh/">
            
              CodeFuse-Evalution
            
          </a>

        <strong class="sidebar-section">CodeFuse-Query</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-query-introduction-zh/">
            
              基本介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-godellanguage-zh/">
            
              查询语言介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-toolchain-zh/">
            
              VSCode插件
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query-usercase-zh/">
            
              用户案例
            
          </a>

        <strong class="sidebar-section">MFTCoder</strong>
          
          <a class="sidebar-link " href="/docs/mftcoder-introduction-zh/">
            
              基本介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-quickstart-zh/">
            
              快速使用
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder-accelerate-zh/">
            
              Accelerate &#43; DeepSpeed/FSDP 框架篇
            
          </a>

        
          
          <a class="sidebar-link current" href="/docs/mftcoder-atorch-zh/">
            
              Atorch框架篇
            
          </a>

        <strong class="sidebar-section">CodeFuse-MFT-VLM</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-mft-vlm-quickstart-zh/">
            
              快速使用
            
          </a>

        <strong class="sidebar-section">🌱 Test Agent</strong>
          
          <a class="sidebar-link " href="/docs/test-agent-quickstart-zh/">
            
              快速开始
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-ModelCache</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-feature-zh/">
            
              功能特性
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-config-zh/">
            
              最佳配置
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache-release-zh/">
            
              版本记录
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-ChatBot</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-chatbot-quickstart-zh/">
            
              快速开始
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/">
            
              启动明细
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/">
            
              本地私有化&amp;大模型接口接入
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/">
            
              ChatBot 技术路线
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-DevOps-Model</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-train-zh/">
            
              训练解析
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model-quickstart-zh/">
            
              快速使用
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-DevOps-Eval</strong>
          
          <a class="sidebar-link " href="/docs/%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D/">
            
              数据介绍
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-eval-quickstart-zh/">
            
              快速开始
            
          </a>

        <strong class="sidebar-section">🌱 CodeFuse-evalution</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-evalution-quickstart-zh/">
            
              快速开始
            
          </a>

        
  </div>
</aside>
  
  <main>
    <article id="article">
      <nav id="article-nav">
  <button id="article-nav-menu-btn"><i class="icon icon-menu"></i> On this section</button>
  
</nav>
      <header id="article-header">
  <h1>MFTCoder训练: Atorch框架篇</h1>
</header>
      <div id="article-content">
        
        
        <p><a href="https://huggingface.co/codefuse-ai"><img src="https://img.shields.io/badge/%F0%9F%A4%97-Huggingface%20Repo-green.svg" alt="Generic badge" target="_blank"></a>
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE" target="_blank">
<img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a></p>
<p>[<strong>中文</strong>] <a href="/docs/mftcoder-atorch">[English]</a></p>
<h2 id="1-更新">1. 更新</h2>
<p>🔥 MFTCoder在Atorch框架下支持GPTNeoX模型的微调；</p>
<p>🔥 MFTCoder支持全量的有监督微调；</p>
<p>🔥 MFTCoder支持LoRA微调；</p>
<h2 id="2-数据格式">2. 数据格式</h2>
<h3 id="21-训练数据格式">2.1 训练数据格式</h3>
<p>训练数据为jsonl格式，每一行的数据格式如下，其中chat_rounds字段是必需的，可以根据实际需求添加或删除其他字段。
可以参考项目中的xxx.jsonl文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;data_name&#34;</span><span class="p">:</span><span class="s2">&#34;code-helper&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;chat_rounds&#34;</span><span class="p">:[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;你是一个智能代码助手，可以回复用户与代码相关的问题&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;chat_round_id&#34;</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;human&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;写一个快速排序&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;chat_round_id&#34;</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;bot&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;以下是一个快速排序算法xxxxxx&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;chat_round_id&#34;</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;human&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;解释一下这段代码&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;chat_round_id&#34;</span><span class="p">:</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;bot&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;好的，这段代码xxx&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;chat_round_id&#34;</span><span class="p">:</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h3 id="22-推理数据格式">2.2 推理数据格式</h3>
<p>推理数据格式为模型在训练数据格式下拼接的字符串形式，它也是推理时输入prompt拼接的方式：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;system&lt;|role_end|&gt;这是System指令
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;human&lt;|role_end|&gt;这是第1轮用户输入的问题
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;bot&lt;|role_end|&gt;这是第1轮模型生成的内容&lt;/s&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;human&lt;|role_end|&gt;这是第2轮用户输入的问题
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;bot&lt;|role_end|&gt;这是第2轮模型生成的内容&lt;/s&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">...
</span></span></span><span class="line"><span class="cl"><span class="s2">...
</span></span></span><span class="line"><span class="cl"><span class="s2">...
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;human&lt;|role_end|&gt;这是第n轮用户输入的问题
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;|role_start|&gt;bot&lt;|role_end|&gt;</span><span class="si">{模型现在要生成的内容}</span><span class="s2">&lt;/s&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span></code></pre></div><h2 id="3-模型训练">3. 模型训练</h2>
<p>目前 &ldquo;MFTCoder/mft_atorch&rdquo; 代码库支持全量参数指令微调和LoRA指令微调。
目前仅支持GPTNeoX模型的训练，理论上，HuggingFace上开源的GPTNeoX模型权重，均可使用本项目进行训练。</p>
<p>我们将训练中使用的各种组件抽取出来，以便后续的扩展和优化，详见主目录下的实现。微调训练的入口目录是<code>train/</code>, 训练入口文件是<code>train/run_train.py</code>, 参数配置存储在启动脚本<code>train/run_gpt_*.sh</code>等文件中，方便统一管理和更改。</p>
<h3 id="31-数据格式">3.1 数据格式</h3>
<p>训练时，我们将多轮对话拼接成如下格式，然后进行tokenize。其中&lt;|role_start|&gt;human&lt;|role_end|&gt;表示human输入提示符，&lt;|role_start|&gt;bot&lt;|role_end|&gt;表示bot输出提示符，<code>&lt;/s&gt;</code> 表示eos_token。</p>
<pre tabindex="0"><code>&#34;&lt;|role_start|&gt;human&lt;|role_end|&gt;input1&lt;/s&gt;target1&lt;/s&gt;input2&lt;/s&gt;target2&lt;/s&gt;...
</code></pre><p>在计算loss时，我们通过mask的方式，input部分的loss不参与参数更新，只有“target</s>”部分的loss参与参数更新。
这种方式充分利用了模型并行计算的优势，训练更加高效，且多轮对话中的每个target部分都参与了训练，训练更充分。
否则，就需要把一个n轮对话，拆分成n条数据，且只计算最后一个target的loss，大大降低了训练效率。</p>
<h3 id="32-全量sft">3.2 全量SFT</h3>
<p>执行如下命令即可进行全量SFT：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sh run_gpt_mft.sh <span class="m">10</span> <span class="m">1</span> <span class="m">8</span> <span class="m">5</span>
</span></span></code></pre></div><p>需注意，启动脚本后的四个参数，分别是：</p>
<ul>
<li>第一个参数是总的per gpu batch size</li>
<li>第二个参数是tensor parallel数（暂时只支持1）</li>
<li>第三个参数是data parallel数，与所用GPU数保持一致</li>
<li>第四个参数是训练epoch数</li>
</ul>
<p>后面其他的训练方式启动脚本，也同样需要配置这四个参数</p>
<h3 id="33-lora微调">3.3 LoRA微调</h3>
<p>执行如下命令即可进行Lora微调：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sh run_gpt_mft_peft.sh <span class="m">10</span> <span class="m">1</span> <span class="m">8</span> <span class="m">5</span>
</span></span></code></pre></div><h3 id="34-启动脚本中主要参数说明">3.4 启动脚本中主要参数说明</h3>
<p><code>train/run_gpt_*.sh</code>中的主要参数说明如下，以下参数可以根据需求进行修改，其他参数建议不做修改：</p>
<ul>
<li>
<p>tokenize_mode: 目前仅支持&quot;sft&quot;。</p>
</li>
<li>
<p>train_mode: 目前仅支持&quot;sft&quot;。</p>
</li>
<li>
<p>load_raw_dataset: 需要保持&quot;True&quot;，后续会支持其它模式数据，当前仅支持jsonl输入</p>
</li>
<li>
<p>data_paths: &ldquo;[path1,path2,path3]&rdquo; 输入数据地址，字符串，开头结尾用[]，中间用<code>,</code>间隔不同path，每个path是一个目录，目录的最后一级名字作为任务名称，下面包含1到多个jsonl数据。</p>
</li>
<li>
<p>output_dir: 训练输出目录，存储checkpoint、lora_adaptor checkpoint等。</p>
</li>
<li>
<p>tensorboard_dir: 可以暂时忽略，实际tensorboard存储在output_dir的runs目录下。</p>
</li>
<li>
<p>model_type: 目前仅支持 gpt_neox。</p>
</li>
<li>
<p>peft_type: 目前仅支持 lora。</p>
</li>
<li>
<p>pretrained_model_path: 预训练模型的本地目录。</p>
</li>
<li>
<p>total_train_batch_size: 所有显卡train的batch size的总和，会根据启动脚本时输入的per gpu batch size自动计算。</p>
</li>
<li>
<p>per_device_valid_batch_size: 每张显卡eval的batch size，会根据启动脚本时输入的per gpu batch size自动计算。</p>
</li>
<li>
<p>gradient_accumulation_steps: 梯度累计步数。global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_steps。</p>
</li>
<li>
<p>checkpoint_activations: 如果显存捉襟见肘，可以开启。以时间换空间，模型不缓存激活状态，会进行两次forward计算，以节省显存。</p>
</li>
<li>
<p>learning_rate: 学习率。全量参数微调的时候，建议小一些，1e-5或5e-6。qlora中的学习率设置更大一些，一般为1e-4、2e-4。</p>
</li>
<li>
<p>min_lr: 最低学习率， 一般是learning_rate的十分之一。</p>
</li>
<li>
<p>seq_length: 训练时的最大长度。按照自己的设备进行设置，越长需要占用越多显存。</p>
</li>
<li>
<p>log_interval: 每隔多少步统计一次train loss。</p>
</li>
<li>
<p>checkpointing_steps: 每隔多少步保存一个模型。</p>
</li>
<li>
<p>evalation_steps: 每隔多少步在验证集上evaluate一次。</p>
</li>
<li>
<p>early_stopping_patience: 多少个eval point不继续收敛，则停止训练。</p>
</li>
<li>
<p>lr_scheduler_type: 学习率变化策略。</p>
</li>
<li>
<p>num_warmup_steps: warm up步数，学习率经过多少步，增长到指定的数值。</p>
</li>
<li>
<p>seed: 随机种子，用于复现实验结果。</p>
</li>
<li>
<p>train_iters: 可以暂时设为比较小的数，如10，实际上不会影响训练步数，留作后面拓展读取其他形式数据集的功能。</p>
</li>
<li>
<p>valid_iters: 可以暂时设为比较小的数，如10，实际上不会影响训练步数，留作后面拓展读取其他形式数据集的功能。</p>
</li>
<li>
<p>evaluation_strategy: 训练期间evaluate的策略，&ldquo;steps&quot;表示每隔&quot;valid_interval&quot;步做一次evaluate，&ldquo;epoch&quot;表示每隔一个epoch做一次evaluate，支持同时开启。</p>
</li>
<li>
<p>save_strategy: 训练期间保存模型权重的策略，&ldquo;steps&quot;表示每隔&quot;checkpointing_steps&quot;步保存一次。</p>
</li>
<li>
<p>extra_save_by_epoch: 每过一个epoch是否要保存一个epoch级别的checkpoint。</p>
</li>
<li>
<p>save_total_limit: 最多保留的模型checkpoint个数，一般设置为2，会保留valid loss最低，以及最新的checkpoint，注意epoch级别的checkpoint会一直保留，且不受限制。</p>
</li>
<li>
<p>weighted_loss_mode: 多任务训练的loss加权方式。</p>
</li>
</ul>
<h2 id="4-模型使用">4. 模型使用</h2>
<h3 id="41-权重合并">4.1 权重合并</h3>
<p>如果使用LoRA进行训练，本项目仅保存adapter的权重和配置文件，需要将adapter权重与base model进行合并。脚本见<code>utils/merge_base_and_lora_to_hf.py</code></p>
<h3 id="42-模型推理">4.2 模型推理</h3>
<p>我们提供了单轮对话和多轮对话的如下脚本，该脚本可同时兼容大部分huggingface格式的模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">AutoTokenizer</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mode_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">legacy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&#34;left&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&#34;&lt;unk&gt;&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s2">&#34;&lt;/s&gt;&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">mode_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">HUMAN_ROLE_START_TAG</span> <span class="o">=</span> <span class="s2">&#34;&lt;|role_start|&gt;human&lt;|role_end|&gt;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">BOT_ROLE_START_TAG</span> <span class="o">=</span> <span class="s2">&#34;&lt;|role_start|&gt;bot&lt;|role_end|&gt;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;write a python function of quick sort.&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">HUMAN_ROLE_START_TAG</span><span class="si">}{</span><span class="n">text</span><span class="si">}{</span><span class="n">BOT_ROLE_START_TAG</span><span class="si">}</span><span class="s2">&#34;</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;attention_mask&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gen_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[:,</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">gen_text</span><span class="p">)</span>
</span></span></code></pre></div><p>生成脚本中的top_p、temperature、repetition_penalty、do_sample等参数对模型的生成效果影响较大，可按照自己的使用场景进行调试修改。
实践中，在代码生成场景中，如果采样模式，do_sample=True, top_p=0.95, temperature=0.1是pass@1指标的不错选择；
如果非采样模式， do_sample=False, beam_num=1或者3是不错的选择，其中beam_num=1即为greedy decoding。</p>
<h2 id="5-faq">5. FAQ</h2>
<h4 id="问题1oom如何解决">问题1：OOM如何解决？</h4>
<p>如果发生OOM，可以缩小per GPU batch size (启动训练脚本时的第一个参数)、seq_length等参数来缓解。也可以设gradient_checkpointing=true，可以大幅降低显存占用，但训练速度会变慢一些。</p>

      </div>
      








  
  
  
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

      
    
        

        
          
        

        
        
      


<footer id="article-footer">
  <time id="article-last-updated" datetime="2024-04-09"><i class="icon icon-calendar"></i>&nbsp;Last updated: 2024-04-09</time>

  
    <a id="article-prev-link" href="/docs/mftcoder-accelerate-zh/"><i class="icon icon-prev icon-colored"></i> Prev</a>
  

  
    <a id="article-next-link"  href="/docs/codefuse-mft-vlm-quickstart-zh/">Next <i class="icon icon-next icon-colored"></i></a>
  
</footer>
    </article>
    <aside id="toc">
  <span class="btn-close"><i class="icon icon-close"></i></span>
  <div class="sticky">
    <strong><i class="icon icon-toc"></i> On this page</strong>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-更新">1. 更新</a></li>
    <li><a href="#2-数据格式">2. 数据格式</a>
      <ul>
        <li><a href="#21-训练数据格式">2.1 训练数据格式</a></li>
        <li><a href="#22-推理数据格式">2.2 推理数据格式</a></li>
      </ul>
    </li>
    <li><a href="#3-模型训练">3. 模型训练</a>
      <ul>
        <li><a href="#31-数据格式">3.1 数据格式</a></li>
        <li><a href="#32-全量sft">3.2 全量SFT</a></li>
        <li><a href="#33-lora微调">3.3 LoRA微调</a></li>
        <li><a href="#34-启动脚本中主要参数说明">3.4 启动脚本中主要参数说明</a></li>
      </ul>
    </li>
    <li><a href="#4-模型使用">4. 模型使用</a>
      <ul>
        <li><a href="#41-权重合并">4.1 权重合并</a></li>
        <li><a href="#42-模型推理">4.2 模型推理</a></li>
      </ul>
    </li>
    <li><a href="#5-faq">5. FAQ</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</aside>
  </main>
</div>

    <footer id="site-footer">
  
  <div id="site-footer-copyright">
    <a href="https://github.com/codefuse-ai" target="_blank">
      <i class="icon icon-copyright"></i> 2023-2024 codefuse-ai
    </a>
  </div>

  
  <div id="site-footer-social">

    

    

    
    <a href="https://github.com/codefuse-ai" target="_blank" aria-label="github url">
      <i class="icon icon-github icon-colored"></i>
    </a>
    

    

  </div>

  
  <div id="site-footer-fund">

    

    

  </div>

  
  <div id="site-footer-love">
    Made with <i class="icon icon-love icon-colored"></i> &nbsp;from&nbsp;<a href="https://docura.github.io/" target="_blank">Docura</a>
  </div>
</footer>
    






<script type="text/javascript" src="/js/base.min.js"></script>




<script type="application/javascript">
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('/sw.js?2024-04-09')
            .catch(function(err) {console.error('ServiceWorker registration failed: ', err);});
    }
</script>
  </body>
</html>