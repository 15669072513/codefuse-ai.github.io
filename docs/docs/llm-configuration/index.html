<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="CodeFuse aims to develop Code Large Language Models (Code LLMs) to support and enhance full-lifecycle AI native sotware developing, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.">
<meta name="author" content="codefuse-ai">

<title>LLM-Configuration ¬∑ CodeFuse-AI</title>

<link rel="canonical" href="/docs/LLM-Configuration/">









<link rel="stylesheet" href="/scss/base.css" integrity="">



<link rel="stylesheet" href="/scss/theme/default.css" integrity="">



  <link rel="preconnect" href="https://-dsn.algolia.net" crossorigin />
  
  <link rel="stylesheet" href="/scss/component/docsearch.css" integrity=""/>

<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/img/icon/favicon.ico">
<link rel="icon" href="/img/icon/icon-16.png" sizes="16x16" type="image/png">
<link rel="icon" href="/img/icon/icon-32.png" sizes="32x32" type="image/png">
<link rel="apple-touch-icon" href="/img/icon/icon-180.png" sizes="180x180">
<meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-xxxx"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-xxxx');
</script>


  
  

  </head>
  <body>
    <header id="site-header">
  
  <div id="site-header-brand">
    <a href="/">CodeFuse-AI</a>
  </div>

  
  <div id="site-header-controls">
    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="theme selector">
        <i class="icon icon-brightness"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        <li role="menuitem"><button class="color-scheme" data-value="light"><i class="icon icon-light-mode"></i> Light</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="dark"><i class="icon icon-dark-mode"></i> Dark &nbsp;</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="night"><i class="icon icon-night-mode"></i> Night</button></li>
      </ul>
    </div>

    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="language selector">
        <i class="icon icon-translate"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        
          
          <li role="menuitem"><a href="/docs/LLM-Configuration/">English</a></li>
          
          <li role="menuitem"><a href="/zh/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/">‰∏≠Êñá</a></li>
          
        
      </ul>
    </div>
  </div>

  
  <div id="site-header-menu">
    <nav>
      <ul>
        
        
        
          
          <li><a href="/" ><i class='icon icon-home'></i>Home</a></li>
        
          
          <li><a href="/docs"  class="active"><i class='icon icon-book'></i>Documentation</a></li>
        
          
          <li><a href="/contribution" ><i class='icon icon-book'></i>Contribution</a></li>
        
      </ul>
    </nav>
  </div>

  
  <div id="site-header-search"></div>
</header>
    
<div id="site-main-content-wrapper">
  
  
    <aside id="sidebar">
  <span class="btn-close"><i class="icon icon-close"></i></span>

  <div class="sticky"><strong class="sidebar-section">üìñ CodeFuse-AI Overview</strong>
          
          <a class="sidebar-link " href="/docs/overview/">
            
              overview
            
          </a>

        <strong class="sidebar-section">üìñ CodeFuse-AI Module</strong>
          
          <a class="sidebar-link " href="/docs/codefuse-chatbot/">
            
              CodeFuse-ChatBot
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-eval/">
            
              CodeFuse-DevOps-Eval
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-devops-model/">
            
              CodeFuse-DevOps-Model
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/mftcoder/">
            
              MFTCoder
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-modelcache/">
            
              CodeFuse-ModelCache
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/fastertransformer4codefuse/">
            
              FasterTransformer4CodeFuse
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/test-agent/">
            
              Test-Agent
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/codefuse-query/">
            
              CodeFuse-Query
            
          </a>

        <strong class="sidebar-section">üå± CodeFuse-ChatBot</strong>
          
          <a class="sidebar-link " href="/docs/quickstart/">
            
              QuickStart
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/start-detail/">
            
              Start-Detail
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/llm-configuration/">
            
              LLM-Configuration
            
          </a>

        
          
          <a class="sidebar-link " href="/docs/chatbot-roadmap/">
            
              ChatBot-RoadMap
            
          </a>

        
  </div>
</aside>
  
  <main>
    <article id="article">
      <nav id="article-nav">
  <button id="article-nav-menu-btn"><i class="icon icon-menu"></i> On this section</button>
  
</nav>
      <header id="article-header">
  <h1>LLM-Configuration</h1>
</header>
      <div id="article-content">
        
        
        <p align="left">
    <a href="/docs/fastchat-zh">‰∏≠Êñá</a>&nbsp ÔΩú &nbsp<a>English&nbsp </a>
</p>
<h1 id="local-privatizationlarge-model-interface-access">Local Privatization/Large Model Interface Access</h1>
<p>Leveraging open-source LLMs (Large Language Models) and Embedding models, this project enables offline private deployment based on open-source models.</p>
<p>In addition, the project supports the invocation of OpenAI API.</p>
<h2 id="local-privatization-model-access">Local Privatization Model Access</h2>
<p><br>Example of model address configuration, modification of the model_config.py configuration:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Recommendation: Use Hugging Face models, preferably the chat models, and avoid using base models, which may not produce correct outputs.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Note: When both `llm_model_dict` and `VLLM_MODEL_DICT` are present, the model configuration in `VLLM_MODEL_DICT` takes precedence.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Example of `llm_model_dict` configuration:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 1. If the model is placed under the ~/codefuse-chatbot/llm_models path</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Suppose the model address is as follows</span>
</span></span><span class="line"><span class="cl">model_dir: ~/codefuse-chatbot/llm_models/THUDM/chatglm-6b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The reference configuration is as follows</span>
</span></span><span class="line"><span class="cl"><span class="nv">llm_model_dict</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;chatglm-6b&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;local_model_path&#34;</span>: <span class="s2">&#34;THUDM/chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_base_url&#34;</span>: <span class="s2">&#34;http://localhost:8888/v1&#34;</span>,  <span class="c1"># &#34;name&#34;‰øÆÊîπ‰∏∫fastchatÊúçÂä°‰∏≠ÁöÑ&#34;api_base_url&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_key&#34;</span>: <span class="s2">&#34;EMPTY&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">VLLM_MODEL_DICT</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl"> <span class="s1">&#39;chatglm2-6b&#39;</span>:  <span class="s2">&#34;THUDM/chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># or If the model address is as follows</span>
</span></span><span class="line"><span class="cl">model_dir: ~/codefuse-chatbot/llm_models/chatglm-6b
</span></span><span class="line"><span class="cl"><span class="nv">llm_model_dict</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;chatglm-6b&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;local_model_path&#34;</span>: <span class="s2">&#34;chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_base_url&#34;</span>: <span class="s2">&#34;http://localhost:8888/v1&#34;</span>,  <span class="c1"># &#34;name&#34;‰øÆÊîπ‰∏∫fastchatÊúçÂä°‰∏≠ÁöÑ&#34;api_base_url&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_key&#34;</span>: <span class="s2">&#34;EMPTY&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">VLLM_MODEL_DICT</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl"> <span class="s1">&#39;chatglm2-6b&#39;</span>:  <span class="s2">&#34;chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. If you do not wish to move the model to ~/codefuse-chatbot/llm_models</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Also, delete the related code below `Model Path Reset`, see model_config.py for details.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Suppose the model address is as follows</span>
</span></span><span class="line"><span class="cl">model_dir: ~/THUDM/chatglm-6b
</span></span><span class="line"><span class="cl"><span class="c1"># The reference configuration is as follows</span>
</span></span><span class="line"><span class="cl"><span class="nv">llm_model_dict</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;chatglm-6b&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;local_model_path&#34;</span>: <span class="s2">&#34;your personl dir/THUDM/chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_base_url&#34;</span>: <span class="s2">&#34;http://localhost:8888/v1&#34;</span>,  <span class="c1"># &#34;name&#34;‰øÆÊîπ‰∏∫fastchatÊúçÂä°‰∏≠ÁöÑ&#34;api_base_url&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_key&#34;</span>: <span class="s2">&#34;EMPTY&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">VLLM_MODEL_DICT</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl"> <span class="s1">&#39;chatglm2-6b&#39;</span>:  <span class="s2">&#34;your personl dir/THUDM/chatglm-6b&#34;</span>,
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 3. Specify the model service to be launched, keeping both consistent</span>
</span></span><span class="line"><span class="cl"><span class="nv">LLM_MODEL</span> <span class="o">=</span> <span class="s2">&#34;chatglm-6b&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">LLM_MODELs</span> <span class="o">=</span> <span class="o">[</span><span class="s2">&#34;chatglm-6b&#34;</span><span class="o">]</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Modification of server_config.py configuration, if LLM_MODELS does not have multiple model configurations, no additional settings are needed.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Modify the configuration of server_config.py#FSCHAT_MODEL_WORKERS</span>
</span></span><span class="line"><span class="cl"><span class="s2">&#34;model_name&#34;</span>: <span class="o">{</span><span class="s1">&#39;host&#39;</span>: DEFAULT_BIND_HOST, <span class="s1">&#39;port&#39;</span>: 20057<span class="o">}</span>
</span></span></code></pre></div><p><br>ÈáèÂåñÊ®°ÂûãÊé•ÂÖ•</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># If you need to support the codellama-34b-int4 model, you need to patch fastchat</span>
</span></span><span class="line"><span class="cl">cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
</span></span><span class="line"><span class="cl"><span class="c1"># If you need to support the qwen-72b-int4 model, you need to patch fastchat</span>
</span></span><span class="line"><span class="cl">cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quantization requires modification of the llm_api.py configuration</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Uncomment `kwargs[&#34;gptq_wbits&#34;] = 4` in examples/llm_api.py#559</span>
</span></span></code></pre></div><h2 id="public-large-model-interface-access">Public Large Model Interface Access</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Modification of model_config.py configuration</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ONLINE_LLM_MODEL</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Other interface development comes from the langchain-chatchat project, untested due to lack of relevant accounts.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Specify the model service to be launched, keeping both consistent</span>
</span></span><span class="line"><span class="cl"><span class="nv">LLM_MODEL</span> <span class="o">=</span> <span class="s2">&#34;gpt-3.5-turbo&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">LLM_MODELs</span> <span class="o">=</span> <span class="o">[</span><span class="s2">&#34;gpt-3.5-turbo&#34;</span><span class="o">]</span>
</span></span></code></pre></div><p>Â§ñÈÉ®Â§ßÊ®°ÂûãÊé•Âè£Êé•ÂÖ•Á§∫‰æã</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 1. Implement a new model access class</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Refer to ~/examples/model_workers/openai.py#ExampleWorker</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Implementing the do_chat function will enable the use of LLM capabilities</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">class XXWorker<span class="o">(</span>ApiModelWorker<span class="o">)</span>:
</span></span><span class="line"><span class="cl">    def __init__<span class="o">(</span>
</span></span><span class="line"><span class="cl">            self,
</span></span><span class="line"><span class="cl">            *,
</span></span><span class="line"><span class="cl">            controller_addr: <span class="nv">str</span> <span class="o">=</span> None,
</span></span><span class="line"><span class="cl">            worker_addr: <span class="nv">str</span> <span class="o">=</span> None,
</span></span><span class="line"><span class="cl">            model_names: List<span class="o">[</span>str<span class="o">]</span> <span class="o">=</span> <span class="o">[</span><span class="s2">&#34;gpt-3.5-turbo&#34;</span><span class="o">]</span>,
</span></span><span class="line"><span class="cl">            version: <span class="nv">str</span> <span class="o">=</span> <span class="s2">&#34;gpt-3.5&#34;</span>,
</span></span><span class="line"><span class="cl">            **kwargs,
</span></span><span class="line"><span class="cl">    <span class="o">)</span>:
</span></span><span class="line"><span class="cl">        kwargs.update<span class="o">(</span><span class="nv">model_names</span><span class="o">=</span>model_names, <span class="nv">controller_addr</span><span class="o">=</span>controller_addr, <span class="nv">worker_addr</span><span class="o">=</span>worker_addr<span class="o">)</span>
</span></span><span class="line"><span class="cl">        kwargs.setdefault<span class="o">(</span><span class="s2">&#34;context_len&#34;</span>, 16384<span class="o">)</span> <span class="c1">#TODO 16KÊ®°ÂûãÈúÄË¶ÅÊîπÊàê16384</span>
</span></span><span class="line"><span class="cl">        super<span class="o">()</span>.__init__<span class="o">(</span>**kwargs<span class="o">)</span>
</span></span><span class="line"><span class="cl">        self.version <span class="o">=</span> version
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    def do_chat<span class="o">(</span>self, params: ApiChatParams<span class="o">)</span> -&gt; Dict:
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        ÊâßË°åChatÁöÑÊñπÊ≥ïÔºåÈªòËÆ§‰ΩøÁî®Ê®°ÂùóÈáåÈù¢ÁöÑchatÂáΩÊï∞„ÄÇ
</span></span></span><span class="line"><span class="cl"><span class="s1">        :params.messages : [
</span></span></span><span class="line"><span class="cl"><span class="s1">            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;hello&#34;}, 
</span></span></span><span class="line"><span class="cl"><span class="s1">            {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;hello&#34;}
</span></span></span><span class="line"><span class="cl"><span class="s1">            ]
</span></span></span><span class="line"><span class="cl"><span class="s1">        :params.xx: ËØ¶ÊÉÖËßÅ ApiChatParams 
</span></span></span><span class="line"><span class="cl"><span class="s1">        Ë¶ÅÊ±ÇËøîÂõûÂΩ¢ÂºèÔºö{&#34;error_code&#34;: int, &#34;text&#34;: str}
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">{</span><span class="s2">&#34;error_code&#34;</span>: 500, <span class="s2">&#34;text&#34;</span>: f<span class="s2">&#34;{self.model_names[0]}Êú™ÂÆûÁé∞chatÂäüËÉΩ&#34;</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Finally, complete the registration in ~/examples/model_workers/__init__.py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># from .xx import XXWorker</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. Complete access through an existing model access class</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Or directly use the existing relevant large model class for use (lacking relevant account testing, community contributions after testing are welcome)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Modification of model_config.py#ONLINE_LLM_MODEL configuration</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Enter exclusive model details: version, api_base_url, api_key, provider (consistent with the class name above)</span>
</span></span><span class="line"><span class="cl"><span class="nv">ONLINE_LLM_MODEL</span> <span class="o">=</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Online models. Please set different ports for each online API in server_config.</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;openai-api&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;model_name&#34;</span>: <span class="s2">&#34;gpt-3.5-turbo&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_base_url&#34;</span>: <span class="s2">&#34;https://api.openai.com/v1&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_key&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;openai_proxy&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="o">}</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;example&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;version&#34;</span>: <span class="s2">&#34;gpt-3.5&#34;</span>,  <span class="c1"># Using openai interface as an example</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_base_url&#34;</span>: <span class="s2">&#34;https://api.openai.com/v1&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;api_key&#34;</span>: <span class="s2">&#34;&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;provider&#34;</span>: <span class="s2">&#34;ExampleWorker&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="o">}</span>,
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="launching-large-model-services">Launching Large Model Services</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># start llm-service (optional)  - Launch the large model service separately</span>
</span></span><span class="line"><span class="cl">python examples/llm_api.py
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Test</span>
</span></span><span class="line"><span class="cl">import openai
</span></span><span class="line"><span class="cl"><span class="c1"># openai.api_key = &#34;EMPTY&#34; # Not support yet</span>
</span></span><span class="line"><span class="cl">openai.api_base <span class="o">=</span> <span class="s2">&#34;http://127.0.0.1:8888/v1&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Select the model you launched</span>
</span></span><span class="line"><span class="cl"><span class="nv">model</span> <span class="o">=</span> <span class="s2">&#34;example&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># create a chat completion</span>
</span></span><span class="line"><span class="cl"><span class="nv">completion</span> <span class="o">=</span> openai.ChatCompletion.create<span class="o">(</span>
</span></span><span class="line"><span class="cl">    <span class="nv">model</span><span class="o">=</span>model,
</span></span><span class="line"><span class="cl">    <span class="nv">messages</span><span class="o">=[{</span><span class="s2">&#34;role&#34;</span>: <span class="s2">&#34;user&#34;</span>, <span class="s2">&#34;content&#34;</span>: <span class="s2">&#34;Hello! What is your name? &#34;</span><span class="o">}]</span>,
</span></span><span class="line"><span class="cl">    <span class="nv">max_tokens</span><span class="o">=</span>100,
</span></span><span class="line"><span class="cl"><span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># print the completion</span>
</span></span><span class="line"><span class="cl">print<span class="o">(</span>completion.choices<span class="o">[</span>0<span class="o">]</span>.message.content<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Once the correct output is confirmed, LLM can be accessed normally.</span>
</span></span></code></pre></div><p>or</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># model_config.py#USE_FASTCHAT - Determine whether to integrate local models via fastchat</span>
</span></span><span class="line"><span class="cl"><span class="nv">USE_FASTCHAT</span> <span class="o">=</span> <span class="s2">&#34;gpt&#34;</span> not in LLM_MODEL
</span></span><span class="line"><span class="cl">python start.py <span class="c1">#221 Automatically executes python llm_api.py</span>
</span></span></code></pre></div>
      </div>
      








  
  
  
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    


<footer id="article-footer">
  <time id="article-last-updated" datetime="2024-01-23"><i class="icon icon-calendar"></i>&nbsp;Last updated: 2024-01-23</time>

  
    <a id="article-prev-link" href="/docs/chatbot-roadmap/"><i class="icon icon-prev icon-colored"></i> Prev</a>
  

  
</footer>
    </article>
    <aside id="toc">
  <span class="btn-close"><i class="icon icon-close"></i></span>
  <div class="sticky">
    <strong><i class="icon icon-toc"></i> On this page</strong>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#local-privatization-model-access">Local Privatization Model Access</a></li>
    <li><a href="#public-large-model-interface-access">Public Large Model Interface Access</a></li>
    <li><a href="#launching-large-model-services">Launching Large Model Services</a></li>
  </ul>
</nav>
  </div>
</aside>
  </main>
</div>

    <footer id="site-footer">
  
  <div id="site-footer-copyright">
    <a href="https://github.com/codefuse-ai" target="_blank">
      <i class="icon icon-copyright"></i> 2023-2024 codefuse-ai
    </a>
  </div>

  
  <div id="site-footer-social">

    

    

    
    <a href="https://github.com/codefuse-ai" target="_blank" aria-label="github url">
      <i class="icon icon-github icon-colored"></i>
    </a>
    

    

  </div>

  
  <div id="site-footer-fund">

    

    

  </div>

  
  <div id="site-footer-love">
    Made with <i class="icon icon-love icon-colored"></i> &nbsp;from&nbsp;<a href="https://docura.github.io/" target="_blank">Docura</a>
  </div>
</footer>
    






<script type="text/javascript" src="/js/base.min.js"></script>



  
  <script src="/js/component/docsearch.min.js"></script>
  <script type="application/javascript">
      docsearch({
          container: '#site-header-search',
          appId: '',
          indexName: '',
          apiKey: '',
      });
  </script>


<script type="application/javascript">
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('/sw.js?2024-01-23')
            .catch(function(err) {console.error('ServiceWorker registration failed: ', err);});
    }
</script>
  </body>
</html>