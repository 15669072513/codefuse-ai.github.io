<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="CodeFuse aims to develop Code Large Language Models (Code LLMs) to support and enhance full-lifecycle AI native sotware developing, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.">
<meta name="author" content="codefuse-ai">

<title>Quick Start · CodeFuse-AI</title>

<link rel="canonical" href="/coagent/quick-start/">









<link rel="stylesheet" href="/scss/base.css" integrity="">



<link rel="stylesheet" href="/scss/theme/default.css" integrity="">



  <link rel="preconnect" href="https://-dsn.algolia.net" crossorigin />
  
  <link rel="stylesheet" href="/scss/component/docsearch.css" integrity=""/>

<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/img/icon/favicon.ico">
<link rel="icon" href="/img/icon/icon-16.png" sizes="16x16" type="image/png">
<link rel="icon" href="/img/icon/icon-32.png" sizes="32x32" type="image/png">
<link rel="apple-touch-icon" href="/img/icon/icon-180.png" sizes="180x180">
<meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#121212" media="(prefers-color-scheme: dark)">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-xxxx"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-xxxx');
</script>


  
  

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.3/css/all.css">
  </head>
  <body>
    <header id="site-header">
  
  <div id="site-header-brand">
    <a href="/">CodeFuse-AI</a>
  </div>

  
  <div id="site-header-controls">
    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="theme selector">
        <i class="icon icon-brightness"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        <li role="menuitem"><button class="color-scheme" data-value="light"><i class="icon icon-light-mode"></i> Light</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="dark"><i class="icon icon-dark-mode"></i> Dark &nbsp;</button></li>
        <li role="menuitem"><button class="color-scheme" data-value="night"><i class="icon icon-night-mode"></i> Night</button></li>
      </ul>
    </div>

    
    <div class="dropdown">
      <button class="dropdown-btn" aria-haspopup="menu" aria-label="language selector">
        <i class="icon icon-translate"></i>
        <i class="icon icon-select"></i>
      </button>
      <ul role="menu" class="dropdown-menu">
        
          
          <li role="menuitem"><a href="/coagent/quick-start/">English</a></li>
          
          <li role="menuitem"><a href="/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/">中文</a></li>
          
        
      </ul>
    </div>
  </div>

  
  <div id="site-header-menu">
    <nav>
      <ul>
        
        
        
          
          <li><a href="/" ><i class='icon icon-home'></i>Home</a></li>
        
          
          <li><a href="/docs" ><i class='icon icon-book'></i>Overview</a></li>
        
          
          <li><a href="/muagent" ><i class='icon icon-book'></i>MuAgent</a></li>
        
          
          <li><a href="/contribution" ><i class='icon icon-book'></i>Contribution</a></li>
        
      </ul>
    </nav>
  </div>

  
  <div id="site-header-search"></div>
</header>
    
<div id="site-main-content-wrapper">
  
  
    <aside id="sidebar">
  <span class="btn-close"><i class="icon icon-close"></i></span>

  <div class="sticky"><strong class="sidebar-section">❤️ CoAgent</strong>
          
          <a class="sidebar-link " href="/coagent/coagent-overview/">
            
              CoAgent Overview
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/agent-flow/">
            
              Agent Flow
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/prompt-manager/">
            
              Prompt Manager
            
          </a>

        
          
          <a class="sidebar-link current" href="/coagent/quick-start/">
            
              Quick Start
            
          </a>

        <strong class="sidebar-section">Modules</strong>
          
          <a class="sidebar-link " href="/coagent/connector-agent/">
            
              Connector-Agent
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/connector-chain/">
            
              Connector-Chain
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/connector-phase/">
            
              Connector-Phase
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/connector-prompt/">
            
              Connector-Prompt
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/connector-memory/">
            
              Connector-Memory
            
          </a>

        
          
          <a class="sidebar-link " href="/coagent/customed-examples/">
            
              Customed Examples
            
          </a>

        
  </div>
</aside>
  
  <main>
    <article id="article">
      <nav id="article-nav">
  <button id="article-nav-menu-btn"><i class="icon icon-menu"></i> On this section</button>
  
</nav>
      <header id="article-header">
  <h1>Quick Start</h1>
</header>
      <div id="article-content">
        
        
        <h2 id="quick-start">Quick Start</h2>
<h3 id="first-set-up-the-llm-configuration">First, set up the LLM configuration</h3>
<pre tabindex="0"><code>import os, sys
import openai

# llm config
os.environ[&#34;API_BASE_URL&#34;] = OPENAI_API_BASE
os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;sk-xxx&#34;
openai.api_key = &#34;sk-xxx&#34;
# os.environ[&#34;OPENAI_PROXY&#34;] = &#34;socks5h://127.0.0.1:13659&#34;
</code></pre><h3 id="next-configure-the-llm-settings-and-vector-model">Next, configure the LLM settings and vector model</h3>
<pre tabindex="0"><code>from coagent.llm_models.llm_config import EmbedConfig, LLMConfig

llm_config = LLMConfig(
    model_name=&#34;gpt-3.5-turbo&#34;, model_device=&#34;cpu&#34;,api_key=os.environ[&#34;OPENAI_API_KEY&#34;], 
    api_base_url=os.environ[&#34;API_BASE_URL&#34;], temperature=0.3
    )

embed_config = EmbedConfig(
    embed_engine=&#34;model&#34;, embed_model=&#34;text2vec-base-chinese&#34;, 
    embed_model_path=&#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&#34;
    )
</code></pre><h3 id="finally-choose-a-pre-existing-scenario-to-execute">Finally, choose a pre-existing scenario to execute</h3>
<pre tabindex="0"><code>from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS
from coagent.connector.phase import BasePhase
from coagent.connector.schema import Message

# Copy the data to a working directory; specify the directory if needed (default can also be used)
import shutil
source_file = &#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&#39;
shutil.copy(source_file, JUPYTER_WORK_PATH)

# Choose a scenario to execute
phase_name = &#34;baseGroupPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, 
)

# round-1: Use a code interpreter to complete tasks
query_content = &#34;Check if &#39;employee_data.csv&#39; exists locally, view its columns and data types; then draw a bar chart&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, tools=[],
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

# phase.pre_print(query)  # This function is used to preview the Prompt of the Agents&#39; execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))

# round-2: Execute tools
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

query_content = &#34;Please check if there were any issues with the server at 127.0.0.1 at 10 o&#39;clock; help me make a judgment&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, tools=tools,
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

# phase.pre_print(query)  # This function is used to preview the Prompt of the Agents&#39; execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h2 id="phase-introduction-and-usage">Phase Introduction and Usage</h2>
<p>Below are some specific Phase introduced and how to use them.</p>
<p>Feel free to brainstorm and create some interesting cases.</p>
<h3 id="basegroupphase">baseGroupPhase</h3>
<p>The group usage Phase in autogen</p>
<pre tabindex="0"><code># Copy the data to a working directory; specify the directory if needed (default can also be used)
import shutil
source_file = &#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&#39;
shutil.copy(source_file, JUPYTER_WORK_PATH)

# Set the log level to control the printing of the prompt, LLM output, or other information
os.environ[&#34;log_verbose&#34;] = &#34;0&#34;

phase_name = &#34;baseGroupPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, 
)

# round-1
query_content = &#34;Check if &#39;employee_data.csv&#39; exists locally, view its columns and data types; then draw a bar chart&#34;

query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, tools=[],
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

# phase.pre_print(query) # This function is used to preview the Prompt of the Agents&#39; execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="basetaskphase">baseTaskPhase</h3>
<p>The task splitting and multi-step execution scenario in xAgents</p>
<pre tabindex="0"><code># if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = &#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&#39;
shutil.copy(source_file, JUPYTER_WORK_PATH)

# log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

phase_name = &#34;baseTaskPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)
# round-1
query_content = &#34;Check if &#39;employee_data.csv&#39; exists locally, view its columns and data types; then draw a bar chart&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;,
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

output_message, output_memory = phase.step(query)

print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="codereactphase">codeReactPhase</h3>
<p>The code interpreter scenario based on React</p>
<pre tabindex="0"><code># if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = &#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&#39;
shutil.copy(source_file, JUPYTER_WORK_PATH)

# then, create a data analyze phase
phase_name = &#34;codeReactPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, 
    jupyter_work_path=JUPYTER_WORK_PATH,
)

# round-1
query_content = &#34;Check if &#39;employee_data.csv&#39; exists locally, view its columns and data types; then draw a bar chart&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;,
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="codetoolreactphase">codeToolReactPhase</h3>
<p>The tool invocation and code interpreter scenario based on the React template</p>
<pre tabindex="0"><code>TOOL_SETS = [
     &#34;StockName&#34;, &#34;StockInfo&#34;, 
    ]
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

# log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

phase_name = &#34;codeToolReactPhase&#34;

phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, 
)

query_content = &#34;查询贵州茅台的股票代码，并查询截止到当前日期(2023年12月24日)的最近10天的每日时序数据，然后用代码画出折线图并分析&#34;

query = Message(
  role_name=&#34;human&#34;, role_type=&#34;user&#34;, 
  input_query=query_content, role_content=query_content, 
  origin_query=query_content, tools=tools
  )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="docchatphase">docChatPhase</h3>
<p>The knowledge base retrieval Q&amp;A Phase</p>
<pre tabindex="0"><code># create your knowledge base
from io import BytesIO
from pathlib import Path

from coagent.service.kb_api import create_kb, upload_doc
from coagent.service.service_factory import get_kb_details
from coagent.utils.server_utils import run_async
kb_list = {x[&#34;kb_name&#34;]: x for x in get_kb_details(KB_ROOT_PATH)}


# create a knowledge base
kb_name = &#34;example_test&#34;
data = {
    &#34;knowledge_base_name&#34;: kb_name,
    &#34;vector_store_type&#34;: &#34;faiss&#34;, # default
    &#34;kb_root_path&#34;: KB_ROOT_PATH, 
    &#34;embed_model&#34;: embed_config.embed_model,
    &#34;embed_engine&#34;: embed_config.embed_engine, 
    &#34;embed_model_path&#34;: embed_config.embed_model_path,
    &#34;model_device&#34;: embed_config.model_device,
}
run_async(create_kb(**data))

# add doc to knowledge base
file = os.path.join(&#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/sources/docs/langchain_text_10.jsonl&#34;)
files = [file]
# if embedding init failed, you can use override = True
data = [{&#34;override&#34;: True, &#34;file&#34;: f, 
         &#34;knowledge_base_name&#34;: kb_name, &#34;not_refresh_vs_cache&#34;: False,
         &#34;kb_root_path&#34;: KB_ROOT_PATH, &#34;embed_model&#34;: embed_config.embed_model,
         &#34;embed_engine&#34;: embed_config.embed_engine, &#34;embed_model_path&#34;: embed_config.embed_model_path,
         &#34;model_device&#34;: embed_config.model_device,
         } 
         for f in files]

for k in data:
    file = Path(file).absolute().open(&#34;rb&#34;)
    filename = file.name

    from fastapi import UploadFile
    from tempfile import SpooledTemporaryFile

    temp_file = SpooledTemporaryFile(max_size=10 * 1024 * 1024)
    temp_file.write(file.read())
    temp_file.seek(0)
    
    k.update({&#34;file&#34;: UploadFile(file=temp_file, filename=filename),})
    run_async(upload_doc(**k))


# start to chat with knowledge base
# log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

# set chat phase
phase_name = &#34;docChatPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)
# round-1
query_content = &#34;what modules does langchain have?&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, 
    origin_query=query_content,
    doc_engine_name=kb_name, score_threshold=1.0, top_k=3
    )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))

# round-2
query_content = &#34;What is the purpose of prompts?&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;,
    origin_query=query_content,
    doc_engine_name=kb_name, score_threshold=1.0, top_k=3
    )
output_message, output_memory = phase.step(query)

print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="metagpt_code_devlop">metagpt_code_devlop</h3>
<p>The code construction Phase in metagpt</p>
<pre tabindex="0"><code># log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

phase_name = &#34;metagpt_code_devlop&#34;
llm_config = LLMConfig(
    model_name=&#34;gpt-4&#34;, model_device=&#34;cpu&#34;,api_key=os.environ[&#34;OPENAI_API_KEY&#34;], 
    api_base_url=os.environ[&#34;API_BASE_URL&#34;], temperature=0.3
    )
embed_config = EmbedConfig(
    embed_engine=&#34;model&#34;, embed_model=&#34;text2vec-base-chinese&#34;, 
    embed_model_path=&#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&#34;
    )

phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)

query_content = &#34;create a snake game by pygame&#34;
query = Message(role_name=&#34;human&#34;, role_type=&#34;user&#34;, input_query=query_content, role_content=query_content, origin_query=query_content)

output_message, output_memory = phase.step(query)

print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="searchchatphase">searchChatPhase</h3>
<p>The fixed Phase: search first, then answer directly with LLM</p>
<pre tabindex="0"><code># log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

phase_name = &#34;searchChatPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, 
)

# round-1
query_content1 = &#34;who is the president of the United States?&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, 
    role_content=query_content1, input_query=query_content1, origin_query=query_content1,
    search_engine_name=&#34;duckduckgo&#34;, score_threshold=1.0, top_k=3
    )

output_message, output_memory = phase.step(query)

print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))

# round-2
query_content2 = &#34;Who was the previous president of the United States, and is there any relationship between the two individuals?&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, 
    role_content=query_content2, input_query=query_content2, origin_query=query_content2,
    search_engine_name=&#34;duckduckgo&#34;, score_threshold=1.0, top_k=3
    )
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre><h3 id="toolreactphase">toolReactPhase</h3>
<p>The tool invocation scene based on the React template</p>
<pre tabindex="0"><code># log-level，print prompt和llm predict
os.environ[&#34;log_verbose&#34;] = &#34;2&#34;

phase_name = &#34;toolReactPhase&#34;
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)

# round-1
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])
query_content = &#34;Please check if there were any issues with the server at 127.0.0.1 at 10 o&#39;clock; help me make a judgment&#34;
query = Message(
    role_name=&#34;human&#34;, role_type=&#34;user&#34;, tools=tools,
    role_content=query_content, input_query=query_content, origin_query=query_content
    )

# phase.pre_print(query)  # This function is used to preview the Prompt of the Agents&#39; execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key=&#34;parsed_output_list&#34;))
</code></pre>
      </div>
      








  
  
  
        

        
          
        

      
    
        

        
          
        

      
    
        

        
          
        

      
    
        

      
    
        

        
          
        

        
        


<footer id="article-footer">
  <time id="article-last-updated" datetime="2024-01-24"><i class="icon icon-calendar"></i>&nbsp;Last updated: 2024-01-24</time>

  
    <a id="article-prev-link" href="/coagent/prompt-manager/"><i class="icon icon-prev icon-colored"></i> Prev</a>
  

  
    <a id="article-next-link"  href="/coagent/connector-agent/">Next <i class="icon icon-next icon-colored"></i></a>
  
</footer>
    </article>
    <aside id="toc">
  <span class="btn-close"><i class="icon icon-close"></i></span>
  <div class="sticky">
    <strong><i class="icon icon-toc"></i> On this page</strong>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#quick-start">Quick Start</a>
      <ul>
        <li><a href="#first-set-up-the-llm-configuration">First, set up the LLM configuration</a></li>
        <li><a href="#next-configure-the-llm-settings-and-vector-model">Next, configure the LLM settings and vector model</a></li>
        <li><a href="#finally-choose-a-pre-existing-scenario-to-execute">Finally, choose a pre-existing scenario to execute</a></li>
      </ul>
    </li>
    <li><a href="#phase-introduction-and-usage">Phase Introduction and Usage</a>
      <ul>
        <li><a href="#basegroupphase">baseGroupPhase</a></li>
        <li><a href="#basetaskphase">baseTaskPhase</a></li>
        <li><a href="#codereactphase">codeReactPhase</a></li>
        <li><a href="#codetoolreactphase">codeToolReactPhase</a></li>
        <li><a href="#docchatphase">docChatPhase</a></li>
        <li><a href="#metagpt_code_devlop">metagpt_code_devlop</a></li>
        <li><a href="#searchchatphase">searchChatPhase</a></li>
        <li><a href="#toolreactphase">toolReactPhase</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</aside>
  </main>
</div>

    <footer id="site-footer">
  
  <div id="site-footer-copyright">
    <a href="https://github.com/codefuse-ai" target="_blank">
      <i class="icon icon-copyright"></i> 2023-2024 codefuse-ai
    </a>
  </div>

  
  <div id="site-footer-social">

    

    

    
    <a href="https://github.com/codefuse-ai" target="_blank" aria-label="github url">
      <i class="icon icon-github icon-colored"></i>
    </a>
    

    

  </div>

  
  <div id="site-footer-fund">

    

    

  </div>

  
  <div id="site-footer-love">
    Made with <i class="icon icon-love icon-colored"></i> &nbsp;from&nbsp;<a href="https://docura.github.io/" target="_blank">Docura</a>
  </div>
</footer>
    






<script type="text/javascript" src="/js/base.min.js"></script>



  
  <script src="/js/component/docsearch.min.js"></script>
  <script type="application/javascript">
      docsearch({
          container: '#site-header-search',
          appId: '',
          indexName: '',
          apiKey: '',
      });
  </script>


<script type="application/javascript">
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('/sw.js?2024-04-09')
            .catch(function(err) {console.error('ServiceWorker registration failed: ', err);});
    }
</script>
  </body>
</html>