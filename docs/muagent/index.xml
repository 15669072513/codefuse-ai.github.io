<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Muagents on CodeFuse-AI</title>
    <link>/muagent/</link>
    <description>Recent content in Muagents on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <atom:link href="/muagent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent Flow</title>
      <link>/muagent/agent-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/agent-flow/</guid>
      <description>Introduction to Core Connectors To facilitate everyone&amp;rsquo;s understanding of the entire muagent link, we adopt the Flow format to introduce in detail how to build through configuration&#xA;Below, we first introduce the related core components&#xA;Agent On the design level of the Agent, we provide four basic types of Agents, with Role settings for these Agents that can meet the interactions and uses of various common scenarios:&#xA;BaseAgent: Provides basic question answering, tool usage, and code execution functions, and realizes input =&amp;gt; output according to the Prompt format.</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/muagent/connector-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-agent/</guid>
      <description>Quickly Build an Agent First, add an OpenAI configuration, or a model with a similar interface to OpenAI (launched through fastchat) import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/muagent/connector-chain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-chain/</guid>
      <description>Quickly Build an Agent First, add an OpenAI configuration, or a model with a similar interface to OpenAI (launched through fastchat) import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model&#xA;from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/muagent/connector-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-memory/</guid>
      <description>Memory Manager Primarily used for managing chat history, not yet completed&#xA;Read and write chat history in the database, including user input, llm output, doc retrieval, code retrieval, search retrieval. Summarize key information from the chat history into a summary context, serving as a prompt context. Provide a search function to retrieve information related to the question from chat history or summary context, aiding in Q&amp;amp;A. Usage Example Create memory manager instance import os&#xD;import openai&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/muagent/connector-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-phase/</guid>
      <description>Quickly Build an Agent Phase First, add OpenAI configuration, which can be models with similar interfaces to OpenAI (triggered via fastchat). import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model. from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/muagent/connector-prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-prompt/</guid>
      <description>Prompt Manager Managing prompt creation in multi-agent linkages&#xA;Quick Configuration: Utilizing preset processing functions, users can easily configure by simply defining the inputs and outputs of the agents, enabling fast assembly and configuration of multi-agent prompts. Customization Support: Allows users to customize the internal processing logic of each module within the prompt to achieve personalized implementation of the agent prompt. Preset Template Structure for Prompts Agent Profile: This section involves the basic description of the agent, including but not limited to the type of agent, its functions, and command set.</description>
    </item>
    <item>
      <title>Custom Retrieval</title>
      <link>/muagent/custom-retrieval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-retrieval/</guid>
      <description>Basic Introduction Doc Retrieval is the document vector database, which is the most mainstream method for knowledge base construction nowadays. It uses Text Embedding models to vectorize documents and stores them in a vector database. In the future, we will also support querying based on knowledge graph and automatically extracting entities and relationships through large models to explore various complex relationships in data.&#xA;Code Retrieval LLM faces challenges in tasks such as code generation, repair, and component understanding, including lagging code training data and the inability to perceive the dependency structure of code context.</description>
    </item>
    <item>
      <title>Custom Tool</title>
      <link>/muagent/custom-tool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-tool/</guid>
      <description>Introduction In MuAgent, it also supports the registration of Tools by Agents. By registering the BaseToolModel class with Python and writing&#xA;Tool_name Tool_description ToolInputArgs ToolOutputArgs run and other relevant properties and methods, the quick integration of tools can be achieved. It also supports the direct use of the langchain Tool interface. For example, functions like the aforementioned XXRetrieval can also be registered as a Tool, to be ultimately called by an LLM.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/muagent/custom-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-examples/</guid>
      <description>How to Create Your Personalized Agent Phase Scenario Below we will use a code repository to demonstrate the automatic generation of API documentation from code, detailing how to customize the construction of an agent phase.&#xA;Design Your Prompt Structure codeGenDocGroup_PROMPT, create group Agent Prompt # update new agent configs&#xD;codeGenDocGroup_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;Your goal is to response according the Context Data&amp;#39;s information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.</description>
    </item>
    <item>
      <title>Embedding Config</title>
      <link>/muagent/embedding-model-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/embedding-model-config/</guid>
      <description>Prepare Relevant Parameters First, add the OpenAI configuration; this could also be a model similar to the OpenAI interface (launched via fastchat).&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34; Build LLM Config Constructing with a local model file from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) Constructing via OpenAI from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;openai&amp;#34;, api_key=api_key, api_base_url=api_base_url,&#xD;) Customizing and inputting langchain embeddings from muagent.</description>
    </item>
    <item>
      <title>LLM Config</title>
      <link>/muagent/llm-model-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/llm-model-config/</guid>
      <description>Prepare Relevant Parameters First, add the OpenAI configuration, or you can use another model similar to the OpenAI interface (launched through fastchat).&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34; Build LLM Config By passing the class openai from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;) Customizing and inputting langchain LLM from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from langchain.llms.base import BaseLLM, LLM&#xD;class CustomizedModel(LLM):&#xD;repetition_penalty = 1.</description>
    </item>
    <item>
      <title>MuAgent</title>
      <link>/muagent/muagent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/muagent/</guid>
      <description>Introduction To enhance the performance of large models in terms of inference accuracy, various innovative Large Language Model (LLM) playbooks have emerged in the industry. From the earliest Chain of Thought (CoT) and Thread of Thought (ToT) to Games on Tracks (GoT), these methods have continually expanded the capability boundaries of LLMs. When handling complex problems, we can select, invoke and execute tool feedback through the ReAct process, while realizing multi-round tool use and multi-step execution.</description>
    </item>
    <item>
      <title>Prompt Manager</title>
      <link>/coagent/prompt-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/prompt-manager/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Response：在与智能体的对话中，如果用户希望智能体继续某个话题或内容，可以在此模块中输入续写的上文。例如，在运用REACT模式时，可以在此区域内详细阐述智能体先前的行为和观察结果，以便于智能体构建连贯的后续响应。 Prompt自定义配置 Prompt模块参数 field_name：唯一的字段名称标识，必须提供。 function：指定如何处理输入数据的函数，必须提供。 title：定义模块的标题。若未提供，将自动生成一个标题，该标题通过把字段名称中的下划线替换为空格并将每个单词的首字母大写来构建。 description：提供模块的简要描述，位于模块最上方（标题下方）。默认为空，可选填。 is_context：标识该字段是否属于上下文模块的一部分。默认为True，意味着除非显式指定为False，否则都被视为上下文的一部分。 omit_if_empty：设定当模块内容为空时，是否在prompt中省略该模块，即不显示相应的模板标题和内容。默认为False，意味着即使内容为空也会显示标题。如果希望内容为空时省略模块，需显式设置为True。 Prompt配置示例 Prompt配置由一系列定义prompt模块的字典组成，这些模块将根据指定的参数和功能来处理输入数据并组织成一个完整的prompt。&#xA;在配置中，每个字典代表一个模块，其中包含相关的参数如 field_name, function_name, is_context, title, description, 和 omit_if_empty，用以控制模块的行为和呈现方式。&#xA;context_placeholder 字段用于标识上下文模板的位置，允许在prompt中插入动态内容。&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>Quick Start</title>
      <link>/muagent/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/quick-start/</guid>
      <description>Quick Start For a complete example, see examples/muagent_examples&#xA;First, prepare the relevant configuration information import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then, set up LLM configuration and Embedding model configuration from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.phase import BasePhase&#xD;from muagent.connector.schema import Message&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
  </channel>
</rss>
