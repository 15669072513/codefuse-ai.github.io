<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CodeFuse-AI</title>
    <link>/</link>
    <description>Recent content on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Acknowledgements</title>
      <link>/contribution/acknowledgements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/acknowledgements/</guid>
      <description>The documentation homepage of CodeFuse-ai is built on docura&#xA;The ChatBot project is based on langchain-chatchat and codebox-api.&#xA;&amp;hellip;&amp;hellip;&#xA;Deep gratitude is extended for their open-source contributions!</description>
    </item>
    <item>
      <title>Agent Flow</title>
      <link>/coagent/agent-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/agent-flow/</guid>
      <description>Introduction to Core Connectors To facilitate everyone&amp;rsquo;s understanding of the entire CoAgent link, we use a Flow format to detail how to build through configuration settings.&#xA;Below, we will first introduce the related core components&#xA;Agent At the design level of the Agent, we provide four basic types of Agents, which allows for the basic role settings of these Agents to meet the interaction and usage of a variety of common scenarios.</description>
    </item>
    <item>
      <title>ChatBot-RoadMap</title>
      <link>/docs/chatbot-roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/chatbot-roadmap/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp RoadMap Roadmap Overview&#xA;Sandbox Environment ✅ Isolated sandbox environment for code execution ✅ File upload and download ✅ Support for Java execution environment ⬜ Vector Database &amp;amp; Retrieval ✅ Task retrieval ✅ Tool retrieval ✅ Prompt Management ✅ Memory Management ✅ Multi Agent Framework ✅ PRD (Product Requirement Document), system analysis, interface design ⬜ Generate code based on requirement documents, system analysis, and interface design ⬜ Automated testing, automated debugger ⬜ Operations process integration (ToolLearning) ⬜ Fully automated end-to-end process ⬜ Integration with LLM based on fastchat ✅ Integration with Text Embedding based on sentencebert ✅ Improved vector loading speed ✅ Connector ✅ React Mode based on langchain ✅ Tool retrieval completed with langchain ✅ General Capability for Web Crawl ⬜ Technical documentation: Zhihu, CSDN, Alibaba Cloud Developer Forum, Tencent Cloud Developer Forum, etc.</description>
    </item>
    <item>
      <title>CoAgent</title>
      <link>/coagent/coagent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/coagent/</guid>
      <description>简介 To enhance the performance of large language models (LLMs) in terms of inference accuracy, the industry has seen various innovative approaches to utilizing LLMs. From the earliest Chain of Thought (CoT), Text of Thought (ToT), to Graph of Thought (GoT), these methods have continually expanded the capability boundaries of LLMs. In dealing with complex problems, we can use the ReAct process to select, invoke, and execute tool feedback, achieving multi-round tool usage and multi-step execution.</description>
    </item>
    <item>
      <title>Codefuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/docs/codefuse-chatbot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-chatbot/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp This project is an open-source AI intelligent assistant, specifically designed for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations. Through knowledge retrieval, tool utilization, and sandbox execution, Codefuse-ChatBot can not only answer professional questions you encounter during the development process but also coordinate multiple independent, dispersed platforms through a conversational interface.&#xA;📜 Contents 🤝 Introduction 🧭 Technical Route 🤝 Introduction 💡 The aim of this project is to construct an AI intelligent assistant for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations, through Retrieval Augmented Generation (RAG), Tool Learning, and sandbox environments.</description>
    </item>
    <item>
      <title>codefuse-devops-eval</title>
      <link>/docs/codefuse-devops-eval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-eval/</guid>
      <description>Comming soon</description>
    </item>
    <item>
      <title>codefuse-devops-model</title>
      <link>/docs/codefuse-devops-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model/</guid>
      <description>Comming soon</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/docs/codefuse-modelcache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache/</guid>
      <description>CodeFuse-ModelCache CodeFuse-ModelCache</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/docs/codefuse-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query/</guid>
      <description>CodeFuse-Query CodeFuse-Query</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/coagent/connector-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-agent/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.agents import BaseAgent&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选一个role来做示例 # 从已有的配置中选择一个config，具体参数细节见下面&#xD;role_configs = load_role_configs(AGETN_CONFIGS)&#xD;agent_config = role_configs[&amp;#34;general_planner&amp;#34;]&#xD;# 生成agent实例&#xD;base_agent = BaseAgent(&#xD;role=agent_config.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/coagent/connector-chain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-chain/</guid>
      <description>快速构建一个 agent chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） # 设置openai的api-key&#xD;import os, sys&#xD;import openai&#xD;import importlib&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选多个role组合成 agent chain from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/coagent/connector-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-memory/</guid>
      <description>Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 创建 memory manager 实例 import os&#xD;import openai&#xD;from coagent.base_configs.env_config import KB_ROOT_PATH&#xD;from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.schema import Message&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/coagent/connector-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-phase/</guid>
      <description>快速构建一个 agent phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的 phase 配置中选一个 phase 来做示例 # log-level，print prompt和llm predict&#xD;os.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/coagent/connector-prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-prompt/</guid>
      <description>Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.&#xD;If it&amp;#39;s &amp;#39;continued&amp;#39;, the context cant answer the origin query.</description>
    </item>
    <item>
      <title>Contribution Guide</title>
      <link>/contribution/contribution-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/contribution-guide/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Thank you for your interest in the Codefuse project. We warmly welcome any suggestions, opinions (including criticisms), comments, and contributions to the Codefuse project.&#xA;Your suggestions, opinions, and comments on Codefuse can be directly submitted through GitHub Issues.&#xA;There are many ways to participate in the Codefuse project and contribute to it: code implementation, test writing, process tool improvement, documentation enhancement, and more. We welcome any contributions and will add you to our list of contributors.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/coagent/customed-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/customed-examples/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 autogen 的 auto_feedback_from_code_execution 构建过来，来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 import os, sys, requests&#xD;# from configs.model_config import *&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.chains import BaseChain&#xD;from coagent.connector.schema import Message&#xD;from coagent.connector.configs import AGETN_CONFIGS, CHAIN_CONFIGS, PHASE_CONFIGS&#xD;import importlib&#xD;# update new agent configs&#xD;auto_feedback_from_code_execution_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;You are a helpful AI assistant. Solve tasks using your coding and language skills.&#xD;In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/docs/fastertransformer4codefuse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/fastertransformer4codefuse/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>Issue Report</title>
      <link>/contribution/issue-report/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/issue-report/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Issue Type Issues can be categorized into three types:&#xA;Bug: Issues where code or execution examples contain bugs or lack dependencies, resulting in incorrect execution. Documentation: Discrepancies in documentation, inconsistencies between documentation content and code, etc. Feature: New functionalities that evolve from the current codebase. Issue Template Issue: Bug Template Checklist before submitting an issue Please confirm that you have checked the document, issues, discussions (GitHub feature), and other publicly available documentation.</description>
    </item>
    <item>
      <title>LLM-Configuration</title>
      <link>/docs/LLM-Configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/LLM-Configuration/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Local Privatization/Large Model Interface Access Leveraging open-source LLMs (Large Language Models) and Embedding models, this project enables offline private deployment based on open-source models.&#xA;In addition, the project supports the invocation of OpenAI API.&#xA;Local Privatization Model Access Example of model address configuration, modification of the model_config.py configuration:&#xA;# Recommendation: Use Hugging Face models, preferably the chat models, and avoid using base models, which may not produce correct outputs.</description>
    </item>
    <item>
      <title>MFTCoder</title>
      <link>/docs/mftcoder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder/</guid>
      <description>MFTCoder MFTCoder</description>
    </item>
    <item>
      <title>overview</title>
      <link>/docs/en_overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/en_overview/</guid>
      <description>HuggingFace | ModelScope Hello World! This is CodeFuse! CodeFuse aims to develop Code Large Language Models (Code LLMs) to support and enhance full-lifecycle AI native sotware developing, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.&#xA;We are passionating about creating innovative open-source solutions that empower developers throughout the software development process as shown above. We also encourage engineers and researchers within this community to join us in co-constructing/improving CodeFuse.</description>
    </item>
    <item>
      <title>Prompt Manager</title>
      <link>/coagent/prompt-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/prompt-manager/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Response：在与智能体的对话中，如果用户希望智能体继续某个话题或内容，可以在此模块中输入续写的上文。例如，在运用REACT模式时，可以在此区域内详细阐述智能体先前的行为和观察结果，以便于智能体构建连贯的后续响应。 Prompt自定义配置 Prompt模块参数 field_name：唯一的字段名称标识，必须提供。 function：指定如何处理输入数据的函数，必须提供。 title：定义模块的标题。若未提供，将自动生成一个标题，该标题通过把字段名称中的下划线替换为空格并将每个单词的首字母大写来构建。 description：提供模块的简要描述，位于模块最上方（标题下方）。默认为空，可选填。 is_context：标识该字段是否属于上下文模块的一部分。默认为True，意味着除非显式指定为False，否则都被视为上下文的一部分。 omit_if_empty：设定当模块内容为空时，是否在prompt中省略该模块，即不显示相应的模板标题和内容。默认为False，意味着即使内容为空也会显示标题。如果希望内容为空时省略模块，需显式设置为True。 Prompt配置示例 Prompt配置由一系列定义prompt模块的字典组成，这些模块将根据指定的参数和功能来处理输入数据并组织成一个完整的prompt。&#xA;在配置中，每个字典代表一个模块，其中包含相关的参数如 field_name, function_name, is_context, title, description, 和 omit_if_empty，用以控制模块的行为和呈现方式。&#xA;context_placeholder 字段用于标识上下文模板的位置，允许在prompt中插入动态内容。&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>Pull Request</title>
      <link>/contribution/pull-request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/pull-request/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Contribution Pre-Checklist First, confirm whether you have checked the document, issue, discussion (GitHub features), or other publicly available documentation. Find the GitHub issue you want to address. If none exists, create an issue or draft PR and ask a Maintainer for a check Check for related, similar, or duplicate pull requests Create a draft pull request Complete the PR template for the description Link any GitHub issue(s) that are resolved by your PR Description A description of the PR should be articulated in concise language, highlighting the work completed by the PR.</description>
    </item>
    <item>
      <title>Quick Start</title>
      <link>/coagent/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/quick-start/</guid>
      <description>Quick Start First, set up the LLM configuration import os, sys&#xD;import openai&#xD;# llm config&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34; Next, configure the LLM settings and vector model from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) Finally, choose a pre-existing scenario to execute from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from coagent.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/quickstart/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 🚀 Quick Start To deploy private models, please install the NVIDIA driver by yourself. This project has been tested on Python 3.9.18 and CUDA 11.7 environments, as well as on Windows and macOS systems with x86 architecture. For Docker installation, private LLM access, and related startup issues, see: Start-detail&amp;hellip;&#xA;Preparation of Python environment It is recommended to use conda to manage the python environment (optional) # Prepare conda environment conda create --name Codefusegpt python=3.</description>
    </item>
    <item>
      <title>Start-Detail</title>
      <link>/docs/chatbot/start-detail/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/chatbot/start-detail/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp If you need to deploy a privatized model, please install the NVIDIA driver yourself.&#xA;Preparation of Python environment It is recommended to use conda to manage the python environment (optional) # Prepare conda environment conda create --name Codefusegpt python=3.9 conda activate Codefusegpt Install related dependencies cd Codefuse-ChatBot pip install -r requirements.txt Sandbox Environment Preparation Windows Docker installation: Docker Desktop for Windows supports 64-bit versions of Windows 10 Pro with Hyper-V enabled (Hyper-V is not required for versions v1903 and above), or 64-bit versions of Windows 10 Home v1903 and above.</description>
    </item>
    <item>
      <title>Test-Agent</title>
      <link>/docs/test-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/test-agent/</guid>
      <description>Test-Agent Test-Agent</description>
    </item>
  </channel>
</rss>
