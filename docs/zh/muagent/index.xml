<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Muagents on CodeFuse-AI</title>
    <link>/zh/muagent/</link>
    <description>Recent content in Muagents on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-CN</language>
    <atom:link href="/zh/muagent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent 编排</title>
      <link>/zh/muagent/agent-%E7%BC%96%E6%8E%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/agent-%E7%BC%96%E6%8E%92/</guid>
      <description>核心Connector介绍 为了便于大家理解整个 muagent 的链路，我们采取 Flow 的形式来详细介绍如何通过配置构建&#xA;下面，我们先介绍相关的核心组件&#xA;Agent 在Agent设计层面，我们提供了四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出&#xA;ReactAgent：提供标准React的功能，根据问题实现当前任务&#xA;ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务&#xA;SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答.&#xA;输出后将 message push 到 memory pool 之中，后续通过Memory Manager进行管理&#xA;Chain 基础链路：BaseChain，串联agent的交互，完成相关message和memory的管理&#xA;Phase 基础场景：BasePhase，串联chain的交互，完成相关message和memory的管理&#xA;Prompt Manager Mutli-Agent链路中每一个agent的prompt创建&#xA;通过对promtp_input_keys和promtp_output_keys对的简单设定，可以沿用预设 Prompt Context 创建逻辑，从而实现agent prompt快速配置 也可以对prompt manager模块进行新的 key-context 设计，实现个性化的 Agent Prompt Memory Manager 主要用于 chat history 的管理&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 </description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/zh/muagent/connector-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-agent-zh/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) Agent 配置 定义两个react agent，进行实际任务执行 # 这里采用了预定义的prompt，也可以参考上述prompt完成编写&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/zh/muagent/connector-chain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-chain-zh/</guid>
      <description>快速构建一个Agent Chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/zh/muagent/connector-memory-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-memory-zh/</guid>
      <description>Memory Manager 将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 完整示例见 ~/tests/connector/memory_manager_test.py&#xA;创建 memory manager 实例 import os&#xD;import openai&#xD;from muagent.base_configs.env_config import KB_ROOT_PATH&#xD;from muagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.schema import Message&#xD;#&#xD;OPENAI_API_BASE = &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;os.environ[&amp;#34;model_name&amp;#34;] = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;# os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/zh/muagent/connector-phase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-phase-zh/</guid>
      <description>快速构建一个Agent Phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/zh/muagent/connector-prompt-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-prompt-zh/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/zh/muagent/custom-examples-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-examples-zh/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 代码库来实现代码转API文档的自动生成， 来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 codeGenDocGroup_PROMPT, 构建 group Agent Prompt # update new agent configs&#xD;codeGenDocGroup_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;Your goal is to response according the Context Data&amp;#39;s information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.&#xD;When you need to select the appropriate role for handling a user&amp;#39;s query, carefully read the provided role names, role descriptions and tool list.</description>
    </item>
    <item>
      <title>Embedding 配置</title>
      <link>/zh/muagent/embedding-model-config-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/embedding-model-config-zh/</guid>
      <description>准备相关参数 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动）&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34; 构建LLM Config 通过本地模型文件构建 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) 通过openai构建 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;openai&amp;#34;, api_key=api_key, api_base_url=api_base_url,&#xD;) 自定义langchain embeddings传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;class CustomizedEmbeddings(Embeddings):&#xD;def embed_documents(self, texts: List[str]) -&amp;gt; List[List[float]]:&#xD;embeddings = []&#xD;# add your embedding code&#xD;return embeddings&#xD;def embed_query(self, text: str) -&amp;gt; List[float]:&#xD;&amp;#34;&amp;#34;&amp;#34;Compute query embeddings using a HuggingFace transformer model.</description>
    </item>
    <item>
      <title>LLM 配置</title>
      <link>/zh/muagent/llm-model-config-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/llm-model-config-zh/</guid>
      <description>准备相关参数 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动）&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34; 构建LLM Config 通过调用 类openai 传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;) 自定义 langchain LLM 传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from langchain.llms.base import BaseLLM, LLM&#xD;class CustomizedModel(LLM):&#xD;repetition_penalty = 1.1&#xD;temperature = 0.2&#xD;top_k = 40&#xD;top_p = 0.9&#xD;def predict(self, prompt: str, stop: Optional[List[str]] = None) -&amp;gt; str:&#xD;return self.</description>
    </item>
    <item>
      <title>MuAgent 概览</title>
      <link>/zh/muagent/muagent-%E6%A6%82%E8%A7%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/muagent-%E6%A6%82%E8%A7%88/</guid>
      <description>简介 为了提高大型模型在推理准确性方面的表现，业界出现了多种创新的大型语言模型(LLM)玩法。从最早的CoT、ToT到GoT，这些方法不断拓展了LLM的能力边界。在处理复杂问题时，我们可以通过ReAct过程来选择、调用和执行工具反馈，同时实现多轮工具使用和多步骤执行。&#xA;但对于更复杂的场景，例如复杂代码的开发，单一功能的LLM Agent显然难以胜任。因此，社区开始发展出多Agent的组合玩法，比如专注于metaGPT、GPT-Engineer、chatDev等开发领域的项目，以及专注于自动化构建Agent和Agent对话的AutoGen项目。&#xA;经过对这些框架的深入分析，发现大多数的Agent框架整体耦合度较高，其易用性和可扩展性较差。在预设场景中实现特定场景，但想要进行场景扩展却困难重重。&#xA;因此，我们希望构建一个可扩展、易于使用的Multi-Agent框架，以支持ChatBot在获取知识库信息的同时，能够辅助完成日常办公、数据分析、开发运维等各种通用任务。&#xA;本项目的Mutli-Agent框架汲取兼容了多个框架的优秀设计，比如metaGPT中的消息池（message pool）、autogen中的代理选择器（agent selector）等。&#xA;MuAgent框架 在MuAgent中，我们除了定义Agent交互链路和AgentBase基础执行流以外，还额外设计了 Prompt Manager 和 Memory Manager 两个基础组件，分别用于自动化构建Prompt和chat history管理。最终构建出一个可扩展、易于使用的Multi-Agent框架，包括以下内容&#xA;Agent Base：构建了四种基本的Agent类型BaseAgent、ReactAgent、ExecutorAgent、SelectorAgent，支撑各种场景的基础活动 Communication：通过Message和Parse Message 实体完成Agent间的信息传递，并与Memory Manager交互再Memory Pool完成记忆管理 Prompt Manager：通过Role Handler、Doc/Tool Handler、Session Handler、Customized Handler，来自动化组装Customized 的Agent Prompt Memory Manager： 用于支撑 chat history 的存储管理、信息压缩、记忆检索等管理，最后通过Memory Pool在数据库、本地、向量数据库中完成存储 Component：用于构建Agent的辅助生态组件，包括Retrieval、Tool、Action、Sandbox等 Customized Model：支持私有化的LLM和Embedding的接入 Agent Base 在Agent层面，提供四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用。所有的Action都由Agent执行。&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出 ReactAgent：提供标准React的功能，根据问题实现当前任务 ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务 Agent接受到任务清单([List[task])，对这个任务清单Task进行循环执行（中间也可添加 Feedback Agent来进行任务重新优化），直到任务完成 SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答. Communication 为了让Agent之间进行更好的交互，以及能够让每一个Agent接受到足够的信息完成它们特定任务，我们将Message信息体分成了多个部分，System Content、Info Content、LLM Content和LLM Parsed Content等&#xA;System Content：用于存储管理当前LLM输出的时间，Role信息等 Info Content：LLM辅助信息，比如像知识库查询信息、代码库检索信息、工具信息、Agent信息等 LLM Content：直接存储和传递LLM 产生的信息 LLM Parsed Content：对LLM进行解析转成更易操作的key-value数据结构，方便对LLM内容进行过滤 Customized Content：用于管理自定义action产生的key-value数据内容，用于后续自定义Prompt模板的组装构建 通过对以上消息格式的定义，我们便可以完成通用消息的传递和管理。具体组装见Prompt Manager模块</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/muagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>Quick Start 完整示例见，examples/muagent_examples&#xA;首先，准备相关配置信息 import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后，设置LLM配置和Embedding模型配置 from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.phase import BasePhase&#xD;from muagent.connector.schema import Message&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) 最后选择一个已有场景进行执行 # if you want to analyze a data.</description>
    </item>
    <item>
      <title>自定义 Retrieval 接入</title>
      <link>/zh/muagent/custom-retrieval-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-retrieval-zh/</guid>
      <description>基本介绍 Doc Retrieval 文档向量数据库是当前最主流的知识库构建方法，使用Text Embedding 模型对文档进行向量化并在向量数据库中存储。未来我们也会去支持基于知识图谱查询以及通过大模型自动抽取实体和关系的方式，来挖掘数据中多种复杂关系。&#xA;Code Retrieval LLM在代码生成、修复以及组件理解的任务上，会面临代码训练数据滞后、无法感知代码上下文依赖结构。以及在开发的过程中，对现有代码库和依赖包的理解、检索相关代码、查询元信息等会占用较长的时间。于是我们希望通过代码结构分析和代码检索生成来，以及为LLM提供知识体系外的代码。&#xA;Search Retrieval 除了现成的文档和代码知识库以及之外，在日常中实践中会去浏览大量网页内容获取更多的知识，帮助我们理解新兴的场景、业务、技术等，于是我们接入了duckduckgosearch这款开源的搜索工具，能够为LLM提供知识储备以外的内容。&#xA;Rertrieval 结构 class IMRertrieval:&#xD;def __init__(self,):&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;init your personal attributes&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;pass&#xD;def run(self, ):&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;execute interface, and can use init&amp;#39; attributes&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;pass&#xD;class BaseDocRetrieval(IMRertrieval):&#xD;def __init__(self, knowledge_base_name: str, search_top=5, score_threshold=1.0, embed_config: EmbedConfig=EmbedConfig(), kb_root_path: str=KB_ROOT_PATH):&#xD;self.knowledge_base_name = knowledge_base_name&#xD;self.search_top = search_top&#xD;self.score_threshold = score_threshold&#xD;self.embed_config = embed_config&#xD;self.kb_root_path = kb_root_path&#xD;def run(self, query: str, search_top=None, score_threshold=None, ):&#xD;docs = DocRetrieval.</description>
    </item>
    <item>
      <title>自定义 Tool 接入</title>
      <link>/zh/muagent/custom-tool-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-tool-zh/</guid>
      <description>基本介绍 在MuAgent中也支持Agent完成Tool的注册，通过Python注册模板BaseToolModel类，编写&#xA;Tool_nam Tool_descriptio ToolInputArgs ToolOutputArgs run 等相关属性和方法即可实现工具的快速接入，同时支持langchain Tool接口的直接使用。 例如像上述 XXRetrieval 的功能也可以注册为Tool，最终由LLM执行调用。&#xA;BaseTool 结构 from langchain.agents import Tool&#xD;from pydantic import BaseModel, Field&#xD;from typing import List, Dict&#xD;import json&#xD;class BaseToolModel:&#xD;name = &amp;#34;BaseToolModel&amp;#34;&#xD;description = &amp;#34;Tool Description&amp;#34;&#xD;class ToolInputArgs(BaseModel):&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;Input for MoveFileTool.&#xD;Tips:&#xD;default control Required, e.g. key1 is not Required/key2 is Required&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;key1: str = Field(default=None, description=&amp;#34;hello world!&amp;#34;)&#xD;key2: str = Field(..., description=&amp;#34;hello world!!&amp;#34;)&#xD;class ToolOutputArgs(BaseModel):&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;Input for MoveFileTool.</description>
    </item>
  </channel>
</rss>
