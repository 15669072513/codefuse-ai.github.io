<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Coagents on CodeFuse-AI</title>
    <link>/zh/coagent/</link>
    <description>Recent content in Coagents on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-CN</language>
    <atom:link href="/zh/coagent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent 编排</title>
      <link>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</guid>
      <description>核心Connector介绍 为了便于大家理解整个 CoAgent 的链路，我们采取 Flow 的形式来详细介绍如何通过配置构建&#xA;下面，我们先介绍相关的核心组件&#xA;Agent 在Agent设计层面，我们提供了四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出 ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务 ReactAgent：提供标准React的功能，根据问题实现当前任务 SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答. 输出后将 message push 到 memory pool 之中，后续通过Memory Manager进行管理&#xA;Chain 基础链路：BaseChain，串联agent的交互，完成相关message和memory的管理&#xA;Phase 基础场景：BasePhase，串联chain的交互，完成相关message和memory的管理&#xA;Prompt Manager Mutli-Agent链路中每一个agent的prompt创建&#xA;通过对promtp_input_keys和promtp_output_keys对的简单设定，可以沿用预设 Prompt Context 创建逻辑，从而实现agent prompt快速配置 也可以对prompt manager模块进行新的 key-context 设计，实现个性化的 Agent Prompt Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 </description>
    </item>
    <item>
      <title>CoAgent 概览</title>
      <link>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</guid>
      <description>简介 为了提高大型模型在推理准确性方面的表现，业界出现了多种创新的大型语言模型(LLM)玩法。从最早的CoT、ToT到GoT，这些方法不断拓展了LLM的能力边界。在处理复杂问题时，我们可以通过ReAct过程来选择、调用和执行工具反馈，同时实现多轮工具使用和多步骤执行。&#xA;但对于更复杂的场景，例如复杂代码的开发，单一功能的LLM Agent显然难以胜任。因此，社区开始发展出多Agent的组合玩法，比如专注于metaGPT、GPT-Engineer、chatDev等开发领域的项目，以及专注于自动化构建Agent和Agent对话的AutoGen项目。&#xA;经过对这些框架的深入分析，发现大多数的Agent框架整体耦合度较高，其易用性和可扩展性较差。在预设场景中实现特定场景，但想要进行场景扩展却困难重重。&#xA;因此，我们希望构建一个可扩展、易于使用的Multi-Agent框架，以支持ChatBot在获取知识库信息的同时，能够辅助完成日常办公、数据分析、开发运维等各种通用任务。&#xA;本项目的Mutli-Agent框架汲取兼容了多个框架的优秀设计，比如metaGPT中的消息池（message pool）、autogen中的代理选择器（agent selector）等。&#xA;以下模块将从5个方面介绍Multi Agent框架所需要素：&#xA;Agent Communication在Multi Agent框架中，确保Agent可以有效地进行信息交流对于管理上下文以及提高问答效率至关重要。 a. 遵循简洁直观易于理解的链式对话原则，将Agent以线性方式排列串连成一个执行链路。 b. 借鉴metaGPT中的Message Pool框架，允许Agent对Message Pool进行推送和订阅，使链路更加灵活。有利于精细化Prompt工程的场景，但难以把握复杂链路的关系分析。 Standard Operation Process（SOP）：对LLM的生成结果进行标准化解析和处理。 a. 定义Agent的 Input 和 Output 范围，能够组装和解析相关Action和Status，保证框架运行的稳定性 b. 封装多种基础Action执行模块，如Tool Using、Planning、Coding、Direct Answering、final answer等SOP标识，以满足Agent的基本工作需求。 Plan and Executor：增加LLM的Tool使用、Agent调度、代码的生成。设置了几种基本链路，例如： a. 单轮问答，也可以扩展到CoT、ToT、GoT等形式。 b. ReAct，基础的响应决策过程，模型设置SOP 状态以终止循环 c. TaskPlaning - Executor，任务完成即可结束 Long-short term memory Management：Multi-Agent与单Agent的关键区别在于，Multi-Agent需要处理大量的交流信息，类似人类团队协作的过程。增加一个专门负责内容总结（类似于会议助理）的Agent，对长期记忆进行总结并提更有效信息传递给下一位Agent，而非传递所有内容给下一位Agent。 Human-agent interaction：面对复杂场景时，需要人类介入Agent交互过程并提供反馈。通过上述 Long-short term memory Management 和 Agent Communication 过程，使LLM能准确理解人类的意图，从而更有效地完成任务。 总的来说，这五个要素共同构建了一个Multi Agent框架，确保Agent之间的协作更加紧密和高效，同时也能够适应更复杂的任务需求和更多样的交互场景。通过组合多个Agent链路来实现一个完整且复杂的项目上线场景（Dev Phase），如Demand Chain（CEO）、Product Arguement Chain（CPO、CFO、CTO）、Engineer Group Chain（Selector、Developer1~N）、QA Engineer Chain（Developer、Tester）、Deploy Chain（Developer、Deploer）。</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/zh/coagent/connector-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-agent-zh/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.agents import BaseAgent&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选一个role来做示例 # 从已有的配置中选择一个config，具体参数细节见下面&#xD;role_configs = load_role_configs(AGETN_CONFIGS)&#xD;agent_config = role_configs[&amp;#34;general_planner&amp;#34;]&#xD;# 生成agent实例&#xD;base_agent = BaseAgent(&#xD;role=agent_config.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/zh/coagent/connector-chain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-chain-zh/</guid>
      <description>快速构建一个 agent chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） # 设置openai的api-key&#xD;import os, sys&#xD;import openai&#xD;import importlib&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选多个role组合成 agent chain from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/zh/coagent/connector-memory-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-memory-zh/</guid>
      <description>Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 创建 memory manager 实例 import os&#xD;import openai&#xD;from coagent.base_configs.env_config import KB_ROOT_PATH&#xD;from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.schema import Message&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/zh/coagent/connector-phase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-phase-zh/</guid>
      <description>快速构建一个 agent phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的 phase 配置中选一个 phase 来做示例 # log-level，print prompt和llm predict&#xD;os.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/zh/coagent/connector-prompt-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-prompt-zh/</guid>
      <description>Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.&#xD;If it&amp;#39;s &amp;#39;continued&amp;#39;, the context cant answer the origin query.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/zh/coagent/customed-examples-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/customed-examples-zh/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 autogen 的 auto_feedback_from_code_execution 构建过来，来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 import os, sys, requests&#xD;# from configs.model_config import *&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.chains import BaseChain&#xD;from coagent.connector.schema import Message&#xD;from coagent.connector.configs import AGETN_CONFIGS, CHAIN_CONFIGS, PHASE_CONFIGS&#xD;import importlib&#xD;# update new agent configs&#xD;auto_feedback_from_code_execution_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;You are a helpful AI assistant. Solve tasks using your coding and language skills.&#xD;In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</description>
    </item>
    <item>
      <title>Prompt 管理器</title>
      <link>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Response：在与智能体的对话中，如果用户希望智能体继续某个话题或内容，可以在此模块中输入续写的上文。例如，在运用REACT模式时，可以在此区域内详细阐述智能体先前的行为和观察结果，以便于智能体构建连贯的后续响应。 Prompt自定义配置 Prompt模块参数 field_name：唯一的字段名称标识，必须提供。 function：指定如何处理输入数据的函数，必须提供。 title：定义模块的标题。若未提供，将自动生成一个标题，该标题通过把字段名称中的下划线替换为空格并将每个单词的首字母大写来构建。 description：提供模块的简要描述，位于模块最上方（标题下方）。默认为空，可选填。 is_context：标识该字段是否属于上下文模块的一部分。默认为True，意味着除非显式指定为False，否则都被视为上下文的一部分。 omit_if_empty：设定当模块内容为空时，是否在prompt中省略该模块，即不显示相应的模板标题和内容。默认为False，意味着即使内容为空也会显示标题。如果希望内容为空时省略模块，需显式设置为True。 Prompt配置示例 Prompt配置由一系列定义prompt模块的字典组成，这些模块将根据指定的参数和功能来处理输入数据并组织成一个完整的prompt。&#xA;在配置中，每个字典代表一个模块，其中包含相关的参数如 field_name, function_name, is_context, title, description, 和 omit_if_empty，用以控制模块的行为和呈现方式。&#xA;context_placeholder 字段用于标识上下文模板的位置，允许在prompt中插入动态内容。&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>快速使用 首先，填写LLM配置 import os, sys&#xD;import openai&#xD;# llm config&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 最后选择一个已有场景进行执行 from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message&#xD;# 选择一个已实现得场景进行执行&#xD;# 如果需要做一个数据分析，需要将数据放到某个工作目录，同时指定工作目录（也可使用默认目录）&#xD;import shutil&#xD;source_file = &amp;#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&amp;#39;&#xD;shutil.copy(source_file, JUPYTER_WORK_PATH)&#xD;# 选择一个场景&#xD;phase_name = &amp;#34;baseGroupPhase&amp;#34;&#xD;phase = BasePhase(&#xD;phase_name, embed_config=embed_config, llm_config=llm_config, )&#xD;# round-1 需要通过代码解释器来完成&#xD;query_content = &amp;#34;确认本地是否存在employee_data.</description>
    </item>
  </channel>
</rss>
