<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CodeFuse-AI</title>
    <link>/zh/</link>
    <description>Recent content on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-CN</language>
    <atom:link href="/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent 编排</title>
      <link>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</guid>
      <description>核心Connector介绍 为了便于大家理解整个 CoAgent 的链路，我们采取 Flow 的形式来详细介绍如何通过配置构建&#xA;下面，我们先介绍相关的核心组件&#xA;Agent 在Agent设计层面，我们提供了四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出 ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务 ReactAgent：提供标准React的功能，根据问题实现当前任务 SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答. 输出后将 message push 到 memory pool 之中，后续通过Memory Manager进行管理&#xA;Chain 基础链路：BaseChain，串联agent的交互，完成相关message和memory的管理&#xA;Phase 基础场景：BasePhase，串联chain的交互，完成相关message和memory的管理&#xA;Prompt Manager Mutli-Agent链路中每一个agent的prompt创建&#xA;通过对promtp_input_keys和promtp_output_keys对的简单设定，可以沿用预设 Prompt Context 创建逻辑，从而实现agent prompt快速配置 也可以对prompt manager模块进行新的 key-context 设计，实现个性化的 Agent Prompt Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 </description>
    </item>
    <item>
      <title>ChatBot 技术路线</title>
      <link>/zh/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp RoadMap 完整路线&#xA;Sandbox 环境 ✅ 环境隔离的sandbox环境与代码执行 ✅ 上传、下载文件 ✅ 支持java执行环境 Vector Database &amp;amp; Retrieval task retrieval ✅ tool retrieval ✅ Prompt Management ✅ memory Management ✅ Multi Agent ✅ PRD需求文档、系分、接口设计 ⬜ 根据需求文档、系分、接口设计生产代码 ⬜ 自动测试、自动debugger ⬜ 运维流程接入（ToolLearning）⬜ 全流程自动 ⬜ 基于fastchat接入LLM ✅ 基于sentencebert接入Text Embedding ✅ 向量加载速度提升 ✅ Connector ✅ 基于langchain的react模式 ✅ 基于langchain完成tool检索 ✅ Web Crawl 通用能力 ✅ 技术文档: 知乎、csdn、阿里云开发者论坛、腾讯云开发者论坛等 ✅ issue document ⬜ SDK Library Document ⬜ v0.0 Sandbox 环境 ✅ 环境隔离的sandbox环境与代码执行 ✅ 基于fastchat接入LLM ✅ 基于sentencebert接入Text Embedding ✅ Web Crawl 通用能力：技术文档: 知乎、csdn、阿里云开发者论坛、腾讯云开发者论坛等 ✅ v0.</description>
    </item>
    <item>
      <title>CoAgent 概览</title>
      <link>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</guid>
      <description>简介 为了提高大型模型在推理准确性方面的表现，业界出现了多种创新的大型语言模型(LLM)玩法。从最早的CoT、ToT到GoT，这些方法不断拓展了LLM的能力边界。在处理复杂问题时，我们可以通过ReAct过程来选择、调用和执行工具反馈，同时实现多轮工具使用和多步骤执行。&#xA;但对于更复杂的场景，例如复杂代码的开发，单一功能的LLM Agent显然难以胜任。因此，社区开始发展出多Agent的组合玩法，比如专注于metaGPT、GPT-Engineer、chatDev等开发领域的项目，以及专注于自动化构建Agent和Agent对话的AutoGen项目。&#xA;经过对这些框架的深入分析，发现大多数的Agent框架整体耦合度较高，其易用性和可扩展性较差。在预设场景中实现特定场景，但想要进行场景扩展却困难重重。&#xA;因此，我们希望构建一个可扩展、易于使用的Multi-Agent框架，以支持ChatBot在获取知识库信息的同时，能够辅助完成日常办公、数据分析、开发运维等各种通用任务。&#xA;本项目的Mutli-Agent框架汲取兼容了多个框架的优秀设计，比如metaGPT中的消息池（message pool）、autogen中的代理选择器（agent selector）等。&#xA;以下模块将从5个方面介绍Multi Agent框架所需要素：&#xA;Agent Communication在Multi Agent框架中，确保Agent可以有效地进行信息交流对于管理上下文以及提高问答效率至关重要。 a. 遵循简洁直观易于理解的链式对话原则，将Agent以线性方式排列串连成一个执行链路。 b. 借鉴metaGPT中的Message Pool框架，允许Agent对Message Pool进行推送和订阅，使链路更加灵活。有利于精细化Prompt工程的场景，但难以把握复杂链路的关系分析。 Standard Operation Process（SOP）：对LLM的生成结果进行标准化解析和处理。 a. 定义Agent的 Input 和 Output 范围，能够组装和解析相关Action和Status，保证框架运行的稳定性 b. 封装多种基础Action执行模块，如Tool Using、Planning、Coding、Direct Answering、final answer等SOP标识，以满足Agent的基本工作需求。 Plan and Executor：增加LLM的Tool使用、Agent调度、代码的生成。设置了几种基本链路，例如： a. 单轮问答，也可以扩展到CoT、ToT、GoT等形式。 b. ReAct，基础的响应决策过程，模型设置SOP 状态以终止循环 c. TaskPlaning - Executor，任务完成即可结束 Long-short term memory Management：Multi-Agent与单Agent的关键区别在于，Multi-Agent需要处理大量的交流信息，类似人类团队协作的过程。增加一个专门负责内容总结（类似于会议助理）的Agent，对长期记忆进行总结并提更有效信息传递给下一位Agent，而非传递所有内容给下一位Agent。 Human-agent interaction：面对复杂场景时，需要人类介入Agent交互过程并提供反馈。通过上述 Long-short term memory Management 和 Agent Communication 过程，使LLM能准确理解人类的意图，从而更有效地完成任务。 总的来说，这五个要素共同构建了一个Multi Agent框架，确保Agent之间的协作更加紧密和高效，同时也能够适应更复杂的任务需求和更多样的交互场景。通过组合多个Agent链路来实现一个完整且复杂的项目上线场景（Dev Phase），如Demand Chain（CEO）、Product Arguement Chain（CPO、CFO、CTO）、Engineer Group Chain（Selector、Developer1~N）、QA Engineer Chain（Developer、Tester）、Deploy Chain（Developer、Deploer）。</description>
    </item>
    <item>
      <title>CodeFuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/zh/docs/codefuse-chatbot-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-chatbot-zh/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp DevOps-ChatBot是由蚂蚁CodeFuse团队开发的开源AI智能助手，致力于简化和优化软件开发生命周期中的各个环节。该项目结合了Multi-Agent的协同调度机制，并集成了丰富的工具库、代码库、知识库和沙盒环境，使得LLM模型能够在DevOps领域内有效执行和处理复杂任务。&#xA;📜 目录 🤝 介绍 🎥 演示视频 🧭 技术路线 🤝 介绍 💡 本项目旨在通过检索增强生成（Retrieval Augmented Generation，RAG）、工具学习（Tool Learning）和沙盒环境来构建软件开发全生命周期的AI智能助手，涵盖设计、编码、测试、部署和运维等阶段。 逐渐从各处资料查询、独立分散平台操作的传统开发运维模式转变到大模型问答的智能化开发运维模式，改变人们的开发运维习惯。&#xA;本项目核心差异技术、功能点：&#xA;🧠 智能调度核心： 构建了体系链路完善的调度核心，支持多模式一键配置，简化操作流程。 使用说明 💻 代码整库分析： 实现了仓库级的代码深入理解，以及项目文件级的代码编写与生成，提升了开发效率。 📄 文档分析增强： 融合了文档知识库与知识图谱，通过检索和推理增强，为文档分析提供了更深层次的支持。 🔧 垂类专属知识： 为DevOps领域定制的专属知识库，支持垂类知识库的自助一键构建，便捷实用。 🤖 垂类模型兼容： 针对DevOps领域的小型模型，保证了与DevOps相关平台的兼容性，促进了技术生态的整合。 🌍 依托于开源的 LLM 与 Embedding 模型，本项目可实现基于开源模型的离线私有部署。此外，本项目也支持 OpenAI API 的调用。接入Demo&#xA;👥 核心研发团队长期专注于 AIOps + NLP 领域的研究。我们发起了 Codefuse-ai 项目，希望大家广泛贡献高质量的开发和运维文档，共同完善这套解决方案，以实现“让天下没有难做的开发”的目标。&#xA;🎥 演示视频 为了帮助您更直观地了解 Codefuse-ChatBot 的功能和使用方法，我们录制了一系列演示视频。您可以通过观看这些视频，快速了解本项目的主要特性和操作流程。&#xA;知识库导入和问答：演示视频 本地代码库导入和问答：演示视频 🧭 技术路线 🧠 Multi-Agent Schedule Core: 多智能体调度核心，简易配置即可打造交互式智能体。 🕷️ Multi Source Web Crawl: 多源网络爬虫，提供对指定 URL 的爬取功能，以搜集所需信息。 🗂️ Data Processor: 数据处理器，轻松完成文档载入、数据清洗，及文本切分，整合不同来源的数据。 🔤 Text Embedding &amp;amp; Index:：文本嵌入索引，用户可以轻松上传文件进行文档检索，优化文档分析过程。 🗄️ Vector Database &amp;amp; Graph Database: 向量与图数据库，提供灵活强大的数据管理解决方案。 📝 Prompt Control &amp;amp; Management:：Prompt 控制与管理，精确定义智能体的上下文环境。 🚧 SandBox:：沙盒环境，安全地执行代码编译和动作。 💬 LLM:：智能体大脑，支持多种开源模型和 LLM 接口。 🛠️ API Management:： API 管理工具，实现对开源组件和运维平台的快速集成。 具体实现明细见：技术路线明细</description>
    </item>
    <item>
      <title>CodeFuse-DevOps</title>
      <link>/zh/docs/codefuse-devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops/</guid>
      <description>CodeFuse-DevOps CodeFuse-DevOps</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Eval</title>
      <link>/zh/docs/codefuse-devops-eval-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops-eval-zh/</guid>
      <description>codefuse-devops-eval codefuse-devops-eval</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Model</title>
      <link>/zh/docs/codefuse-devops-model-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops-model-zh/</guid>
      <description>codeFuse-devops-model codeFuse-devops-model</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/zh/docs/codefuse-modelcache-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-modelcache-zh/</guid>
      <description>CodeFuse-ModelCache CodeFuse-ModelCache</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/zh/docs/codefuse-query-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-query-zh/</guid>
      <description>CodeFuse-Query CodeFuse-Query</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/zh/coagent/connector-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-agent-zh/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.agents import BaseAgent&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选一个role来做示例 # 从已有的配置中选择一个config，具体参数细节见下面&#xD;role_configs = load_role_configs(AGETN_CONFIGS)&#xD;agent_config = role_configs[&amp;#34;general_planner&amp;#34;]&#xD;# 生成agent实例&#xD;base_agent = BaseAgent(&#xD;role=agent_config.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/zh/coagent/connector-chain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-chain-zh/</guid>
      <description>快速构建一个 agent chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） # 设置openai的api-key&#xD;import os, sys&#xD;import openai&#xD;import importlib&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选多个role组合成 agent chain from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/zh/coagent/connector-memory-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-memory-zh/</guid>
      <description>Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 创建 memory manager 实例 import os&#xD;import openai&#xD;from coagent.base_configs.env_config import KB_ROOT_PATH&#xD;from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.schema import Message&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/zh/coagent/connector-phase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-phase-zh/</guid>
      <description>快速构建一个 agent phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的 phase 配置中选一个 phase 来做示例 # log-level，print prompt和llm predict&#xD;os.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/zh/coagent/connector-prompt-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-prompt-zh/</guid>
      <description>Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.&#xD;If it&amp;#39;s &amp;#39;continued&amp;#39;, the context cant answer the origin query.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/zh/coagent/customed-examples-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/customed-examples-zh/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 autogen 的 auto_feedback_from_code_execution 构建过来，来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 import os, sys, requests&#xD;# from configs.model_config import *&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.chains import BaseChain&#xD;from coagent.connector.schema import Message&#xD;from coagent.connector.configs import AGETN_CONFIGS, CHAIN_CONFIGS, PHASE_CONFIGS&#xD;import importlib&#xD;# update new agent configs&#xD;auto_feedback_from_code_execution_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;You are a helpful AI assistant. Solve tasks using your coding and language skills.&#xD;In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/zh/docs/fastertransformer4codefuse-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/fastertransformer4codefuse-zh/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>MFTCoder</title>
      <link>/zh/docs/mftcoder-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/mftcoder-zh/</guid>
      <description>MFTCoder MFTCoder</description>
    </item>
    <item>
      <title>Prompt 管理器</title>
      <link>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Response：在与智能体的对话中，如果用户希望智能体继续某个话题或内容，可以在此模块中输入续写的上文。例如，在运用REACT模式时，可以在此区域内详细阐述智能体先前的行为和观察结果，以便于智能体构建连贯的后续响应。 Prompt自定义配置 Prompt模块参数 field_name：唯一的字段名称标识，必须提供。 function：指定如何处理输入数据的函数，必须提供。 title：定义模块的标题。若未提供，将自动生成一个标题，该标题通过把字段名称中的下划线替换为空格并将每个单词的首字母大写来构建。 description：提供模块的简要描述，位于模块最上方（标题下方）。默认为空，可选填。 is_context：标识该字段是否属于上下文模块的一部分。默认为True，意味着除非显式指定为False，否则都被视为上下文的一部分。 omit_if_empty：设定当模块内容为空时，是否在prompt中省略该模块，即不显示相应的模板标题和内容。默认为False，意味着即使内容为空也会显示标题。如果希望内容为空时省略模块，需显式设置为True。 Prompt配置示例 Prompt配置由一系列定义prompt模块的字典组成，这些模块将根据指定的参数和功能来处理输入数据并组织成一个完整的prompt。&#xA;在配置中，每个字典代表一个模块，其中包含相关的参数如 field_name, function_name, is_context, title, description, 和 omit_if_empty，用以控制模块的行为和呈现方式。&#xA;context_placeholder 字段用于标识上下文模板的位置，允许在prompt中插入动态内容。&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>Test-Agent</title>
      <link>/zh/docs/test-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/test-agent-zh/</guid>
      <description>Test-Agent Test-Agent</description>
    </item>
    <item>
      <title>本地私有化&amp;大模型接口接入</title>
      <link>/zh/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 本地私有化/大模型接口接入 依托于开源的 LLM 与 Embedding 模型，本项目可实现基于开源模型的离线私有部署。此外，本项目也支持 OpenAI API 的调用。&#xA;本地私有化模型接入 模型地址配置示例，model_config.py配置修改&#xA;# 建议：走huggingface接入，尽量使用chat模型，不要使用base，无法获取正确输出 # 注意：当llm_model_dict和VLLM_MODEL_DICT同时存在时，优先启动VLLM_MODEL_DICT中的模型配置 # llm_model_dict 配置接入示例如下 # 1、若把模型放到 ~/codefuse-chatbot/llm_models 路径下 # 若模型地址如下 model_dir: ~/codefuse-chatbot/llm_models/THUDM/chatglm-6b # 参考配置如下 llm_model_dict = { &amp;#34;chatglm-6b&amp;#34;: { &amp;#34;local_model_path&amp;#34;: &amp;#34;THUDM/chatglm-6b&amp;#34;, &amp;#34;api_base_url&amp;#34;: &amp;#34;http://localhost:8888/v1&amp;#34;, # &amp;#34;name&amp;#34;修改为fastchat服务中的&amp;#34;api_base_url&amp;#34; &amp;#34;api_key&amp;#34;: &amp;#34;EMPTY&amp;#34; } } VLLM_MODEL_DICT = { &amp;#39;chatglm2-6b&amp;#39;: &amp;#34;THUDM/chatglm-6b&amp;#34;, } # or 若模型地址如下 model_dir: ~/codefuse-chatbot/llm_models/chatglm-6b llm_model_dict = { &amp;#34;chatglm-6b&amp;#34;: { &amp;#34;local_model_path&amp;#34;: &amp;#34;chatglm-6b&amp;#34;, &amp;#34;api_base_url&amp;#34;: &amp;#34;http://localhost:8888/v1&amp;#34;, # &amp;#34;name&amp;#34;修改为fastchat服务中的&amp;#34;api_base_url&amp;#34; &amp;#34;api_key&amp;#34;: &amp;#34;EMPTY&amp;#34; } } VLLM_MODEL_DICT = { &amp;#39;chatglm2-6b&amp;#39;: &amp;#34;chatglm-6b&amp;#34;, } # 2、若不想移动相关模型到 ~/codefuse-chatbot/llm_models # 同时删除 `模型路径重置` 以下的相关代码，具体见model_config.</description>
    </item>
    <item>
      <title>概览</title>
      <link>/zh/docs/zh_overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/zh_overview/</guid>
      <description>HuggingFace | 魔搭社区 | 产品主页 Hello World! This is CodeFuse!&#xA;CodeFuse的使命是开发专门设计用于支持整个软件开发生命周期的大型代码语言模型（Code LLMs），涵盖设计、需求、编码、测试、部署、运维等关键阶段。我们致力于打造创新的解决方案，让软件开发者们在研发的过程中如丝般顺滑。&#xA;我们非常有激情去构建创新的解决方案来支持全生命周期AI驱动的软件开发，如上图所示。同时，我们也诚邀志同道合的工程师和研究人员加入这个社区，共同构建和增强CodeFuse。</description>
    </item>
    <item>
      <title>贡献指南</title>
      <link>/zh/contribution/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 非常感谢您对 Codefuse 项目感兴趣，我们非常欢迎您对 Codefuse 项目的各种建议、意见（包括批评）、评论和贡献。&#xA;您对 Codefuse 的各种建议、意见、评论可以直接通过 GitHub 的 Issues 提出。&#xA;参与 Codefuse 项目并为其作出贡献的方法有很多：代码实现、测试编写、流程工具改进、文档完善等等。任何贡献我们都会非常欢迎，并将您加入贡献者列表.&#xA;进一步，有了足够的贡献后，您还可以有机会成为 Codefuse 的 Committer。&#xA;任何问题，您都可以联系我们得到及时解答，联系方式包括微信、Gitter（GitHub提供的即时聊天工具）、邮件等等。&#xA;初次接触 初次来到 Codefuse 社区，您可以：&#xA;关注 Codefuse Github 代码库 加入 Codefuse 相关的微信群 随时提问； 通过以上方式及时了解 Codefuse 项目的开发动态并为您关注的话题发表意见。 贡献方式 这份贡献指南并不仅仅关于编写代码。我们重视并感激在各个领域的帮助。以下是一些您可以贡献的方式&#xA;文档 Issue PR 改进文档 文档是您了解 Codefuse 的最主要的方式，也是我们最需要帮助的地方！&#xA;浏览文档，可以加深您对 Codefuse 的了解，也可以帮助您理解 Codefuse 的功能和技术细节，如果您发现文档有问题，请及时联系我们；&#xA;如果您对改进文档的质量感兴趣，不论是修订一个页面的地址、更正一个链接、以及写一篇更优秀的入门文档，我们都非常欢迎！&#xA;我们的文档大多数是使用 markdown 格式编写的，您可以直接通过在 GitHub 中的 docs/ 中修改并提交文档变更。如果提交代码变更，可以参阅 Pull Request。&#xA;如果发现了一个 Bug 或问题 如果发现了一个 Bug 或问题，您可以直接通过 GitHub 的 Issues 提一个新的 Issue，我们会有人定期处理。详情见Issue模板</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>快速使用 首先，填写LLM配置 import os, sys&#xD;import openai&#xD;# llm config&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 最后选择一个已有场景进行执行 from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message&#xD;# 选择一个已实现得场景进行执行&#xD;# 如果需要做一个数据分析，需要将数据放到某个工作目录，同时指定工作目录（也可使用默认目录）&#xD;import shutil&#xD;source_file = &amp;#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&amp;#39;&#xD;shutil.copy(source_file, JUPYTER_WORK_PATH)&#xD;# 选择一个场景&#xD;phase_name = &amp;#34;baseGroupPhase&amp;#34;&#xD;phase = BasePhase(&#xD;phase_name, embed_config=embed_config, llm_config=llm_config, )&#xD;# round-1 需要通过代码解释器来完成&#xD;query_content = &amp;#34;确认本地是否存在employee_data.</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/docs/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 🚀 快速使用 如需使用私有化模型部署，请自行安装 nvidia 驱动程序，本项目已在 Python 3.9.18，CUDA 11.7 环境下，Windows、X86 架构的 macOS 系统中完成测试。&#xA;Docker安装、私有化LLM接入及相关启动问题见：快速使用明细&#xA;python 环境准备 推荐采用 conda 对 python 环境进行管理（可选） # 准备 conda 环境 conda create --name devopsgpt python=3.9 conda activate devopsgpt 安装相关依赖 cd codefuse-chatbot pip install -r requirements.txt 基础配置 # 修改服务启动的基础配置 cd configs cp model_config.py.example model_config.py cp server_config.py.example server_config.py # model_config#11~12 若需要使用openai接口，openai接口key os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34; # 可自行替换自己需要的api_base_url os.environ[&amp;#34;API_BASE_URL&amp;#34;] = &amp;#34;https://api.openai.com/v1&amp;#34; # vi model_config#LLM_MODEL 你需要选择的语言模型 LLM_MODEL = &amp;#34;gpt-3.</description>
    </item>
    <item>
      <title>启动明细</title>
      <link>/zh/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 如需使用私有化模型部署，请自行安装 nvidia 驱动程序。&#xA;python 环境准备 推荐采用 conda 对 python 环境进行管理（可选） # 准备 conda 环境 conda create --name devopsgpt python=3.9 conda activate devopsgpt 安装相关依赖 cd codefuse-chatbot # python=3.9，notebook用最新即可，python=3.8用notebook=6.5.6 pip install -r requirements.txt 沙盒环境准备 windows Docker 安装： Docker Desktop for Windows 支持 64 位版本的 Windows 10 Pro，且必须开启 Hyper-V（若版本为 v1903 及以上则无需开启 Hyper-V），或者 64 位版本的 Windows 10 Home v1903 及以上版本。&#xA;【全面详细】Windows10 Docker安装详细教程 Docker 从入门到实践 Docker Desktop requires the Server service to be enabled 处理 安装wsl或者等报错提示 Linux Docker 安装： Linux 安装相对比较简单，请自行 baidu/google 相关安装</description>
    </item>
    <item>
      <title>如何提交Issue</title>
      <link>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4issue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4issue/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Issue Type Issue分为三种类型&#xA;Bug: 代码或者执行示例存在bug或缺少依赖导致无法正确执行 Documentation：文档表述存在争议、文档内容与代码不一致等 Feature：在当前代码基础继续演进的新功能 Issue Template Issue: Bug Template 提交Issue前的确认清单 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息&#xA;我搜索了Codefuse相关的所有文档。 我使用GitHub搜索寻找了一个类似的问题，但没有找到。 我为这个问题添加了一个非常描述性的标题。 系统信息 确认系统，如 mac -xx 、windwos-xx、linux-xx&#xA;代码版本 确认代码版本或者分支，master、release等&#xA;问题描述 描述您碰到的问题，想要实现的事情、或代码执行Bug&#xA;代码示例 附上你的执行代码和相关配置，以便能够快速介入进行复现&#xA;报错信息、日志 执行上述代码示例后的报错日志和相关信息&#xA;相关依赖的模块 以chatbot项目为例&#xA;connector codechat sandbox &amp;hellip; Issue: Documentation Template Issue with current documentation: 请帮忙指出当前文档中的问题、错别字或者令人困惑的地方&#xA;Idea or request for content 您觉得合理的文档表述方式应该是什么样的&#xA;Issue: Feature Template 提交Issue前的确认清单 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息&#xA;我搜索了Codefuse相关的所有文档。 我使用GitHub Issue搜索寻找了一个类似的问题，但没有找到。 我为这个问题添加了一个非常描述性的标题。 功能描述 描述这个功能作何用途</description>
    </item>
    <item>
      <title>如何提交PR</title>
      <link>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4pr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4pr/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Contribution Pre-Checklist 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息 找到你想处理的GitHub问题。如果不存在，创建一个问题或草案PR，并请求维护者进行检查。 检查相关的、相似的或重复的拉取请求。 创建一个草案拉取请求。 完成PR模板中的描述。 链接任何被你的PR解决的GitHub问题。 Description PR的描述信息，用简洁的语言表达PR完成的事情，具体规范见Commit 格式规范&#xA;Related Issue #xx if has&#xA;Test Code with Result 请提供相关的测试代码如果有必要的话&#xA;Commit 格式规范 Commit 分为“标题”和“内容”。原则上标题全部小写。内容首字母大写。&#xA;标题 commit message的标题：[&amp;lt;type&amp;gt;](&amp;lt;scope&amp;gt;) &amp;lt;subject&amp;gt; (#pr)&#xA;type 可选值 本次提交的类型，限定在以下类型（全小写）&#xA;fix：bug修复 feature：新增功能 feature-wip：开发中的功能，比如某功能的部分代码。 improvement：原有功能的优化和改进 style：代码风格调整 typo：代码或文档勘误 refactor：代码重构（不涉及功能变动） performance/optimize：性能优化 test：单元测试的添加或修复 deps：第三方依赖库的修改 community：社区相关的修改，如修改 Github Issue 模板等。 几点说明：&#xA;如在一次提交中出现多种类型，需增加多个类型。 如代码重构带来了性能提升，可以同时添加 [refactor][optimize] 不得出现如上所列类型之外的其他类型。如有必要，需要将新增类型添加到这个文档中。&#xA;scope 可选值 本次提交涉及的模块范围。因为功能模块繁多，在此仅罗列部分，后续根据需求不断完善。 以 chatbot的框架为例&#xA;connector codechat sandbox &amp;hellip; 几点说明：&#xA;尽量使用列表中已存在的选项。如需添加，请及时更新本文档。</description>
    </item>
    <item>
      <title>致谢</title>
      <link>/zh/contribution/%E8%87%B4%E8%B0%A2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E8%87%B4%E8%B0%A2/</guid>
      <description>CodeFuse-ai 文档主页基于docura构建！&#xA;ChatBot 项目基于langchain-chatchat和codebox-api!&#xA;&amp;hellip;&amp;hellip;&#xA;在此深深感谢他们的开源贡献！</description>
    </item>
  </channel>
</rss>
