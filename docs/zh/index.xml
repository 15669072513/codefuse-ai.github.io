<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CodeFuse-AI</title>
    <link>/zh/</link>
    <description>Recent content on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-CN</language>
    <atom:link href="/zh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>/zh/docs/codefuse-query/1_abstract/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-query/1_abstract/</guid>
      <description>引言 随着大规模软件开发的普及，对可扩展且易于适应的静态代码分析技术的需求正在加大。传统的静态分析工具，如 Clang Static Analyzer (CSA) 或 PMD，在检查编程规则或样式问题方面已经展现出了良好的效果。然而，这些工具通常是为了满足特定的目标而设计的，往往无法满足现代软件开发环境中多变和多元化的需求。这些需求可以涉及服务质量 (QoS)、各种编程语言、不同的算法需求，以及各种性能需求。例如，安全团队可能需要复杂的算法，如上下文敏感的污点分析，来审查较小的代码库，而项目经理可能需要一种相对较轻的算法，例如计算圈复杂度的算法，以在较大的代码库上测量开发人员的生产力。&#xA;这些多元化的需求，加上大型组织中常见的计算资源限制，构成了一项重大的挑战。由于传统工具采用的是问题特定的计算方式，往往无法在这种环境中实现扩展。因此，我们推出了 CodeQuery，这是一个专为大规模静态分析设计的集中式数据平台。 在 CodeQuery 的实现中，我们把源代码和分析结果看作数据，把执行过程看作大数据处理，这与传统的以工具为中心的方法有着显著的不同。我们利用大型组织中的常见系统，如数据仓库、MaxCompute 和 Hive 等数据计算设施、OSS 对象存储和 Kubernetes 等灵活计算资源，让 CodeQuery 能够无缝地融入这些系统中。这种方法使 CodeQuery 高度可维护和可扩展，能够支持多元化的需求，并有效应对不断变化的需求。此外，CodeQuery 的开放架构鼓励各种内部系统之间的互操作性，实现了无缝的交互和数据交换。这种集成和交互能力不仅提高了组织内部的自动化程度，也提高了效率，降低了手动错误的可能性。通过打破信息孤岛，推动更互联、更自动化的环境，CodeQuery 显著提高了软件开发过程的整体生产力和效率。 此外，CodeQuery 的以数据为中心的方法在处理静态源代码分析的领域特定挑战时具有独特的优势。例如，源代码通常是一个高度结构化和互联的数据集，与其他代码和配置文件有强烈的信息和连接。将代码视为数据，CodeQuery 可以巧妙地处理这些问题，这使得它特别适合在大型组织中使用，其中代码库持续但逐步地进行演变，大部分代码在每天进行微小的改动同时保持稳定。 CodeQuery 还支持如基于代码数据的商业智能 (BI) 这类用例，能生成报告和仪表板，协助监控和决策过程。此外，CodeQuery 在分析大型语言模型 (LLM) 的训练数据方面发挥了重要作用，提供了增强这些模型整体效果的深入见解。&#xA;在当前的静态分析领域，CodeQuery 带来了一种新的范式。它不仅满足了大规模、复杂的代码库分析需求，还能适应不断变化和多元化的静态分析场景。CodeQuery 的以数据为中心的方法，使得其在处理大数据环境中的代码分析问题时具有独特优势。CodeQuery 的设计，旨在解决大规模软件开发环境中的静态分析问题。它能够将源代码和分析结果视作数据，使得其可以灵活地融入大型组织的各种系统中。这种方法不仅可以有效地处理大规模的代码库，还可以应对各种复杂的分析需求，从而使得静态分析工作变得更加高效和准确。&#xA;CodeQuery 的特点和优势可以概括为以下几点：&#xA;高度可扩展：CodeQuery 可以处理大规模的代码库，且能够适应不同的分析需求。这种高度的可扩展性使得 CodeQuery 可以在大型组织中发挥重要作用。 以数据为中心：CodeQuery 将源代码和分析结果视作数据，这种以数据为中心的方法使其在处理大数据环境中的代码分析问题时具有独特优势。 高度集成：CodeQuery 能够无缝地融入大型组织的各种系统中，包括数据仓库、数据计算设施、对象存储和灵活计算资源等。这种高度的集成性使得 CodeQuery 在大型组织中的使用变得更加方便和高效。 支持多元化的需求：CodeQuery 不仅可以处理大规模的代码库，还可以应对各种复杂的分析需求，包括服务质量分析需求、跨编程语言分析需求、算法需求和性能需求等。 CodeQuery 是一种强大的静态代码分析平台，适合大规模、复杂的代码库分析场景。它的以数据为中心的方法和高度的可扩展性使得它在现代软件开发环境中具有独特的优势。未来，随着静态代码分析技术的不断发展，CodeQuery 有望在这个领域中扮演更加重要的角色。</description>
    </item>
    <item>
      <title></title>
      <link>/zh/docs/devops_eval/tool_learning_evalution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/devops_eval/tool_learning_evalution/</guid>
      <description>tool learning 数据集评测教程 chatml接入方式 如果需要在自己的 huggingface 格式的模型上进行测试的话，总的步骤分为如下几步:&#xA;编写 ~/evals/FuncCallEvalution 的 create_prompts 函数 编写 ~/models/base_model 的 相关函数 注册模型和评估函数 执行测试脚本 如果模型在加载进来后不需要特殊的处理，而且输入也不需要转换为特定的格式（e.g. chatml 格式或者其他的 human-bot 格式），请直接跳转到第四步直接发起测试。 1. 编写 loader 函数 如果模型在加载进来还需要做一些额外的处理（e.g. tokenizer 调整），需要去 src.context_builder.context_builder_family.py 中继承 ModelAndTokenizerLoader 类来覆写对应的 load_model 和 load_tokenizer 函数，具体可以参照以下示例：&#xA;class FuncCallEvalution(ToolEvalution): def create_prompts(self, func_call_datas): &amp;#39;&amp;#39;&amp;#39; datas: [ { &amp;#34;instruction&amp;#34;: history[his_idx], &amp;#34;input&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;output&amp;#34;: output, &amp;#34;history&amp;#34;: [(human_content, ai_content), (), ()], &amp;#34;functions&amp;#34;: tools } ] &amp;#39;&amp;#39;&amp;#39; system_content = &amp;#39;&amp;#39;&amp;#39;CodeFuse是一个面向研发领域的智能助手，旨在中立的、无害的帮助用户解决开发相关的问题，所有的回答均使用Markdown格式返回。 你能利用许多工具和功能来完成给定的任务，在每一步中，你需要分析当前状态，并通过执行函数调用来确定下一步的行动方向。你可以进行多次尝试。如果你计划连续尝试不同的条件，请每次尝试一种条件。若给定了Finish函数,则以Finish调用结束，若没提供Finish函数，则以不带function_call的对话结束。&amp;#39;&amp;#39;&amp;#39; function_format = &amp;#39;&amp;#39;&amp;#39;You are ToolGPT, you have access to the following APIs:\n{tools}&amp;#39;&amp;#39;&amp;#39; func_call_train_datas = [] history_error_cnt = 0 funccall_error_cnt = 0 for data in func_call_datas: tools = data[&amp;#34;functions&amp;#34;] chatrounds = data[&amp;#34;chatrounds&amp;#34;] function_content = &amp;#34;&amp;#34; if len(tools) &amp;gt; 0: function_content = function_format.</description>
    </item>
    <item>
      <title></title>
      <link>/zh/docs/devops_eval/tool_learning_info_zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/devops_eval/tool_learning_info_zh/</guid>
      <description>数据样例 在数据上我们完全兼容了 OpenAI Function Calling，具体格式如下：&#xA;Function Call的数据格式&#xA;Input Key Input Type Input Description functions List[Swagger] 工具集合 chatrounds List[chatround] 多轮对话数据 chatrounds的数据格式&#xA;Input Key Input Type Input Description role string 角色名称，包含三种类别，user、assistant、function name string 若role为function，则存在name字段，为function的名称 content string role的返回内容 function_call dict 工具调用 { &amp;#34;functions&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;查询复旦大学往年分数线，例如：查询2020年复旦大学的分数线&amp;#34;, &amp;#34;parameters&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;, &amp;#34;properties&amp;#34;: { &amp;#34;year&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;年份，例如：2020，2019，2018&amp;#34; } }, &amp;#34;required&amp;#34;: [ &amp;#34;year&amp;#34; ] } } ], &amp;#34;chatrounds&amp;#34;: [ { &amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;CodeFuse是一个面向研发领域的智能助手，旨在中立的、无害的帮助用户解决开发相关的问题，所有的回答均使用Markdown格式返回。\n你能利用许多工具和功能来完成给定的任务，在每一步中，你需要分析当前状态，并通过执行函数调用来确定下一步的行动方向。你可以进行多次尝试。如果你计划连续尝试不同的条件，请每次尝试一种条件。若给定了Finish函数,则以Finish调用结束，若没提供Finish函数，则以不带function_call的对话结束。&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;查询2020年复旦大学的分数线&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;assistant&amp;#34;, &amp;#34;content&amp;#34;: null, &amp;#34;function_call&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;arguments&amp;#34;: &amp;#34;{\n \&amp;#34;year\&amp;#34;: \&amp;#34;2020\&amp;#34;\n}&amp;#34; } }, { &amp;#34;role&amp;#34;: &amp;#34;function&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;{\n \&amp;#34;scoreline\&amp;#34;:{\n \&amp;#34;文科一批\&amp;#34;: 630, \n \&amp;#34;文科二批\&amp;#34;: 610, \n \&amp;#34;理科一批\&amp;#34;: 650, \n \&amp;#34;理科二批\&amp;#34;: 630 \n }\n}&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;assistant&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;2020年复旦大学的分数线如下：\n\n- 文科一批：630分\n- 文科二批：610分\n- 理科一批：650分\n- 理科二批：630分&amp;#34; } ] } 上述Function Call的数据样例为给定特定工具集后，用于回答用户查询某高校录取分数线的问题。</description>
    </item>
    <item>
      <title></title>
      <link>/zh/docs/devops_eval/tutorial_zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/devops_eval/tutorial_zh/</guid>
      <description>数据集评测教程 🚀 如何进行测试 如果需要在自己的 huggingface 格式的模型上进行测试的话，总的步骤分为如下几步:&#xA;编写 Model 的 loader 函数 编写 Model 的 context_builder 函数 注册模型到配置文件中 执行测试脚本 如果模型在加载进来后不需要特殊的处理，而且输入也不需要转换为特定的格式（e.g. chatml 格式或者其他的 human-bot 格式），请直接跳转到第四步直接发起测试。 1. 编写 loader 函数 如果模型在加载进来还需要做一些额外的处理（e.g. tokenizer 调整），需要去 src.context_builder.context_builder_family.py 中继承 ModelAndTokenizerLoader 类来覆写对应的 load_model 和 load_tokenizer 函数，具体可以参照以下示例：&#xA;class QwenModelAndTokenizerLoader(ModelAndTokenizerLoader): def __init__(self): super().__init__() pass def load_model(self, model_path: str): model = super().load_model(model_path) model.generation_config = GenerationConfig.from_pretrained(model_path) return model def load_tokenizer(self, model_path: str): tokenizer = super().load_tokenizer(model_path) # read generation config with open(model_path + &amp;#39;/generation_config.</description>
    </item>
    <item>
      <title>Agent 编排</title>
      <link>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/agent-%E7%BC%96%E6%8E%92/</guid>
      <description>核心Connector介绍 为了便于大家理解整个 CoAgent 的链路，我们采取 Flow 的形式来详细介绍如何通过配置构建&#xA;下面，我们先介绍相关的核心组件&#xA;Agent 在Agent设计层面，我们提供了四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出 ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务 ReactAgent：提供标准React的功能，根据问题实现当前任务 SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答. 输出后将 message push 到 memory pool 之中，后续通过Memory Manager进行管理&#xA;Chain 基础链路：BaseChain，串联agent的交互，完成相关message和memory的管理&#xA;Phase 基础场景：BasePhase，串联chain的交互，完成相关message和memory的管理&#xA;Prompt Manager Mutli-Agent链路中每一个agent的prompt创建&#xA;通过对promtp_input_keys和promtp_output_keys对的简单设定，可以沿用预设 Prompt Context 创建逻辑，从而实现agent prompt快速配置 也可以对prompt manager模块进行新的 key-context 设计，实现个性化的 Agent Prompt Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 </description>
    </item>
    <item>
      <title>Agent 编排</title>
      <link>/zh/muagent/agent-%E7%BC%96%E6%8E%92/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/agent-%E7%BC%96%E6%8E%92/</guid>
      <description>核心Connector介绍 为了便于大家理解整个 muagent 的链路，我们采取 Flow 的形式来详细介绍如何通过配置构建&#xA;下面，我们先介绍相关的核心组件&#xA;Agent 在Agent设计层面，我们提供了四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出&#xA;ReactAgent：提供标准React的功能，根据问题实现当前任务&#xA;ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务&#xA;SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答.&#xA;输出后将 message push 到 memory pool 之中，后续通过Memory Manager进行管理&#xA;Chain 基础链路：BaseChain，串联agent的交互，完成相关message和memory的管理&#xA;Phase 基础场景：BasePhase，串联chain的交互，完成相关message和memory的管理&#xA;Prompt Manager Mutli-Agent链路中每一个agent的prompt创建&#xA;通过对promtp_input_keys和promtp_output_keys对的简单设定，可以沿用预设 Prompt Context 创建逻辑，从而实现agent prompt快速配置 也可以对prompt manager模块进行新的 key-context 设计，实现个性化的 Agent Prompt Memory Manager 主要用于 chat history 的管理&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 </description>
    </item>
    <item>
      <title>ChatBot 技术路线</title>
      <link>/zh/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/chatbot-%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp RoadMap 完整路线&#xA;Sandbox 环境 ✅ 环境隔离的sandbox环境与代码执行 ✅ 上传、下载文件 ✅ 支持java执行环境 Vector Database &amp;amp; Retrieval task retrieval ✅ tool retrieval ✅ Prompt Management ✅ memory Management ✅ Multi Agent ✅ PRD需求文档、系分、接口设计 ⬜ 根据需求文档、系分、接口设计生产代码 ⬜ 自动测试、自动debugger ⬜ 运维流程接入（ToolLearning）⬜ 全流程自动 ⬜ 基于fastchat接入LLM ✅ 基于sentencebert接入Text Embedding ✅ 向量加载速度提升 ✅ Connector ✅ 基于langchain的react模式 ✅ 基于langchain完成tool检索 ✅ Web Crawl 通用能力 ✅ 技术文档: 知乎、csdn、阿里云开发者论坛、腾讯云开发者论坛等 ✅ issue document ⬜ SDK Library Document ⬜ v0.0 Sandbox 环境 ✅ 环境隔离的sandbox环境与代码执行 ✅ 基于fastchat接入LLM ✅ 基于sentencebert接入Text Embedding ✅ Web Crawl 通用能力：技术文档: 知乎、csdn、阿里云开发者论坛、腾讯云开发者论坛等 ✅ v0.</description>
    </item>
    <item>
      <title>CoAgent 概览</title>
      <link>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/coagent-%E6%A6%82%E8%A7%88/</guid>
      <description>简介 为了提高大型模型在推理准确性方面的表现，业界出现了多种创新的大型语言模型(LLM)玩法。从最早的CoT、ToT到GoT，这些方法不断拓展了LLM的能力边界。在处理复杂问题时，我们可以通过ReAct过程来选择、调用和执行工具反馈，同时实现多轮工具使用和多步骤执行。&#xA;但对于更复杂的场景，例如复杂代码的开发，单一功能的LLM Agent显然难以胜任。因此，社区开始发展出多Agent的组合玩法，比如专注于metaGPT、GPT-Engineer、chatDev等开发领域的项目，以及专注于自动化构建Agent和Agent对话的AutoGen项目。&#xA;经过对这些框架的深入分析，发现大多数的Agent框架整体耦合度较高，其易用性和可扩展性较差。在预设场景中实现特定场景，但想要进行场景扩展却困难重重。&#xA;因此，我们希望构建一个可扩展、易于使用的Multi-Agent框架，以支持ChatBot在获取知识库信息的同时，能够辅助完成日常办公、数据分析、开发运维等各种通用任务。&#xA;本项目的Mutli-Agent框架汲取兼容了多个框架的优秀设计，比如metaGPT中的消息池（message pool）、autogen中的代理选择器（agent selector）等。&#xA;以下模块将从5个方面介绍Multi Agent框架所需要素：&#xA;Agent Communication在Multi Agent框架中，确保Agent可以有效地进行信息交流对于管理上下文以及提高问答效率至关重要。 a. 遵循简洁直观易于理解的链式对话原则，将Agent以线性方式排列串连成一个执行链路。 b. 借鉴metaGPT中的Message Pool框架，允许Agent对Message Pool进行推送和订阅，使链路更加灵活。有利于精细化Prompt工程的场景，但难以把握复杂链路的关系分析。 Standard Operation Process（SOP）：对LLM的生成结果进行标准化解析和处理。 a. 定义Agent的 Input 和 Output 范围，能够组装和解析相关Action和Status，保证框架运行的稳定性 b. 封装多种基础Action执行模块，如Tool Using、Planning、Coding、Direct Answering、final answer等SOP标识，以满足Agent的基本工作需求。 Plan and Executor：增加LLM的Tool使用、Agent调度、代码的生成。设置了几种基本链路，例如： a. 单轮问答，也可以扩展到CoT、ToT、GoT等形式。 b. ReAct，基础的响应决策过程，模型设置SOP 状态以终止循环 c. TaskPlaning - Executor，任务完成即可结束 Long-short term memory Management：Multi-Agent与单Agent的关键区别在于，Multi-Agent需要处理大量的交流信息，类似人类团队协作的过程。增加一个专门负责内容总结（类似于会议助理）的Agent，对长期记忆进行总结并提更有效信息传递给下一位Agent，而非传递所有内容给下一位Agent。 Human-agent interaction：面对复杂场景时，需要人类介入Agent交互过程并提供反馈。通过上述 Long-short term memory Management 和 Agent Communication 过程，使LLM能准确理解人类的意图，从而更有效地完成任务。 总的来说，这五个要素共同构建了一个Multi Agent框架，确保Agent之间的协作更加紧密和高效，同时也能够适应更复杂的任务需求和更多样的交互场景。通过组合多个Agent链路来实现一个完整且复杂的项目上线场景（Dev Phase），如Demand Chain（CEO）、Product Arguement Chain（CPO、CFO、CTO）、Engineer Group Chain（Selector、Developer1~N）、QA Engineer Chain（Developer、Tester）、Deploy Chain（Developer、Deploer）。</description>
    </item>
    <item>
      <title>CodeFuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/zh/docs/codefuse-chatbot-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-chatbot-zh/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp DevOps-ChatBot是由蚂蚁CodeFuse团队开发的开源AI智能助手，致力于简化和优化软件开发生命周期中的各个环节。该项目结合了Multi-Agent的协同调度机制，并集成了丰富的工具库、代码库、知识库和沙盒环境，使得LLM模型能够在DevOps领域内有效执行和处理复杂任务。&#xA;📜 目录 🤝 介绍 🎥 演示视频 🧭 技术路线 🤝 介绍 💡 本项目旨在通过检索增强生成（Retrieval Augmented Generation，RAG）、工具学习（Tool Learning）和沙盒环境来构建软件开发全生命周期的AI智能助手，涵盖设计、编码、测试、部署和运维等阶段。 逐渐从各处资料查询、独立分散平台操作的传统开发运维模式转变到大模型问答的智能化开发运维模式，改变人们的开发运维习惯。&#xA;本项目核心差异技术、功能点：&#xA;🧠 智能调度核心： 构建了体系链路完善的调度核心，支持多模式一键配置，简化操作流程。 使用说明 💻 代码整库分析： 实现了仓库级的代码深入理解，以及项目文件级的代码编写与生成，提升了开发效率。 📄 文档分析增强： 融合了文档知识库与知识图谱，通过检索和推理增强，为文档分析提供了更深层次的支持。 🔧 垂类专属知识： 为DevOps领域定制的专属知识库，支持垂类知识库的自助一键构建，便捷实用。 🤖 垂类模型兼容： 针对DevOps领域的小型模型，保证了与DevOps相关平台的兼容性，促进了技术生态的整合。 🌍 依托于开源的 LLM 与 Embedding 模型，本项目可实现基于开源模型的离线私有部署。此外，本项目也支持 OpenAI API 的调用。接入Demo&#xA;👥 核心研发团队长期专注于 AIOps + NLP 领域的研究。我们发起了 Codefuse-ai 项目，希望大家广泛贡献高质量的开发和运维文档，共同完善这套解决方案，以实现“让天下没有难做的开发”的目标。&#xA;🎥 演示视频 为了帮助您更直观地了解 Codefuse-ChatBot 的功能和使用方法，我们录制了一系列演示视频。您可以通过观看这些视频，快速了解本项目的主要特性和操作流程。&#xA;知识库导入和问答：演示视频 本地代码库导入和问答：演示视频 🧭 技术路线 🧠 Multi-Agent Schedule Core: 多智能体调度核心，简易配置即可打造交互式智能体。 🕷️ Multi Source Web Crawl: 多源网络爬虫，提供对指定 URL 的爬取功能，以搜集所需信息。 🗂️ Data Processor: 数据处理器，轻松完成文档载入、数据清洗，及文本切分，整合不同来源的数据。 🔤 Text Embedding &amp;amp; Index:：文本嵌入索引，用户可以轻松上传文件进行文档检索，优化文档分析过程。 🗄️ Vector Database &amp;amp; Graph Database: 向量与图数据库，提供灵活强大的数据管理解决方案。 📝 Prompt Control &amp;amp; Management:：Prompt 控制与管理，精确定义智能体的上下文环境。 🚧 SandBox:：沙盒环境，安全地执行代码编译和动作。 💬 LLM:：智能体大脑，支持多种开源模型和 LLM 接口。 🛠️ API Management:： API 管理工具，实现对开源组件和运维平台的快速集成。 具体实现明细见：技术路线明细</description>
    </item>
    <item>
      <title>CodeFuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/zh/docs/overview/codefuse-chatbot-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-chatbot-zh/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp DevOps-ChatBot是由蚂蚁CodeFuse团队开发的开源AI智能助手，致力于简化和优化软件开发生命周期中的各个环节。该项目结合了Multi-Agent的协同调度机制，并集成了丰富的工具库、代码库、知识库和沙盒环境，使得LLM模型能够在DevOps领域内有效执行和处理复杂任务。&#xA;📜 目录 🤝 介绍 🎥 演示视频 🧭 技术路线 🤝 介绍 💡 本项目旨在通过检索增强生成（Retrieval Augmented Generation，RAG）、工具学习（Tool Learning）和沙盒环境来构建软件开发全生命周期的AI智能助手，涵盖设计、编码、测试、部署和运维等阶段。 逐渐从各处资料查询、独立分散平台操作的传统开发运维模式转变到大模型问答的智能化开发运维模式，改变人们的开发运维习惯。&#xA;本项目核心差异技术、功能点：&#xA;🧠 智能调度核心： 构建了体系链路完善的调度核心，支持多模式一键配置，简化操作流程。 使用说明 💻 代码整库分析： 实现了仓库级的代码深入理解，以及项目文件级的代码编写与生成，提升了开发效率。 📄 文档分析增强： 融合了文档知识库与知识图谱，通过检索和推理增强，为文档分析提供了更深层次的支持。 🔧 垂类专属知识： 为DevOps领域定制的专属知识库，支持垂类知识库的自助一键构建，便捷实用。 🤖 垂类模型兼容： 针对DevOps领域的小型模型，保证了与DevOps相关平台的兼容性，促进了技术生态的整合。 🌍 依托于开源的 LLM 与 Embedding 模型，本项目可实现基于开源模型的离线私有部署。此外，本项目也支持 OpenAI API 的调用。接入Demo&#xA;👥 核心研发团队长期专注于 AIOps + NLP 领域的研究。我们发起了 Codefuse-ai 项目，希望大家广泛贡献高质量的开发和运维文档，共同完善这套解决方案，以实现“让天下没有难做的开发”的目标。&#xA;🎥 演示视频 为了帮助您更直观地了解 Codefuse-ChatBot 的功能和使用方法，我们录制了一系列演示视频。您可以通过观看这些视频，快速了解本项目的主要特性和操作流程。&#xA;知识库导入和问答：演示视频 本地代码库导入和问答：演示视频 🧭 技术路线 🧠 Multi-Agent Schedule Core: 多智能体调度核心，简易配置即可打造交互式智能体。 🕷️ Multi Source Web Crawl: 多源网络爬虫，提供对指定 URL 的爬取功能，以搜集所需信息。 🗂️ Data Processor: 数据处理器，轻松完成文档载入、数据清洗，及文本切分，整合不同来源的数据。 🔤 Text Embedding &amp;amp; Index:：文本嵌入索引，用户可以轻松上传文件进行文档检索，优化文档分析过程。 🗄️ Vector Database &amp;amp; Graph Database: 向量与图数据库，提供灵活强大的数据管理解决方案。 📝 Prompt Control &amp;amp; Management:：Prompt 控制与管理，精确定义智能体的上下文环境。 🚧 SandBox:：沙盒环境，安全地执行代码编译和动作。 💬 LLM:：智能体大脑，支持多种开源模型和 LLM 接口。 🛠️ API Management:： API 管理工具，实现对开源组件和运维平台的快速集成。 具体实现明细见：技术路线明细</description>
    </item>
    <item>
      <title>CodeFuse-DevOps</title>
      <link>/zh/docs/codefuse-devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops/</guid>
      <description>CodeFuse-DevOps CodeFuse-DevOps</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Eval</title>
      <link>/zh/docs/codefuse-devops-eval-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops-eval-zh/</guid>
      <description>codefuse-devops-eval codefuse-devops-eval</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Eval</title>
      <link>/zh/docs/overview/codefuse-devops-eval-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-devops-eval-zh/</guid>
      <description>DevOps-Eval是一个专门为DevOps领域大模型设计的综合评估数据集。我们希望DevOps-Eval能够帮助开发者，尤其是DevOps领域的开发者，追踪进展并分析他们拥有的DevOps大模型的优势和不足之处。&#xA;📚 该仓库包含与DevOps和AIOps相关的问题和练习, 还添加了关于ToolLearning相关的样本。&#xA;💥 目前有 7486 个多项选择题，根据DevOps的通用流程将其归纳未8个模块，如下图所示。&#xA;🔥 AIOps样本总计 2840 个，覆盖的场景包括日志解析、时序异常检测、时序分类、时序预测和根因分析。&#xA;🔧 ToolLearning样本 1509 个，涵盖59个领域，总计 239 种工具类别。&#xA;🏆 排行榜 以下是我们获得的初版评测结果，包括多个开源模型的zero-shot和five-shot准确率。我们注意到，对于大多数指令模型来说，five-shot的准确率要优于zero-shot。&#xA;👀 DevOps Zero Shot 模型 plan code build test release deploy operate monitor 平均分 DevOpsPal-14B-Chat 60.61 78.35 84.86 84.65 87.26 82.75 69.89 79.17 78.23 DevOpsPal-14B-Base 54.55 77.82 83.49 85.96 86.32 81.96 71.18 82.41 78.23 Qwen-14B-Chat 60.61 75.4 85.32 84.21 89.62 82.75 69.57 80.56 77.18 Qwen-14B-Base 57.58 73.81 84.4 85.</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Model</title>
      <link>/zh/docs/codefuse-devops-model-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops-model-zh/</guid>
      <description>codeFuse-devops-model codeFuse-devops-model</description>
    </item>
    <item>
      <title>CodeFuse-DevOps-Model</title>
      <link>/zh/docs/overview/codefuse-devops-model-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-devops-model-zh/</guid>
      <description>codeFuse-devops-model DevOps-Model 是蚂蚁集团联合北京大学发布面向中文 DevOps 领域的大语言模型，通过收集 DevOps 领域相关的专业数据，再针对模型进行语言模型的加训和对齐训练，产出可以帮助工程师在整个开发运维生命周期提效的大模型。弥补当前大模型在 DevOps 领域的缺失，旨在做到有问题，问 DevOps-Model !&#xA;当前我们已经开源了 7B 和 14B 两种规格的经过加训得 Base 模型和经过对齐后的 Chat 模型，同时还开源了对应的训练代码，欢迎大家一起合作建设！&#xA;项目地址 Github 地址：https://github.com/codefuse-ai/CodeFuse-DevOps-Model/tree/main&#xA;ModelScope 地址：&#xA;DevOps-Model-7B-Base：https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Base/summary DevOps-Model-7B-Chat：https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat/summary DevOps-Model-14B-Base：https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Base/summary DevOps-Model-14B-Chat：https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Chat/summary 评测考题 针对模型评测，最初并没有这样的一个 benchmark 用来 DevOps 领域进行测试，所以我们首先选用了一些通用开源测试中和 DevOps 领域相关的选择题进行测试，具体测试数据如下：&#xA;数据集 考试科目 题目总数 CMMLU Computer science&#x9;204 Computer security 171 Machine learning 122 CEval college programming 37 CEval computer_architecture 21 CEval computer_network 19 总计 总计题目数 574 评测方式 由于都是单选题,我们采用的是选取模型产出的第一个 Token 中四个选项 Token 中得分最高的作为模型对于问题的回答。同时我们还测试了 Zero-shot 和 Five-shot 的结果。</description>
    </item>
    <item>
      <title>CodeFuse-MFT-VLM</title>
      <link>/zh/docs/overview/codefuse-mft-vlm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-mft-vlm/</guid>
      <description>CodeFuse-VLM CodeFuse-VLM 是一个多模态大语言模型框架，该框架为用户提供多种视觉编码器，模态对齐模块和大语言模型的选择，以适配用户对不同任务的需求。&#xA;随着huggingface开源社区的不断更新，会有更多的vision encoder 和 LLM 底座发布，这些vision encoder 和 LLM底座都有各自的强项，例如 code-llama 适合生成代码类任务，但是不适合生成中文类的任务；因此我们搭建了CodeFuse-VLM 框架，支持多种视觉模型和语言大模型，使得CodeFuse-VLM可以适应不同种类的任务。&#xA;我们在CodeFuse-VLM 框架下, 使用Qwen-VL的视觉编码器, cross attention模态对齐模块, 和 Qwen-14B 模型训练了 CodeFuse-VLM-14B&#xA;CodeFuse-VLM-14B 在多个benchmarks 上的性能超过了Qwen-VL和LLAVA-1.5 各个模型得分如下表所示:&#xA;模型 MMBench MMBench-CN VqaV2 GQA TextVQA Vizwiz LLAVA-1.5 67.7 63.6 80.0 63.3 61.3 53.6 Qwen-VL 60.6 56.7 78.2 57.5 63.8 38.9 CodeFuse-VLM-14B 75.7 69.8 79.3 59.4 63.9 45.3 我们的模型在MMBenchmark 多模态大模型榜单上取得了很高的排名: https://mmbench.opencompass.org.cn/leaderboard&#xA;这是我们模型的展示视频&#xA;https://private-user-images.githubusercontent.com/22836551/300386230-8e64f615-ac0e-447e-9695-c96b254d484f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDY1MjExODksIm5iZiI6MTcwNjUyMDg4OSwicGF0aCI6Ii8yMjgzNjU1MS8zMDAzODYyMzAtOGU2NGY2MTUtYWMwZS00NDdlLTk2OTUtYzk2YjI1NGQ0ODRmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTI5VDA5MzQ0OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ5NzNjM2U1ZWU4NDU0Yzc5NmE4ZTM1NzY2ZjU4YjRjY2ZhNjMzODk0ZDgzMDg4N2FjYjZhYTllM2E3NTAyMWQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.pr-ad7rKYBgk26DTItj2q2q9I5dRWnBNHbV9M7GSVCo</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/zh/docs/codefuse-modelcache-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-modelcache-zh/</guid>
      <description>CodeFuse-ModelCache CodeFuse-ModelCache</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/zh/docs/overview/codefuse-modelcache-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-modelcache-zh/</guid>
      <description>中文 | English Contents 新闻 项目简介 架构大图 致谢 Contributing 新闻 🔥🔥[2023.12.10] 增加llmEmb、onnx、paddlenlp、fasttext等LLM embedding框架，并增加timm 图片embedding框架，用于提供更丰富的embedding能力。 🔥🔥[2023.11.20] codefuse-ModelCache增加本地存储能力, 适配了嵌入式数据库sqlite、faiss，方便用户快速启动测试。 [2023.10.31] codefuse-ModelCache&amp;hellip; 项目简介 Codefuse-ModelCache 是一个开源的大模型语义缓存系统，通过缓存已生成的模型结果，降低类似请求的响应时间，提升用户体验。该项目从服务优化角度出发，引入缓存机制，在资源有限和对实时性要求较高的场景下，帮助企业和研究机构降低推理部署成本、提升模型性能和效率、提供规模化大模型服务。我们希望通过开源，分享交流大模型语义Cache的相关技术。&#xA;架构大图 致谢 本项目参考了以下开源项目，在此对相关项目和研究开发人员表示感谢。&#xA;GPTCache&#xA;Contributing ModelCache是一个非常有趣且有用的项目，我们相信这个项目有很大的潜力，无论你是经验丰富的开发者，还是刚刚入门的新手，都欢迎你为这个项目做出一些贡献，包括但不限于：提交问题和建议，参与代码编写，完善文档和示例。你的参与将会使这个项目变得更好，同时也会为开源社区做出贡献。</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/zh/docs/codefuse-query-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-query-zh/</guid>
      <description>CodeFuse-Query CodeFuse-Query</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/zh/docs/overview/codefuse-query-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/codefuse-query-zh/</guid>
      <description>CodeFuse-Query 随着大规模软件开发的普及，对可扩展且易于适应的静态代码分析技术的需求正在加大。传统的静态分析工具，如 Clang Static Analyzer (CSA) 或 PMD，在检查编程规则或样式问题方面已经展现出了良好的效果。然而，这些工具通常是为了满足特定的目标而设计的，往往无法满足现代软件开发环境中多变和多元化的需求。这些需求可以涉及服务质量 (QoS)、各种编程语言、不同的算法需求，以及各种性能需求。例如，安全团队可能需要复杂的算法，如上下文敏感的污点分析，来审查较小的代码库，而项目经理可能需要一种相对较轻的算法，例如计算圈复杂度的算法，以在较大的代码库上测量开发人员的生产力。&#xA;这些多元化的需求，加上大型组织中常见的计算资源限制，构成了一项重大的挑战。由于传统工具采用的是问题特定的计算方式，往往无法在这种环境中实现扩展。因此，我们推出了 CodeQuery，这是一个专为大规模静态分析设计的集中式数据平台。 在 CodeQuery 的实现中，我们把源代码和分析结果看作数据，把执行过程看作大数据处理，这与传统的以工具为中心的方法有着显著的不同。我们利用大型组织中的常见系统，如数据仓库、MaxCompute 和 Hive 等数据计算设施、OSS 对象存储和 Kubernetes 等灵活计算资源，让 CodeQuery 能够无缝地融入这些系统中。这种方法使 CodeQuery 高度可维护和可扩展，能够支持多元化的需求，并有效应对不断变化的需求。此外，CodeQuery 的开放架构鼓励各种内部系统之间的互操作性，实现了无缝的交互和数据交换。这种集成和交互能力不仅提高了组织内部的自动化程度，也提高了效率，降低了手动错误的可能性。通过打破信息孤岛，推动更互联、更自动化的环境，CodeQuery 显著提高了软件开发过程的整体生产力和效率。 此外，CodeQuery 的以数据为中心的方法在处理静态源代码分析的领域特定挑战时具有独特的优势。例如，源代码通常是一个高度结构化和互联的数据集，与其他代码和配置文件有强烈的信息和连接。将代码视为数据，CodeQuery 可以巧妙地处理这些问题，这使得它特别适合在大型组织中使用，其中代码库持续但逐步地进行演变，大部分代码在每天进行微小的改动同时保持稳定。 CodeQuery 还支持如基于代码数据的商业智能 (BI) 这类用例，能生成报告和仪表板，协助监控和决策过程。此外，CodeQuery 在分析大型语言模型 (LLM) 的训练数据方面发挥了重要作用，提供了增强这些模型整体效果的深入见解。&#xA;在当前的静态分析领域，CodeQuery 带来了一种新的范式。它不仅满足了大规模、复杂的代码库分析需求，还能适应不断变化和多元化的静态分析场景。CodeQuery 的以数据为中心的方法，使得其在处理大数据环境中的代码分析问题时具有独特优势。CodeQuery 的设计，旨在解决大规模软件开发环境中的静态分析问题。它能够将源代码和分析结果视作数据，使得其可以灵活地融入大型组织的各种系统中。这种方法不仅可以有效地处理大规模的代码库，还可以应对各种复杂的分析需求，从而使得静态分析工作变得更加高效和准确。&#xA;CodeQuery 的特点和优势可以概括为以下几点：&#xA;高度可扩展：CodeQuery 可以处理大规模的代码库，且能够适应不同的分析需求。这种高度的可扩展性使得 CodeQuery 可以在大型组织中发挥重要作用。 以数据为中心：CodeQuery 将源代码和分析结果视作数据，这种以数据为中心的方法使其在处理大数据环境中的代码分析问题时具有独特优势。 高度集成：CodeQuery 能够无缝地融入大型组织的各种系统中，包括数据仓库、数据计算设施、对象存储和灵活计算资源等。这种高度的集成性使得 CodeQuery 在大型组织中的使用变得更加方便和高效。 支持多元化的需求：CodeQuery 不仅可以处理大规模的代码库，还可以应对各种复杂的分析需求，包括服务质量分析需求、跨编程语言分析需求、算法需求和性能需求等。 CodeQuery 是一种强大的静态代码分析平台，适合大规模、复杂的代码库分析场景。它的以数据为中心的方法和高度的可扩展性使得它在现代软件开发环境中具有独特的优势。未来，随着静态代码分析技术的不断发展，CodeQuery 有望在这个领域中扮演更加重要的角色。</description>
    </item>
    <item>
      <title>CodeFuse-Query 介绍</title>
      <link>/zh/docs/codefuse-query-introduction-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-query-introduction-zh/</guid>
      <description>概述 CodeFuse-Query 是一个支持对 各种编程语言 进行 结构化分析 的 代码数据平台。核心思想是利用各种语言解析器将所有代码转化为数据，并将其结构化存储到代码数据库中。通过使用自定义查询语言，按照业务需求进行数据分析。如下图所示： 2.1 CodeFuse-Query的架构 从整体上来说，CodeFuse-Query代码数据平台分为三大部分：代码数据模型、代码查询DSL、平台产品化服务。主要工作流程如下图所示：&#xA;代码数据化和标准化：COREF 我们定义了一种代码数据化和标准化的模型：COREF，要求所有代码都要能通过各种语言抽取器转化到该模型。 COREF主要包含以下几种信息： COREF = AST （抽象语法树） + ASG（抽象语义图） + CFG（控制流图） + PDG（程序依赖图）+ Call Graph（函数调用图） + Class Hierarchy （类继承关系）+ Documentation（文档/注释信息） 注：由于每种信息的计算难度不一，所以并不是所有语言的COREF信息均包含以上全部信息，基础信息主要有AST、ASG、Call Graph、Class Hierarchy和Documentation，其他信息（ CFG 和 PDG ）仍在建设中，后续会逐步支持。&#xA;代码查询DSL 基于生成的COREF代码数据，CodeFuse-Query 使用一种自定义的DSL语言 Gödel 来进行查询，从而完成代码分析需求。 Gödel是一种逻辑推理语言，它的底层实现是基于逻辑推理语言Datalog，通过描述“事实”和“规则”， 程序可以不断地推导出新的事实。Gödel也是一个声明式语言，相较于命令式编程，声明式编程更加着重描述“要什么”，而把如何实现交给计算引擎。 既然代码已经转化为关系型数据（COREF数据以关系型数据表的形式存储），相信大家会有疑问，为什么不直接用SQL，或者是直接使用SDK，而是又要专门去学习一个新的DSL语言呢？因为Datalog的计算具备单调性和终止性，简单理解就是，Datalog是在牺牲了表达能力的前提下获得了更高的性能，而Gödel继承了这个特点。&#xA;相比较SDK，Gödel的主要优点是易学易用，声明式的描述，用户不需要关注中间的运算过程，只需要像SQL一样简单描述清楚需求即可。 相比较SQL，Gödel的优点主要是描述能力更强、计算速度更快，例如描述递归算法和多表联合查询，而这些对于SQL来说都是比较困难的。 平台化、产品化 CodeFuse-Query 包括Sparrow CLI 和CodeFuse-Query在线服务Query中心。Sparrow CLI包含了所有组件和依赖，例如抽取器，数据模型，编译器等，用户完全可以通过使用Sparrow CLI在本地进行代码数据生成和查询（Sparrow CLI的使用方式请见 第3节 安装、配置、运行）。如果用户有在线查询的需求，可以使用Query中心进行实验。&#xA;2.2 CodeFuse-Query支持的分析语言 截至2023-10-31为止，CodeFuse-Query支持对11种编程语言进行数据分析。其中对5种编程语言（ Java、JavaScript、TypeScript、XML、Go ）的支持度非常成熟，对剩余6种编程语言（Object-C、C++、Python3、Swift、SQL、Properties ）的支持度处于beta阶段，还有进一步提升和完善的空间，具体的支持情况见下表：&#xA;语言 状态 COREF模型节点数 Java 成熟 162 XML 成熟 12 TS/JS 成熟 392 Go 成熟 40 OC/C++ beta 53/397 Python3 beta 93 Swift beta 248 SQL beta 750 Properties beta 9 注：以上语言状态的成熟程度判断标准是根据COREF包含的信息种类和实际落地情况来进行判定，除了OC/C++外，所有语言均支持了完整的AST信息和Documentation信息，以Java为例，COREF for Java还支持了ASG、Call Graph、Class Hierarchy、以及部分CFG信息。</description>
    </item>
    <item>
      <title>CodeFuseEval: 代码大语言模型的多任务评估基准</title>
      <link>/zh/docs/overview/b10.codefuse-evalution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/b10.codefuse-evalution/</guid>
      <description>English｜&#xD;CodeFuseEval on ModelScope｜&#xD;CodeFuseEval on Hugging Face&#xD;CodeFuseEval在HumanEval-x、MBPP的基准上，结合CodeFuse大模型多任务场景，开发的编程领域多任务的评测基准， 可用于评估模型在代码补全，自然语言生成代码，测试用例生成、跨语言代码翻译，中文指令生成代码等多类任务的性能。持续开放中，敬请期待！</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/zh/coagent/connector-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-agent-zh/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.agents import BaseAgent&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选一个role来做示例 # 从已有的配置中选择一个config，具体参数细节见下面&#xD;role_configs = load_role_configs(AGETN_CONFIGS)&#xD;agent_config = role_configs[&amp;#34;general_planner&amp;#34;]&#xD;# 生成agent实例&#xD;base_agent = BaseAgent(&#xD;role=agent_config.</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/zh/muagent/connector-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-agent-zh/</guid>
      <description>快速构建一个Agent 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) Agent 配置 定义两个react agent，进行实际任务执行 # 这里采用了预定义的prompt，也可以参考上述prompt完成编写&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/zh/coagent/connector-chain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-chain-zh/</guid>
      <description>快速构建一个 agent chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） # 设置openai的api-key&#xD;import os, sys&#xD;import openai&#xD;import importlib&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的agent配置选多个role组合成 agent chain from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/zh/muagent/connector-chain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-chain-zh/</guid>
      <description>快速构建一个Agent Chain 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/zh/coagent/connector-memory-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-memory-zh/</guid>
      <description>Memory Manager 主要用于 chat history 的管理，暂未完成&#xA;将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 创建 memory manager 实例 import os&#xD;import openai&#xD;from coagent.base_configs.env_config import KB_ROOT_PATH&#xD;from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.schema import Message&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/zh/muagent/connector-memory-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-memory-zh/</guid>
      <description>Memory Manager 将chat history在数据库进行读写管理，包括user input、 llm output、doc retrieval、code retrieval、search retrieval 对 chat history 进行关键信息总结 summary context，作为 prompt context 提供检索功能，检索 chat history 或者 summary context 中与问题相关信息，辅助问答 使用示例 完整示例见 ~/tests/connector/memory_manager_test.py&#xA;创建 memory manager 实例 import os&#xD;import openai&#xD;from muagent.base_configs.env_config import KB_ROOT_PATH&#xD;from muagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.schema import Message&#xD;#&#xD;OPENAI_API_BASE = &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;os.environ[&amp;#34;model_name&amp;#34;] = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;# os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/zh/coagent/connector-phase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-phase-zh/</guid>
      <description>快速构建一个 agent phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 配置相关 LLM 和 Embedding Model # LLM 和 Embedding Model 配置&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 这里从已有的 phase 配置中选一个 phase 来做示例 # log-level，print prompt和llm predict&#xD;os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/zh/muagent/connector-phase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-phase-zh/</guid>
      <description>快速构建一个Agent Phase 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动） import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 配置相关 LLM 和 Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.connector.chains import BaseChain&#xD;from muagent.connector.schema import Role, Message, ChainConfig&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/zh/coagent/connector-prompt-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/connector-prompt-zh/</guid>
      <description>Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.&#xD;If it&amp;#39;s &amp;#39;continued&amp;#39;, the context cant answer the origin query.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/zh/muagent/connector-prompt-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/connector-prompt-zh/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Prompt 的标准结构 在整个Prompt的整个结构中，我们需要去定义三个部分&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/zh/coagent/customed-examples-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/customed-examples-zh/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 autogen 的 auto_feedback_from_code_execution 构建过来，来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 import os, sys, requests&#xD;# from configs.model_config import *&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.chains import BaseChain&#xD;from coagent.connector.schema import Message&#xD;from coagent.connector.configs import AGETN_CONFIGS, CHAIN_CONFIGS, PHASE_CONFIGS&#xD;import importlib&#xD;# update new agent configs&#xD;auto_feedback_from_code_execution_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;You are a helpful AI assistant. Solve tasks using your coding and language skills.&#xD;In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/zh/muagent/custom-examples-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-examples-zh/</guid>
      <description>如何创建你个性化的 agent phase 场景 下面通过 代码库来实现代码转API文档的自动生成， 来详细演示如何自定义一个 agent phase 的构建&#xA;设计你的prompt结构 codeGenDocGroup_PROMPT, 构建 group Agent Prompt # update new agent configs&#xD;codeGenDocGroup_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;Your goal is to response according the Context Data&amp;#39;s information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.&#xD;When you need to select the appropriate role for handling a user&amp;#39;s query, carefully read the provided role names, role descriptions and tool list.</description>
    </item>
    <item>
      <title>Embedding 配置</title>
      <link>/zh/muagent/embedding-model-config-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/embedding-model-config-zh/</guid>
      <description>准备相关参数 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动）&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34; 构建LLM Config 通过本地模型文件构建 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) 通过openai构建 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;openai&amp;#34;, api_key=api_key, api_base_url=api_base_url,&#xD;) 自定义langchain embeddings传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;class CustomizedEmbeddings(Embeddings):&#xD;def embed_documents(self, texts: List[str]) -&amp;gt; List[List[float]]:&#xD;embeddings = []&#xD;# add your embedding code&#xD;return embeddings&#xD;def embed_query(self, text: str) -&amp;gt; List[float]:&#xD;&amp;#34;&amp;#34;&amp;#34;Compute query embeddings using a HuggingFace transformer model.</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/zh/docs/fastertransformer4codefuse-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/fastertransformer4codefuse-zh/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/zh/docs/overview/fastertransformer4codefuse-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/fastertransformer4codefuse-zh/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>LLM 配置</title>
      <link>/zh/muagent/llm-model-config-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/llm-model-config-zh/</guid>
      <description>准备相关参数 首先增加openai配置，也可以是其它类似于openai接口的模型（通过fastchat启动）&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34; 构建LLM Config 通过调用 类openai 传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;) 自定义 langchain LLM 传入 from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from langchain.llms.base import BaseLLM, LLM&#xD;class CustomizedModel(LLM):&#xD;repetition_penalty = 1.1&#xD;temperature = 0.2&#xD;top_k = 40&#xD;top_p = 0.9&#xD;def predict(self, prompt: str, stop: Optional[List[str]] = None) -&amp;gt; str:&#xD;return self.</description>
    </item>
    <item>
      <title>MFTCoder</title>
      <link>/zh/docs/mftcoder-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/mftcoder-zh/</guid>
      <description>MFTCoder MFTCoder</description>
    </item>
    <item>
      <title>MFTCoder 介绍</title>
      <link>/docs/mftcoder-introduction-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-introduction-zh/</guid>
      <description>项目简介 国际首个高精度、高效率、多任务、多模型支持、多训练算法，大模型代码能力微调框架；&#xA;Codefuse-MFTCoder 是一个开源的多任务代码大语言模型项目，包含代码大模型的模型、数据、训练等。我们希望通过开源，分享交流大语言模型在代码领域的进步。&#xA;项目框架 项目优势 :white_check_mark: 多任务：一个模型同时支持多个任务，会保证多个任务之间的平衡，甚至可以泛化到新的没有见过的任务上去；&#xA;:white_check_mark: 多模型：支持最新的多个开源模型，包括gpt-neox，llama，llama-2，baichuan，Qwen，chatglm2等；&#xA;:white_check_mark: 多框架：既支持主流开源的Accelerate+DeepSpeed/FSDP，也支持新开源的ATorch 框架；&#xA;:white_check_mark: 高效微调：支持LoRA和QLoRA，可以用很少的资源去微调很大的模型，且训练速度能满足几乎所有微调场景；&#xA;本项目主要内容如下：&#xA;同时支持单任务SFT(Supervised FineTuning)和MFT(Multi-task FineTuning), 当前开源支持数据均衡，未来将持续开源难易均衡， 收敛均衡等 支持QLoRA低成本高效指令微调、LoRA高效指令微调、全量参数高精度微调。 支持绝大部分主流的开源大模型，重点关注代码能力优秀的开源大模型，如DeepSeek-coder, Mistral, Mistral(MoE), Chatglm3, Qwen, GPT-Neox, Starcoder, Codegeex2, Code-LLaMA等。 支持lora与base model进行权重合并，推理更便捷。 整理并开源2个指令微调数据集：Evol-instruction-66k和CodeExercise-Python-27k。 开源多个[Codefuse系列指令微调模型权重]，具体参见我们的huggingface组织和modelscope组织下的模型：codefuse-ai huggingface or codefuse-ai 魔搭。 </description>
    </item>
    <item>
      <title>MFTCoder: Accelerate &#43; DeepSpeed/FSDP 框架篇</title>
      <link>/docs/mftcoder-accelerate-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-accelerate-zh/</guid>
      <description>[中文] [English]&#xA;1. 更新 🔥 MFTCoder-accelerate 新增支持accelerate + FSDP框架， 支持全量微调和LoRA;&#xA;🔥 MFTCoder-accelerate 支持最新更多主流开源模型: mistral, mixtral-8x7b(Mixture of Experts), deepseek, chatglm3；&#xA;🔥 MFTCoder-accelerate 新增self-paced Loss, 用于收敛均衡；&#xA;🔥 MFTCoder-accelerate 支持使用accelerate + DeepSpeed框架下支持 全量参数/QLoRA/LoRA微调；&#xA;🔥 MFTCoder-accelerate 在训练中支持了多任务微调MFT， 可以同时平衡多个任务的训练，训练的模型支持多任务推理；&#xA;🔥 MFTCoder-accelerate 在训练中支持多种模型基座： codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwen等&#xA;2. 数据格式 2.1 训练数据格式 训练数据为jsonl格式，每一行的数据格式如下，其中chat_rounds字段是必需的，可以根据实际需求添加或删除其他字段。 可以参考项目中的xxx.jsonl文件。&#xA;{ &amp;#34;id&amp;#34;:0, &amp;#34;data_name&amp;#34;:&amp;#34;code-helper&amp;#34;, &amp;#34;chat_rounds&amp;#34;:[ { &amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;你是一个智能代码助手，可以回复用户与代码相关的问题&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;human&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;写一个快速排序&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;bot&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;以下是一个快速排序算法xxxxxx&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;human&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;解释一下这段代码&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;bot&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;好的，这段代码xxx&amp;#34; } ] } 2.</description>
    </item>
    <item>
      <title>MFTCoder: 高效准确的多任务大模型微调框架</title>
      <link>/zh/docs/overview/mftcoder-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/mftcoder-zh/</guid>
      <description>🤗 HuggingFace • 🤖 魔搭 [中文] [English]&#xA;目录 新闻 文章 项目简介 环境 训练 模型 数据集 新闻 🔥🔥🔥 [2024/01/17] MFTCoder-v0.3.0发布。新增对Mixtral(MoE), DeepSeek等模型的支持；新增支持FSDP(Fully Sharded Data Parallel)；新增Self-paced Loss, 支持多任务收敛均衡。 感兴趣详见微信公众号CodeFuse的文章MFTCoder 重磅升级v0.3.0发布&#xA;🔥🔥🔥 [2024/01/17] 开源了CodeFuse-DeepSeek-33B模型，在HumanEval pass@1(greedy decoding)上可以达到78.7%。该模型在Big Code榜单的结果近期发布，请关注公众号获取最新信息。&#xA;🔥🔥🔥 [2024/01/17] 开源了CodeFuse-Mixtral-8x7B模型，在HumanEval pass@1(greedy decoding)上可以达到56.1%。感兴趣详见微信公众号CodeFuse的文章MFTCoder提升Mixtral-8x7B混合专家模型的代码能力实践&#xA;🔥🔥 [2023/11/07] MFTCoder论文在Arxiv公布，介绍了多任务微调的技术细节。&#xA;🔥🔥 [2023/10/20] 开源了CodeFuse-QWen-14B模型，在HumanEval pass@1(greedy decoding)上可以达到48.8%。相比较与基座模型Qwen-14b提升16%。感兴趣详见微信公众号CodeFuse文章&#xA;🔥🔥 [2023/09/27] 开源了CodeFuse-StarCoder-15B模型，在HumanEval pass@1(greedy decoding)上可以达到54.9%。&#xA;🔥🔥 [2023/09/26] CodeFuse-CodeLlama-34B-4bits量化版本发布，量化后模型在HumanEval pass@1指标为73.8% (贪婪解码)。&#xA;🔥🔥 [2023/09/07]MFTCoder微调的模型CodeFuse-CodeLlama-34B在HumanEval Benchmarks的Python Pass@1 取得了74.4%（greedy decoding）的开源SOTA成绩。&#xA;🔥🔥 [2023/08/26]MFTCoder-v0.1.0 支持使用LoRA/QLoRA对Code Llama、Llama、Llama2、StarCoder、ChatGLM2、CodeGeeX2、Qwen和GPT-NeoX模型进行微调。&#xA;HumanEval表现 模型 HumanEval(Pass@1) 日期 CodeFuse-DeepSeek-33B 78.</description>
    </item>
    <item>
      <title>MFTCoder训练: Atorch框架篇</title>
      <link>/docs/mftcoder-atorch-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-atorch-zh/</guid>
      <description>[中文] [English]&#xA;1. 更新 🔥 MFTCoder在Atorch框架下支持GPTNeoX模型的微调；&#xA;🔥 MFTCoder支持全量的有监督微调；&#xA;🔥 MFTCoder支持LoRA微调；&#xA;2. 数据格式 2.1 训练数据格式 训练数据为jsonl格式，每一行的数据格式如下，其中chat_rounds字段是必需的，可以根据实际需求添加或删除其他字段。 可以参考项目中的xxx.jsonl文件。&#xA;{ &amp;#34;id&amp;#34;:0, &amp;#34;data_name&amp;#34;:&amp;#34;code-helper&amp;#34;, &amp;#34;chat_rounds&amp;#34;:[ { &amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;你是一个智能代码助手，可以回复用户与代码相关的问题&amp;#34;, &amp;#34;chat_round_id&amp;#34;: 0 }, { &amp;#34;role&amp;#34;: &amp;#34;human&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;写一个快速排序&amp;#34;, &amp;#34;chat_round_id&amp;#34;: 1 }, { &amp;#34;role&amp;#34;: &amp;#34;bot&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;以下是一个快速排序算法xxxxxx&amp;#34;, &amp;#34;chat_round_id&amp;#34;: 1 }, { &amp;#34;role&amp;#34;: &amp;#34;human&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;解释一下这段代码&amp;#34;, &amp;#34;chat_round_id&amp;#34;: 2 }, { &amp;#34;role&amp;#34;: &amp;#34;bot&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;好的，这段代码xxx&amp;#34;, &amp;#34;chat_round_id&amp;#34;: 2 } ] } 2.2 推理数据格式 推理数据格式为模型在训练数据格式下拼接的字符串形式，它也是推理时输入prompt拼接的方式：&#xA;&amp;#34;&amp;#34;&amp;#34; &amp;lt;|role_start|&amp;gt;system&amp;lt;|role_end|&amp;gt;这是System指令 &amp;lt;|role_start|&amp;gt;human&amp;lt;|role_end|&amp;gt;这是第1轮用户输入的问题 &amp;lt;|role_start|&amp;gt;bot&amp;lt;|role_end|&amp;gt;这是第1轮模型生成的内容&amp;lt;/s&amp;gt; &amp;lt;|role_start|&amp;gt;human&amp;lt;|role_end|&amp;gt;这是第2轮用户输入的问题 &amp;lt;|role_start|&amp;gt;bot&amp;lt;|role_end|&amp;gt;这是第2轮模型生成的内容&amp;lt;/s&amp;gt; .</description>
    </item>
    <item>
      <title>MuAgent 概览</title>
      <link>/zh/muagent/muagent-%E6%A6%82%E8%A7%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/muagent-%E6%A6%82%E8%A7%88/</guid>
      <description>简介 为了提高大型模型在推理准确性方面的表现，业界出现了多种创新的大型语言模型(LLM)玩法。从最早的CoT、ToT到GoT，这些方法不断拓展了LLM的能力边界。在处理复杂问题时，我们可以通过ReAct过程来选择、调用和执行工具反馈，同时实现多轮工具使用和多步骤执行。&#xA;但对于更复杂的场景，例如复杂代码的开发，单一功能的LLM Agent显然难以胜任。因此，社区开始发展出多Agent的组合玩法，比如专注于metaGPT、GPT-Engineer、chatDev等开发领域的项目，以及专注于自动化构建Agent和Agent对话的AutoGen项目。&#xA;经过对这些框架的深入分析，发现大多数的Agent框架整体耦合度较高，其易用性和可扩展性较差。在预设场景中实现特定场景，但想要进行场景扩展却困难重重。&#xA;因此，我们希望构建一个可扩展、易于使用的Multi-Agent框架，以支持ChatBot在获取知识库信息的同时，能够辅助完成日常办公、数据分析、开发运维等各种通用任务。&#xA;本项目的Mutli-Agent框架汲取兼容了多个框架的优秀设计，比如metaGPT中的消息池（message pool）、autogen中的代理选择器（agent selector）等。&#xA;MuAgent框架 在MuAgent中，我们除了定义Agent交互链路和AgentBase基础执行流以外，还额外设计了 Prompt Manager 和 Memory Manager 两个基础组件，分别用于自动化构建Prompt和chat history管理。最终构建出一个可扩展、易于使用的Multi-Agent框架，包括以下内容&#xA;Agent Base：构建了四种基本的Agent类型BaseAgent、ReactAgent、ExecutorAgent、SelectorAgent，支撑各种场景的基础活动 Communication：通过Message和Parse Message 实体完成Agent间的信息传递，并与Memory Manager交互再Memory Pool完成记忆管理 Prompt Manager：通过Role Handler、Doc/Tool Handler、Session Handler、Customized Handler，来自动化组装Customized 的Agent Prompt Memory Manager： 用于支撑 chat history 的存储管理、信息压缩、记忆检索等管理，最后通过Memory Pool在数据库、本地、向量数据库中完成存储 Component：用于构建Agent的辅助生态组件，包括Retrieval、Tool、Action、Sandbox等 Customized Model：支持私有化的LLM和Embedding的接入 Agent Base 在Agent层面，提供四种基本的Agent类型，对这些Agent进行Role的基础设定，可满足多种通用场景的交互和使用。所有的Action都由Agent执行。&#xA;BaseAgent：提供基础问答、工具使用、代码执行的功能，根据Prompt格式实现 输入 =&amp;gt; 输出 ReactAgent：提供标准React的功能，根据问题实现当前任务 ExecutorAgent：对任务清单进行顺序执行，根据 User 或 上一个Agent编排的计划，完成相关任务 Agent接受到任务清单([List[task])，对这个任务清单Task进行循环执行（中间也可添加 Feedback Agent来进行任务重新优化），直到任务完成 SelectorAgent：提供选择Agent的功能，根据User 或 上一个 Agent的问题选择合适的Agent来进行回答. Communication 为了让Agent之间进行更好的交互，以及能够让每一个Agent接受到足够的信息完成它们特定任务，我们将Message信息体分成了多个部分，System Content、Info Content、LLM Content和LLM Parsed Content等&#xA;System Content：用于存储管理当前LLM输出的时间，Role信息等 Info Content：LLM辅助信息，比如像知识库查询信息、代码库检索信息、工具信息、Agent信息等 LLM Content：直接存储和传递LLM 产生的信息 LLM Parsed Content：对LLM进行解析转成更易操作的key-value数据结构，方便对LLM内容进行过滤 Customized Content：用于管理自定义action产生的key-value数据内容，用于后续自定义Prompt模板的组装构建 通过对以上消息格式的定义，我们便可以完成通用消息的传递和管理。具体组装见Prompt Manager模块</description>
    </item>
    <item>
      <title>Prompt 管理器</title>
      <link>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/prompt-%E7%AE%A1%E7%90%86%E5%99%A8/</guid>
      <description>提示管理器（Prompt Manager） 管理多智能体链路中的prompt创建&#xA;快速配置：采用预设的处理函数，用户仅需通过定义智能体的输入输出即可轻松配置，实现多智能体的prompt快速组装和配置。 自定义支持：允许用户自定义prompt内部各模块的处理逻辑，以达到个性化的智能体prompt实现。 Prompt预设模板结构 Agent Profile：此部分涉及到智能体的基础描述，包括但不限于代理的类型、功能和指令集。用户可以在这里设置智能体的基本属性，确保其行为与预期相符。 Context：上下文信息，给智能体做参考，帮助智能体更好的进行决策。 Tool Information：此部分为智能体提供了一套可用工具的清单，智能体可以根据当前的场景需求从中挑选合适的工具以辅助其执行任务。 Reference Documents：这里可以包含代理参考使用的文档或代码片段，以便于它在处理请求时能够参照相关资料。 Session Records：在进行多轮对话时，此部分会记录之前的交谈内容，确保智能体能够在上下文中保持连贯性。 Response Output Format：用户可以在此设置智能体的输出格式，以确保生成的响应满足特定的格式要求，包括结构、语法等。 Response：在与智能体的对话中，如果用户希望智能体继续某个话题或内容，可以在此模块中输入续写的上文。例如，在运用REACT模式时，可以在此区域内详细阐述智能体先前的行为和观察结果，以便于智能体构建连贯的后续响应。 Prompt自定义配置 Prompt模块参数 field_name：唯一的字段名称标识，必须提供。 function：指定如何处理输入数据的函数，必须提供。 title：定义模块的标题。若未提供，将自动生成一个标题，该标题通过把字段名称中的下划线替换为空格并将每个单词的首字母大写来构建。 description：提供模块的简要描述，位于模块最上方（标题下方）。默认为空，可选填。 is_context：标识该字段是否属于上下文模块的一部分。默认为True，意味着除非显式指定为False，否则都被视为上下文的一部分。 omit_if_empty：设定当模块内容为空时，是否在prompt中省略该模块，即不显示相应的模板标题和内容。默认为False，意味着即使内容为空也会显示标题。如果希望内容为空时省略模块，需显式设置为True。 Prompt配置示例 Prompt配置由一系列定义prompt模块的字典组成，这些模块将根据指定的参数和功能来处理输入数据并组织成一个完整的prompt。&#xA;在配置中，每个字典代表一个模块，其中包含相关的参数如 field_name, function_name, is_context, title, description, 和 omit_if_empty，用以控制模块的行为和呈现方式。&#xA;context_placeholder 字段用于标识上下文模板的位置，允许在prompt中插入动态内容。&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-modelcache-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-quickstart-zh/</guid>
      <description>ModelCache易于使用，只需1步骤即可构建缓存测试Demo&#xA;快速开始 构建Cache Cache的默认接口如下所示：&#xA;class Cache:&#xD;# it should be called when start the cache system&#xD;def __init__(self):&#xD;self.has_init = False&#xD;self.cache_enable_func = None&#xD;self.embedding_func = None&#xD;self.post_process_messages_func = None&#xD;self.config = Config() 在创建ModelCache之前，请考虑以下问题：&#xA;你将如何为查询生成嵌入向量？（embedding_func） 该函数将文本嵌入到一个用于上下文相似性搜索的密集向量中。ModelCache可以支持多种嵌入上下文的方法：Huggingface、ONNX和SentenceTransformers。默认逻辑中，使用了在中文领域表现更好的huggingface中的text2vec模型。只需将你的嵌入函数初始化为：text2vec.to_embeddings data_manager = get_data_manager(CacheBase(&amp;#34;mysql&amp;#34;, config=mysql_config),&#xD;VectorBase(&amp;#34;milvus&amp;#34;, dimension=data2vec.dimension, milvus_config=milvus_config))&#xD;cache.init(&#xD;embedding_func=data2vec.to_embeddings,&#xD;data_manager=data_manager,&#xD;similarity_evaluation=SearchDistanceEvaluation(),&#xD;query_pre_embedding_func=query_multi_splicing,&#xD;insert_pre_embedding_func=insert_multi_splicing,&#xD;) 你将在哪里缓存数据？（data_manager缓存存储） 缓存存储用于存储所有标量数据，例如原始问题、提示、答案和访问时间。ModelCache支持多种缓存存储选项，如SQLite、MySQL和OceanBase。未来还将添加更多的NoSQL数据库选项。 你将在哪里存储和搜索向量嵌入？（data_manager向量存储） 向量存储组件用于存储和搜索所有嵌入向量，以便在语义上找到最相似的结果。ModelCache支持使用FAISS等向量搜索库或Milvus等向量数据库。未来还将添加更多的向量数据库和云服务选项。 以下是一些示例：&#xA;data_manager = get_data_manager(CacheBase(&amp;#34;sqlite&amp;#34;), VectorBase(&amp;#34;faiss&amp;#34;, dimension=data2vec.dimension))&#xD;data_manager = get_data_manager(CacheBase(&amp;#34;oceanbase&amp;#34;), VectorBase(&amp;#34;milvus&amp;#34;, dimension=data2vec.dimension)) </description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/mftcoder-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-quickstart-zh/</guid>
      <description>环境 首先, 你需要将CUDA(&amp;gt;=11.4, 推荐11.7)及其相关驱动安装成功，并确保其工作正常, 并且安装基本的torch（&amp;gt;=2.0.0） 在requirements.txt下固定了几个主要的python包的版本，执行如下脚本即可：&#xA;sh init_env.sh 我们强烈建议您安装flash attention（&amp;gt;=2.1.0, 推荐2.3.6）, 安装请参考 https://github.com/Dao-AILab/flash-attention&#xA;训练 如果你熟悉大模型训练的各种主流开源资源，例如 transformers, DeepSpeed, FSDP等， 为了用开源项目快速上手高性能微调，我们建议您尝试：&#xA;🚀🚀 MFTCoder-accelerate: Accelerate + DeepSpeed/FSDP Codebase for MFT(Multi-task Finetuning)&#xA;如果你想探索一些新兴的训练框架，可以尝试：&#xA;🚀 MFTCoder-atorch: Atorch Codebase for MFT(Multi-task Finetuning)&#xA;模型 使用本项目的训练代码，以及上述训练数据，我们训练并在huggingface, modelscope开源了以下模型。&#xA;模型 HuggingFace链接 魔搭 链接 基座模型 训练数据 Batch Size Seq Length 🔥🔥🔥 CodeFuse-DeepSeek-33B h-link m-link DeepSeek-coder-33B 60万 80 4096 🔥🔥🔥 CodeFuse-Mixtral-8x7B h-link m-link Mixtral-8x7B 60万 80 4096 🔥🔥🔥 CodeFuse-CodeLlama-34B h-link m-link CodeLlama-34b-Python 60万 80 4096 🔥🔥🔥 CodeFuse-CodeLlama-34B-4bits h-link m-link CodeLlama-34b-Python 4096 🔥🔥🔥 CodeFuse-StarCoder-15B h-link m-link StarCoder-15B 60万 80 4096 🔥🔥🔥 CodeFuse-QWen-14B h-link m-link Qwen-14b 110万 256 4096 🔥🔥🔥 CodeFuse-CodeGeex2-6B h-link m-link CodeGeex2-6B 110万 256 4096 数据集 目前本项目主要整理了如下指令数据集，并将其整理成统一的数据格式，这两个指令微调数据集是我们多任务训练中数十个任务中的2个，未来我们会陆续开源更多的代码任务指令微调数据集：</description>
    </item>
    <item>
      <title>Test-Agent</title>
      <link>/zh/docs/test-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/test-agent-zh/</guid>
      <description>Test-Agent Test-Agent</description>
    </item>
    <item>
      <title>Test-Agent: 您的智能测试助理</title>
      <link>/zh/docs/overview/test-agent-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/overview/test-agent-zh/</guid>
      <description>本地Mac M1体验效果 魔搭体验效果 魔搭模型访问链接：ModelScope TestGPT-7B 什么是Test Agent？（Introduction） Test Agent 旨在构建测试领域的“智能体”，融合大模型和质量领域工程化技术，促进质量技术代系升级。我们期望和社区成员一起合作，打造创新的测试领域解决方案，构建24小时在线的测试助理服务，让测试如丝般顺滑。&#xA;本期特性（Features） 模型 本期我们开源了测试领域模型TestGPT-7B。模型以CodeLlama-7B为基座，进行了相关下游任务的微调：&#xA;多语言测试用例生成（Java/Python/Javascript） 一直以来都是学术界和工业界非常关注的领域，近年来不断有新产品或工具孵化出来，如EvoSuite、Randoop、SmartUnit等。然而传统的用例生成存在其难以解决的痛点问题，基于大模型的测试用例生成在测试用例可读性、测试场景完整度、多语言支持方面都优于传统用例生成工具。本次重点支持了多语言测试用例生成，在我们本次开源的版本中首先包含了Java、Python、Javascript的测试用例生成能力，下一版本中逐步开放Go、C++等语言。 测试用例Assert补全 对当前测试用例现状的分析与探查时，我们发现代码仓库中存在一定比例的存量测试用例中未包含Assert。没有Assert的测试用例虽然能够在回归过程中执行通过，却无法发现问题。因此我们拓展了测试用例Assert自动补全这一场景。通过该模型能力，结合一定的工程化配套，可以实现对全库测试用例的批量自动补全，智能提升项目质量水位。 工程框架 本地模型快速发布和体验工程化框架&#xA;ChatBot页面 模型快速启动 私有化部署，本地化的GPT大模型与您的数据和环境进行交互，无数据泄露风险，100%安全 后续我们会持续迭代模型和工程化能力：&#xA;不断加入更多令人激动的测试域应用场景，如领域知识问答、测试场景分析等 支撑面向测试场景的copilot 工程框架开放，如测试领域知识智能embedding、测试通用工具API体系、智能测试Agent等，敬请期待！ 以7B为基础，逐步扩展至13B、34B模型。欢迎关注！ 性能最强的7B测试领域大模型（Model） 目前在TestAgent中，我们默认使用了TestGPT-7B模型。与当前已有开源模型相比，TestGPT-7B模型在用例执行通过率（pass@1）、用例场景覆盖（平均测试场景数）上都处于业界领先水平。 TestGPT-7B模型核心能力的评测结果如下：&#xA;多语言测试用例生成 针对模型支持的三种语言：Java、Python、Javascript，Pass@1评测结果如下： Model Java pass@1 Java Average number of test scenarios Python pass@1 Python Average number of test scenarios Javascript pass@1 Javascript Average number of test scenarios TestGPT-7B 48.6% 4.37 35.67% 3.56 36% 2.76 CodeLlama-13B-Instruct 40.54% 1.08 30.57% 1.65 31.7% 3.</description>
    </item>
    <item>
      <title>VSCode插件</title>
      <link>/docs/codefuse-query-toolchain-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-toolchain-zh/</guid>
      <description>开发插件(VSCode) 安装 从VSCode官方插件市场安装(推荐) 插件地址&#xA;使用VSIX安装包安装 下载插件 手动从 vsix 安装: 或者使用指令直接从终端安装: code --install-extension [扩展vsix文件路径] 环境准备 Sparrow CLI ，参照 3 安装、配置、运行 扩展特性 本扩展提供了以下功能模块:&#xA;COREF AST Viewer Gödel Language Server Gödel Language Runner COREF AST Viewer 以下功能需要在扩展设置中设置相关项后启用。目前仅支持于Java语言&#xA;Java 文件转成树状的 COREF Node Node 与代码位置的相互定位 在Lib API Viewer 查看 Node 的API,Node 复制 Lib API Viewer:查询与复制使用 Gödel Language Server Features 以下功能均需要在设置扩展后启用。不设置相关项的情况下，语法高亮仍然可用。&#xA;错误信息提示 错误信息会随着代码的更新而自动更新。 符号信息提示和补全 包含local变量和全局符号信息的补全提示，关键字等信息会提供对应的使用样例，全局符号信息会提供更详细的内部信息，如包含的成员变量、成员方法、静态方法。&#xA;关键字补全和使用样例提示 local 变量类型信息和符号补全 . 跟随的符号信息和补全 :: 跟随的符号信息和补全 注解使用样例提示 全局符号类型信息 (内部结构，成员方法，静态方法) 跳转到定义 可以通过右键跳转定义或者ctrl/command+left click直接跳转到准确的符号定义位置。</description>
    </item>
    <item>
      <title>本地私有化&amp;大模型接口接入</title>
      <link>/zh/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E6%9C%AC%E5%9C%B0%E7%A7%81%E6%9C%89%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%8F%A3%E6%8E%A5%E5%85%A5/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 本地私有化/大模型接口接入 依托于开源的 LLM 与 Embedding 模型，本项目可实现基于开源模型的离线私有部署。此外，本项目也支持 OpenAI API 的调用。&#xA;本地私有化模型接入 模型地址配置示例，model_config.py配置修改&#xA;# 建议：走huggingface接入，尽量使用chat模型，不要使用base，无法获取正确输出 # 注意：当llm_model_dict和VLLM_MODEL_DICT同时存在时，优先启动VLLM_MODEL_DICT中的模型配置 # llm_model_dict 配置接入示例如下 # 1、若把模型放到 ~/codefuse-chatbot/llm_models 路径下 # 若模型地址如下 model_dir: ~/codefuse-chatbot/llm_models/THUDM/chatglm-6b # 参考配置如下 llm_model_dict = { &amp;#34;chatglm-6b&amp;#34;: { &amp;#34;local_model_path&amp;#34;: &amp;#34;THUDM/chatglm-6b&amp;#34;, &amp;#34;api_base_url&amp;#34;: &amp;#34;http://localhost:8888/v1&amp;#34;, # &amp;#34;name&amp;#34;修改为fastchat服务中的&amp;#34;api_base_url&amp;#34; &amp;#34;api_key&amp;#34;: &amp;#34;EMPTY&amp;#34; } } VLLM_MODEL_DICT = { &amp;#39;chatglm2-6b&amp;#39;: &amp;#34;THUDM/chatglm-6b&amp;#34;, } # or 若模型地址如下 model_dir: ~/codefuse-chatbot/llm_models/chatglm-6b llm_model_dict = { &amp;#34;chatglm-6b&amp;#34;: { &amp;#34;local_model_path&amp;#34;: &amp;#34;chatglm-6b&amp;#34;, &amp;#34;api_base_url&amp;#34;: &amp;#34;http://localhost:8888/v1&amp;#34;, # &amp;#34;name&amp;#34;修改为fastchat服务中的&amp;#34;api_base_url&amp;#34; &amp;#34;api_key&amp;#34;: &amp;#34;EMPTY&amp;#34; } } VLLM_MODEL_DICT = { &amp;#39;chatglm2-6b&amp;#39;: &amp;#34;chatglm-6b&amp;#34;, } # 2、若不想移动相关模型到 ~/codefuse-chatbot/llm_models # 同时删除 `模型路径重置` 以下的相关代码，具体见model_config.</description>
    </item>
    <item>
      <title>查询语言介绍</title>
      <link>/docs/codefuse-query-godellanguage-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-godellanguage-zh/</guid>
      <description>GödelScript 查询语言 目录 GödelScript 基本概念和语法 简介 基本程序构成 基础类型和编译器内建函数 函数 语句 Schema 数据库 Trait Import Query Ungrounded Error: 未赋值/未绑定错误 查询示例 Java Python JavaScript XML Go 查询调试和优化技巧 Schema 传参导致笛卡尔积过大 多层 for 导致笛卡尔积过大 不要滥用@inline 在本机使用查询脚本流程 GödelScript 基本概念和语法 简介 // script fn hello(greeting: string) -&amp;gt; bool { return greeting = &amp;#34;hello world!&amp;#34; } fn main() { output(hello()) } GödelScript 即 Gödel 查询语言。GödelScript 是 CodeQuery 用于查询和数据处理的领域专用语言 (DSL)。GödelScript 使用了类 Rust 的语法，提供了严格的类型检查、方便快捷的类型推导、智能友好的错误提示信息，使用户能够快速上手。&#xA;GödelScript 编译器主要应用场景为：&#xA;面向用户编写简单或复杂查询，提供更便捷的写法，提高编写查询的效率； 提供严格类型检查与类型推导，给予更智能的代码修改提示； 提供严格的 ungrounded(未赋值/未绑定) 检测，避免触发 Soufflé Ungrounded Error； Language Server 以及 IDE Extension 支持。 基本程序构成 程序结构 GödelScript 程序可能包含:</description>
    </item>
    <item>
      <title>概览</title>
      <link>/zh/docs/zh_overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/zh_overview/</guid>
      <description>HuggingFace | 魔搭社区 | 产品主页 Hello World! This is CodeFuse!&#xA;CodeFuse的使命是开发专门设计用于支持整个软件开发生命周期的大型代码语言模型（Code LLMs），涵盖设计、需求、编码、测试、部署、运维等关键阶段。我们致力于打造创新的解决方案，让软件开发者们在研发的过程中如丝般顺滑。&#xA;我们非常有激情去构建创新的解决方案来支持全生命周期AI驱动的软件开发，如上图所示。同时，我们也诚邀志同道合的工程师和研究人员加入这个社区，共同构建和增强CodeFuse。</description>
    </item>
    <item>
      <title>功能特性</title>
      <link>/docs/codefuse-modelcache-feature-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-feature-zh/</guid>
      <description>功能方面，为了解决huggingface网络问题并提升推理速度，增加了embedding本地推理能力。鉴于SqlAlchemy框架存在一些限制，我们对关系数据库交互模块进行了重写，以更灵活地实现数据库操作。在实践中，大型模型产品需要与多个用户和多个模型对接，因此在ModelCache中增加了对多租户的支持，同时也初步兼容了系统指令和多轮会话。&#xA;模块&#xD;功能&#xD;ModelCache&#xD;GPTCache&#xD;基础接口&#xD;数据查询接口&#xD;&amp;#9745; &amp;#9745; 数据写入接口&#xD;&amp;#9745; &amp;#9745; Embedding&#xD;embedding模型配置&#xD;&amp;#9745; &amp;#9745; 大模型embedding层&#xD;&amp;#9745; bert模型长文本处理&#xD;&amp;#9745; Large model invocation&#xD;是否与大模型解耦&#xD;&amp;#9745; embeddingg模型本地加载&#xD;&amp;#9745; 数据隔离&#xD;模型数据隔离&#xD;&amp;#9745; &amp;#9745; 超参数隔离&#xD;数据库&#xD;MySQL&#xD;&amp;#9745; &amp;#9745; Milvus&#xD;&amp;#9745; &amp;#9745; OceanBase&#xD;&amp;#9745; 会话管理&#xD;单轮回话&#xD;&amp;#9745; &amp;#9745; system指令&#xD;&amp;#9745; 多轮回话&#xD;&amp;#9745; 数据管理&#xD;数据持久化&#xD;&amp;#9745; &amp;#9745; 一键清空缓存&#xD;&amp;#9745; 租户管理&#xD;支持多租户（多模型）&#xD;&amp;#9745; milvus多表能力&#xD;&amp;#9745; 其他&#xD;长短对话区分能力&#xD;&amp;#9745; 核心功能 在ModelCache中，沿用了GPTCache的主要思想，包含了一系列核心模块：adapter、embedding、similarity和data_manager。adapter模块主要功能是处理各种任务的业务逻辑，并且能够将embedding、similarity、data_manager等模块串联起来；embedding模块主要负责将文本转换为语义向量表示，它将用户的查询转换为向量形式，并用于后续的召回或存储操作；rank模块用于对召回的向量进行相似度排序和评估；data_manager模块主要用于管理数据库。同时，为了更好的在工业界落地，我们做了架构和功能上的升级，如下：&#xA;架构调整（轻量化集成）：以类redis的缓存模式嵌入到大模型产品中，提供语义缓存能力，不会干扰LLM调用和安全审核等功能，适配所有大模型服务。 多种模型加载方案： 支持加载本地embedding模型，解决huggingface网络连通问题 支持加载多种预训练模型embeding层 数据隔离能力 环境隔离：可依据环境，拉取不同的数据库配置，实现环境隔离（开发、预发、生产） 多租户数据隔离：根据模型动态创建collection，进行数据隔离，用于大模型产品中多个模型/服务数据隔离问题 支持系统指令：采用拼接的方式，解决propmt范式中sys指令问题。 长短文本区分：长文本会给相似评估带来更多挑战，增加了长短文本的区分，可单独配置判断阈值。 milvus性能优化：milvus consistency_level调整为&amp;quot;Session&amp;quot;级别，可以得到更好的性能。 数据管理能力： 一键清空缓存的能力，用于模型升级后的数据管理。 召回hitquery，用于后续的数据分析和模型迭代参考。 异步日志回写能力，用于数据分析和统计 增加model字段和数据统计字段，用于功能拓展。 未来会持续建设的功能：</description>
    </item>
    <item>
      <title>贡献指南</title>
      <link>/zh/contribution/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 非常感谢您对 Codefuse 项目感兴趣，我们非常欢迎您对 Codefuse 项目的各种建议、意见（包括批评）、评论和贡献。&#xA;您对 Codefuse 的各种建议、意见、评论可以直接通过 GitHub 的 Issues 提出。&#xA;参与 Codefuse 项目并为其作出贡献的方法有很多：代码实现、测试编写、流程工具改进、文档完善等等。任何贡献我们都会非常欢迎，并将您加入贡献者列表.&#xA;进一步，有了足够的贡献后，您还可以有机会成为 Codefuse 的 Committer。&#xA;任何问题，您都可以联系我们得到及时解答，联系方式包括微信、Gitter（GitHub提供的即时聊天工具）、邮件等等。&#xA;初次接触 初次来到 Codefuse 社区，您可以：&#xA;关注 Codefuse Github 代码库 加入 Codefuse 相关的微信群 随时提问； 通过以上方式及时了解 Codefuse 项目的开发动态并为您关注的话题发表意见。 贡献方式 这份贡献指南并不仅仅关于编写代码。我们重视并感激在各个领域的帮助。以下是一些您可以贡献的方式&#xA;文档 Issue PR 改进文档 文档是您了解 Codefuse 的最主要的方式，也是我们最需要帮助的地方！&#xA;浏览文档，可以加深您对 Codefuse 的了解，也可以帮助您理解 Codefuse 的功能和技术细节，如果您发现文档有问题，请及时联系我们；&#xA;如果您对改进文档的质量感兴趣，不论是修订一个页面的地址、更正一个链接、以及写一篇更优秀的入门文档，我们都非常欢迎！&#xA;我们的文档大多数是使用 markdown 格式编写的，您可以直接通过在 GitHub 中的 docs/ 中修改并提交文档变更。如果提交代码变更，可以参阅 Pull Request。&#xA;如果发现了一个 Bug 或问题 如果发现了一个 Bug 或问题，您可以直接通过 GitHub 的 Issues 提一个新的 Issue，我们会有人定期处理。详情见Issue模板</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/coagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>快速使用 首先，填写LLM配置 import os, sys&#xD;import openai&#xD;# llm config&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34; 然后设置LLM配置和向量模型配置 from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) 最后选择一个已有场景进行执行 from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message&#xD;# 选择一个已实现得场景进行执行&#xD;# 如果需要做一个数据分析，需要将数据放到某个工作目录，同时指定工作目录（也可使用默认目录）&#xD;import shutil&#xD;source_file = &amp;#39;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv&amp;#39;&#xD;shutil.copy(source_file, JUPYTER_WORK_PATH)&#xD;# 选择一个场景&#xD;phase_name = &amp;#34;baseGroupPhase&amp;#34;&#xD;phase = BasePhase(&#xD;phase_name, embed_config=embed_config, llm_config=llm_config, )&#xD;# round-1 需要通过代码解释器来完成&#xD;query_content = &amp;#34;确认本地是否存在employee_data.</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/docs/codefuse-chatbot-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-chatbot-quickstart-zh/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 🚀 快速使用 如需使用私有化模型部署，请自行安装 nvidia 驱动程序，本项目已在 Python 3.9.18，CUDA 11.7 环境下，Windows、X86 架构的 macOS 系统中完成测试。&#xA;Docker安装、私有化LLM接入及相关启动问题见：快速使用明细&#xA;python 环境准备 推荐采用 conda 对 python 环境进行管理（可选） # 准备 conda 环境 conda create --name devopsgpt python=3.9 conda activate devopsgpt 安装相关依赖 cd codefuse-chatbot pip install -r requirements.txt 基础配置 # 修改服务启动的基础配置 cd configs cp model_config.py.example model_config.py cp server_config.py.example server_config.py # model_config#11~12 若需要使用openai接口，openai接口key os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34; # 可自行替换自己需要的api_base_url os.environ[&amp;#34;API_BASE_URL&amp;#34;] = &amp;#34;https://api.openai.com/v1&amp;#34; # vi model_config#LLM_MODEL 你需要选择的语言模型 LLM_MODEL = &amp;#34;gpt-3.</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/docs/codefuse-query-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-quickstart-zh/</guid>
      <description>安装、配置、运行 硬件和软件要求 硬件：4C8G&#xA;环境要求：java 1.8 和 python3.8 以上执行环境, 请保证 java python 可执行环境&#xA;Sparrow 安装步骤和指导 CodeFuse-Query 下载包是一个 zip 存档，其中包含工具、脚本和各种特定于 CodeFuse-Query 的文件。如果您没有 CodeFuse-Query 许可证，那么下载此存档即表示您同意 CodeFuse-Query 条款和条件。 目前仅支持 mac，linux 系统下使用 CodeFuse-Query，下载地址为:（目前仅给出示例，开源后给出正式下载地址） mac: CodeFuse-Query 2.0.0 linux: CodeFuse-Query 2.0.0 您应该始终使用 CodeFuse-Query 捆绑包，确保版本兼容性 Tips： mac系统下直接下载软件包会提示需要验证开发者 可在安全性设置中进行修改验证 点击仍然允许&#xA;详细步骤可参照：Mac 官方文档: 如何在 Mac 上安全地打开 App&#xA;或使用xattr -d com.apple.quarantine命令，删除 CodeFuse-Query 被 macOS 赋予的外部属性&#xA;xattr -d com.apple.quarantine是一个命令行指令，用于删除文件的 com.apple.quarantine 扩展属性。该扩展属性是 macOS 系统用来标记从外部来源下载的文件或应用程序的属性，以确保安全性。&#xA;xattr -d com.apple.quarantine path/to/file 配置和初始化 CodeFuse-Query 开发环境 解压缩：命令行解压或者直接点一下解压缩即可&#xA;需要具备 java8 和 python3.</description>
    </item>
    <item>
      <title>快速开始</title>
      <link>/zh/muagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/</guid>
      <description>Quick Start 完整示例见，examples/muagent_examples&#xA;首先，准备相关配置信息 import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; 然后，设置LLM配置和Embedding模型配置 from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.phase import BasePhase&#xD;from muagent.connector.schema import Message&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) 最后选择一个已有场景进行执行 # if you want to analyze a data.</description>
    </item>
    <item>
      <title>快速使用</title>
      <link>/zh/docs/codefuse-evalution-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-evalution-quickstart-zh/</guid>
      <description>推理环境： CodeFuse-13B: python 3.8及以上版本，pytorch 2.0及以上版本，transformers 4.24.0及以上版本，CUDA 11.4及以上；&#xA;CodeFuse-CodeLlama-34B: python 3.8及以上版本，pytorch2.0及以上版本，transformers==4.32.0 ，Sentencepiece，CUDA 11.4及以上。&#xA;评测执行环境 评测生成的代码需要使用多种语言编译、运行。我们使用的各编程语言依赖及所用包的版本如下：&#xA;依赖 版本 Python 3.10.9 JDK 18.0.2.1 Node.js 16.14.0 js-md5 0.7.3 C++ 11 g++ 7.5.0 Boost 1.75.0 OpenSSL 3.0.0 go 1.18.4 cargo 1.71.1 为了省去使用者配置这些语言环境的麻烦，我们构建了一个Docker镜像，并在其中配置了所需要的环境，你可以按照下面的指令拉取使用&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/codefuse/codefuseeval:latest 如果您熟悉Dockerfile，也可以从codefuseEval/docker/Dockerfile构建镜像，或者修改之以定制自己的配置：&#xA;cd codefuseEval/docker docker build [OPTIONS] . 获取镜像后，使用如下命令创建容器：&#xA;docker run -it --gpus all --mount type=bind,source=&amp;lt;LOCAL PATH&amp;gt;,target=&amp;lt;PATH IN CONTAINER&amp;gt; [OPTIONS] &amp;lt;IMAGE NAME:TAG&amp;gt; 检查推理结果指令 我们提供脚本来检查所提供代码 LLM 的结果。请使用以下脚本检查相应的推理结果。&#xA;bash codefuseEval/script/check_reference.sh codefuseEval/result/CodeFuse-CodeLlama-34B/humaneval_result_python.jsonl humaneval_python&#xD;bash codefuseEval/script/check_reference.</description>
    </item>
    <item>
      <title>快速使用</title>
      <link>/zh/docs/codefuse-mft-vlm/%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-mft-vlm/%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8/</guid>
      <description>Contents Install Datasets Multimodal Alignment Visual Instruction Tuning Evaluation Install 请执行 sh init_env.sh&#xA;Datasets 使用了以下数据集训练模型:&#xA;数据集 任务种类 样本量 synthdog-en OCR 800,000 synthdog-zh OCR 800,000 cc3m(downsampled) Image Caption 600,000 cc3m(downsampled) Image Caption 600,000 SBU Image Caption 850,000 Visual Genome VQA (Downsampled) Visual Question Answer(VQA) 500,000 Visual Genome Region descriptions (Downsampled) Reference Grouding 500,000 Visual Genome objects (Downsampled) Grounded Caption 500,000 OCR VQA (Downsampled) OCR and VQA 500,000 请到各个数据集的官网上下载这些数据。&#xA;Multimodal Alignment 请执行 sh scripts/pretrain.</description>
    </item>
    <item>
      <title>快速使用</title>
      <link>/docs/codefuse-devops-model-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model-quickstart-zh/</guid>
      <description>依赖安装 需要先 PIP 安装一下 Github 地址下的 requirement.txt 中的包，可以参考一下代码 pip install -r requirements.txt&#xA;模型下载 模型下载相关信息如下： 🤗 Huggingface 地址&#xA;- 基座模型 对齐模型 7B DevOps-Model-7B-Base DevOps-Model-7B-Chat 14B DevOps-Model-14B-Base DevOps-Model-14B-Chat 🤖 ModelScope 地址&#xA;- 基座模型 对齐模型 7B DevOps-Model-7B-Base DevOps-Model-7B-Chat 14B DevOps-Model-14B-Base DevOps-Model-14B-Chat 找到自己想要下载的 Chat 模型版本，当前提供了 7B 和 14B 的模型&#xA;模型使用 根据以下代码来和 Chat 模型进行交互&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xD;from transformers.generation import GenerationConfig&#xD;tokenizer = AutoTokenizer.from_pretrained(&amp;#34;path_to_DevOps-Model-Chat&amp;#34;, trust_remote_code=True)&#xD;model = AutoModelForCausalLM.from_pretrained(&amp;#34;path_to_DevOps-Model-Chat&amp;#34;, device_map=&amp;#34;auto&amp;#34;, trust_remote_code=True, bf16=True).eval()&#xD;# 指定 generation_config&#xD;model.</description>
    </item>
    <item>
      <title>快速使用</title>
      <link>/docs/test-agent-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/test-agent-quickstart-zh/</guid>
      <description>快速使用（QuickStart） 前置准备 模型下载 您可在modelscope或huggingface上获取到模型的详细信息并下载模型文件。 需要注意的是： 1）如果您通过modelscope下载模型，下载方式可参考：下载说明； 2）如果您通过huggingface下载模型，请确保您可以正常访问huggingface。&#xA;环境安装 python&amp;gt;=3.8 transformers==4.33.2 git clone https://github.com/codefuse-ai/Test-Agent cd Test-Agent pip install -r requirements.txt 在开始运行TestGPT-7B模型之前，请确保你的执行环境拥有大约14GB的显存。&#xA;启动服务 项目提供了网页端快速搭建UI的能力能够更直观的展示模型交互和效果，我们可以使用简单的几个命令把前端页面唤醒并实时调用模型能力。在项目目录下，依次启动以下服务：&#xA;1.启动controller python3 -m chat.server.controller&#xA;2.启动模型worker python3 -m chat.server.model_worker &amp;ndash;model-path models/TestGPT-7B &amp;ndash;device mps&#xA;（models/TestGPT-7B 为实际模型文件路径）&#xA;对于启动方式，可以按需选择以下几种配置选项：&#xA;&amp;ndash;device mps 用于在Mac电脑上开启GPU加速的选项（Apple Silicon或AMD GPUs）； &amp;ndash;device xpu 用于在Intel XPU上开启加速的选项（Intel Data Center and Arc A-Series GPUs）； 需安装Intel Extension for PyTorch 设置OneAPI环境变量：source /opt/intel/oneapi/setvars.sh &amp;ndash;device npu 用于在华为AI处理器上开启加速的选项； 需安装Ascend PyTorch Adapter 设置CANN环境变量：source /usr/local/Ascend/ascend-toolkit/set_env.sh &amp;ndash;device cpu 单独使用CPU运行的选项，不需要GPU； &amp;ndash;num-gpus 2 指定并发gpu运行的选项。 启动web服务 python3 -m chat.</description>
    </item>
    <item>
      <title>评测</title>
      <link>/zh/docs/codefuse-devops-eval-quickstart-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/codefuse-devops-eval-quickstart-zh/</guid>
      <description>🚀 如何进行测试 如果需要在自己的 HuggingFace 格式的模型上进行测试的话，总的步骤分为如下几步:&#xA;编写 Model 的 loader 函数 编写 Model 的 context_builder 函数 注册模型到配置文件中 执行测试脚本 如果模型在加载进来后不需要特殊的处理，而且输入也不需要转换为特定的格式（e.g. chatml 格式或者其他的 human-bot 格式），请直接跳转到第四步直接发起测试。 1. 编写 loader 函数 模型加载时还需要做一些额外的处理（e.g. tokenizer 调整），需要继承 ModelAndTokenizerLoader 类来覆写对应的 load_model 和 load_tokenizer 函数， 如下所示：&#xA;class QwenModelAndTokenizerLoader(ModelAndTokenizerLoader): def __init__(self): super().__init__() pass @override def load_model(self, model_path: str): # Implementation of the method pass @override def load_tokenizer(self, model_path: str): # Implementation of the method pass 2. 编写 Model 的 context_builder 函数 如果输入需要转换为特定的格式（e.</description>
    </item>
    <item>
      <title>启动明细</title>
      <link>/zh/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E5%90%AF%E5%8A%A8%E6%98%8E%E7%BB%86/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp 如需使用私有化模型部署，请自行安装 nvidia 驱动程序。。&#xA;python 环境准备 推荐采用 conda 对 python 环境进行管理（可选） # 准备 conda 环境 conda create --name devopsgpt python=3.9 conda activate devopsgpt 安装相关依赖 cd codefuse-chatbot # python=3.9，notebook用最新即可，python=3.8用notebook=6.5.6 pip install -r requirements.txt 沙盒环境准备 windows Docker 安装： Docker Desktop for Windows 支持 64 位版本的 Windows 10 Pro，且必须开启 Hyper-V（若版本为 v1903 及以上则无需开启 Hyper-V），或者 64 位版本的 Windows 10 Home v1903 及以上版本。&#xA;【全面详细】Windows10 Docker安装详细教程 Docker 从入门到实践 Docker Desktop requires the Server service to be enabled 处理 安装wsl或者等报错提示 Linux Docker 安装： Linux 安装相对比较简单，请自行 baidu/google 相关安装</description>
    </item>
    <item>
      <title>如何提交Issue</title>
      <link>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4issue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4issue/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Issue Type Issue分为三种类型&#xA;Bug: 代码或者执行示例存在bug或缺少依赖导致无法正确执行 Documentation：文档表述存在争议、文档内容与代码不一致等 Feature：在当前代码基础继续演进的新功能 Issue Template Issue: Bug Template 提交Issue前的确认清单 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息&#xA;我搜索了Codefuse相关的所有文档。 我使用GitHub搜索寻找了一个类似的问题，但没有找到。 我为这个问题添加了一个非常描述性的标题。 系统信息 确认系统，如 mac -xx 、windwos-xx、linux-xx&#xA;代码版本 确认代码版本或者分支，master、release等&#xA;问题描述 描述您碰到的问题，想要实现的事情、或代码执行Bug&#xA;代码示例 附上你的执行代码和相关配置，以便能够快速介入进行复现&#xA;报错信息、日志 执行上述代码示例后的报错日志和相关信息&#xA;相关依赖的模块 以chatbot项目为例&#xA;connector codechat sandbox &amp;hellip; Issue: Documentation Template Issue with current documentation: 请帮忙指出当前文档中的问题、错别字或者令人困惑的地方&#xA;Idea or request for content 您觉得合理的文档表述方式应该是什么样的&#xA;Issue: Feature Template 提交Issue前的确认清单 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息&#xA;我搜索了Codefuse相关的所有文档。 我使用GitHub Issue搜索寻找了一个类似的问题，但没有找到。 我为这个问题添加了一个非常描述性的标题。 功能描述 描述这个功能作何用途</description>
    </item>
    <item>
      <title>如何提交PR</title>
      <link>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4pr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4pr/</guid>
      <description>中文&amp;nbsp ｜ &amp;nbspEnglish&amp;nbsp Contribution Pre-Checklist 要先确认是否查看 document、issue、discussion(github 功能) 等公开的文档信息 找到你想处理的GitHub问题。如果不存在，创建一个问题或草案PR，并请求维护者进行检查。 检查相关的、相似的或重复的拉取请求。 创建一个草案拉取请求。 完成PR模板中的描述。 链接任何被你的PR解决的GitHub问题。 Description PR的描述信息，用简洁的语言表达PR完成的事情，具体规范见Commit 格式规范&#xA;Related Issue #xx if has&#xA;Test Code with Result 请提供相关的测试代码如果有必要的话&#xA;Commit 格式规范 Commit 分为“标题”和“内容”。原则上标题全部小写。内容首字母大写。&#xA;标题 commit message的标题：[&amp;lt;type&amp;gt;](&amp;lt;scope&amp;gt;) &amp;lt;subject&amp;gt; (#pr)&#xA;type 可选值 本次提交的类型，限定在以下类型（全小写）&#xA;fix：bug修复 feature：新增功能 feature-wip：开发中的功能，比如某功能的部分代码。 improvement：原有功能的优化和改进 style：代码风格调整 typo：代码或文档勘误 refactor：代码重构（不涉及功能变动） performance/optimize：性能优化 test：单元测试的添加或修复 deps：第三方依赖库的修改 community：社区相关的修改，如修改 Github Issue 模板等。 几点说明：&#xA;如在一次提交中出现多种类型，需增加多个类型。 如代码重构带来了性能提升，可以同时添加 [refactor][optimize] 不得出现如上所列类型之外的其他类型。如有必要，需要将新增类型添加到这个文档中。&#xA;scope 可选值 本次提交涉及的模块范围。因为功能模块繁多，在此仅罗列部分，后续根据需求不断完善。 以 chatbot的框架为例&#xA;connector codechat sandbox &amp;hellip; 几点说明：&#xA;尽量使用列表中已存在的选项。如需添加，请及时更新本文档。</description>
    </item>
    <item>
      <title>数据</title>
      <link>/zh/docs/%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/docs/%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D/</guid>
      <description>⏬ 数据 下载 方法一：下载zip压缩文件（你也可以直接用浏览器打开下面的链接）：&#xA;wget https://huggingface.co/datasets/codefuse-admin/devopseval-exam/resolve/main/devopseval-exam.zip 然后可以使用 pandas加载数据：&#xA;import os import pandas as pd File_Dir=&amp;#34;devopseval-exam&amp;#34; test_df=pd.read_csv(os.path.join(File_Dir,&amp;#34;test&amp;#34;,&amp;#34;UnitTesting.csv&amp;#34;)) 方法二：使用Hugging Face datasets直接加载数据集。示例如下：&#xA;from datasets import load_dataset dataset=load_dataset(r&amp;#34;DevOps-Eval/devopseval-exam&amp;#34;,name=&amp;#34;UnitTesting&amp;#34;) print(dataset[&amp;#39;val&amp;#39;][0]) # {&amp;#34;id&amp;#34;: 1, &amp;#34;question&amp;#34;: &amp;#34;单元测试应该覆盖以下哪些方面？&amp;#34;, &amp;#34;A&amp;#34;: &amp;#34;正常路径&amp;#34;, &amp;#34;B&amp;#34;: &amp;#34;异常路径&amp;#34;, &amp;#34;C&amp;#34;: &amp;#34;边界值条件&amp;#34;，&amp;#34;D&amp;#34;: 所有以上，&amp;#34;answer&amp;#34;: &amp;#34;D&amp;#34;, &amp;#34;explanation&amp;#34;: &amp;#34;&amp;#34;} ``` 方法三：使用modelscope下载相关所有数据。示例如下：&#xA;from modelscope.msdatasets import MsDataset MsDataset.clone_meta(dataset_work_dir=&amp;#39;./xxx&amp;#39;, dataset_id=&amp;#39;codefuse-ai/devopseval-exam&amp;#39;)``` 👀 说明 为了方便使用，我们已经整理出了 55 个细分类别以及它们的中英文名称。具体细节请查看 category_mapping.json 。格式如下：&#xA;{ &amp;#34;UnitTesting.csv&amp;#34;: [ &amp;#34;unit testing&amp;#34;, &amp;#34;单元测试&amp;#34;, {&amp;#34;dev&amp;#34;: 5, &amp;#34;test&amp;#34;: 32} &amp;#34;TEST&amp;#34; ], ... &amp;#34;file_name&amp;#34;:[ &amp;#34;英文名称&amp;#34;, &amp;#34;中文名称&amp;#34;, &amp;#34;样本数量&amp;#34;, &amp;#34;类别(PLAN,CODE,BUILD,TEST,RELEASE,DEPOLY,OPERATE,MONITOR八选一)&amp;#34; ] } 每个细分类别由两个部分组成：dev 和 test。每个细分类别的 dev 集包含五个示范实例以及为 few-shot 评估提供的解释。而 test 集则用于模型评估，并且test数据已包含准确标签。</description>
    </item>
    <item>
      <title>训练解析</title>
      <link>/docs/codefuse-devops-model-train-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model-train-zh/</guid>
      <description>训练流程 根据查阅文献可知，大部分领域模型都是在对话模型的基础上，通过SFT微调来进行知识注入。而SFT微调所需要QA预料基本都来自于ChatGPT生成。然而，该方案可能存在QA语料无法完全覆盖领域知识的情况。 因此，DevOps-Model采用的是预训练加训 + SFT微调的方案，如图2.1所示。我们认为针对领域大模型，预训练的加训是必要的，因为其可以将领域内的一些知识在预训练阶段注入到大模型，如果这些知识在通用大模型预训练时没有出现过，那会让大模型学习到新的知识；如果出现过，就可以让大模型进一步加深印象。第二步则是大模型对齐，目的是让大模型可以根据问题来回答最合适的内容。&#xA;训练数据 数据收集 模型的定位是中文 DevOps 领域大模型，因此收集与中文DevOps相关的预训练数据和QA数据。&#xA;预训练数据主要来自互联网技术博客、技术文档、技术书籍等，最终收集到了 50G+ 的预训练语料数据； 针对 QA 数据，我们的目的是想让模型不但对齐到通用的问答能力，而且针对 DevOps 领域也可以学会如何更好的回答问题，因此不但收集了通用领域的单轮和多轮对话数据，还针对 DevOps 领域，通过爬取和 ChatGPT 生成的方式产出了属于 DevOps 领域的问答数据。最终我们精心筛选了约 200K 的 QA 数据进行 SFT微调训练，具体数据量如下表所示。 数据类型 数据量级 通用单轮 QA 50K 通用多轮 QA 20K DevOps 领域 QA 130K 数据筛选 由于预训练数据大部分是从互联网上收集的数据，质量会参差不齐，而大模型训练中数据是最重要的一环，我们建立了如上图所示的清洗 Pipeline，来针对收集到的数据进行质量的全面过滤。&#xA;首先，由专家经验和人工筛选，总结出来了一批文档级别的 Heuristic 过滤规则，这一步主要用来过滤掉那些质量非常差的文档； 然后，即便是一篇质量稍差的文章中，也有可能还是含有一些有价值的领域知识，我们也需要尽可能的进行收集。此处，我们对文章进行段落拆分，将文章拆分成一个个段落； 然后，我们将拆分后的段落会再次通过步骤1进行过滤，便得到了一批经过规则过滤后的段落； 然后，我们摘取了其中 1000 个段落，由经验丰富的专业开发人员来进行打标，获得高质量的打标数据； 最后，我们根据打标后的结果来训练了一个打分模型来针对段落进行质量的打分，段落的向量模型选用了预训练好的中文版本的 Sentence-Bert，打分算法选用了逻辑回归，为了避免打分模型的误差，会再通过帕累托分布来根据段落的质量打分进行采样来决定要不要过滤这个段落。 经过这个 Pipeline 后，我们最终沉淀下 15G 左右的数据来进行大模型的预训练加训。 </description>
    </item>
    <item>
      <title>用户案例</title>
      <link>/docs/codefuse-query-usercase-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-usercase-zh/</guid>
      <description>使用场景 查询代码特征 小开发同学想知道 Repo A 里面使用了哪些 String 型的变量，所以他写了一个 Gödel 如下，交给 CodeFuse-Query 系统给他返回了结果。&#xA;// script use coref::java::* fn out(var: string) -&amp;gt; bool { for(v in Variable(JavaDB::load(&amp;#34;coref_java_src.db&amp;#34;))) { if (v.getType().getName() = &amp;#34;String&amp;#34; &amp;amp;&amp;amp; var = v.getName()) { return true } } } fn main() { output(out()) } 类似需求：查询：类，函数，变量，返回值，调用图，类继承等等。&#xA;代码规则检查器 小 TL 同学发现团队总是写出很多类似的 Bug A，他想针对 Bug A 制定一个代码规则和其检查器，并在 CodeReview 阶段做个卡点。小 TL 通过在 CodeFuse-Query 平台上面编写了一段分析 Query，在平台上面测试符合要求，把这段分析 Query 固化下来作为一个代码规则，并上线到了 CodeReview/CI 阶段。从此这个 Bug 再也没发生过了。 类似需求：编写静态缺陷扫描规则进行代码风险拦截。&#xA;获取统计数据 小研究发现传统的代码复杂度指标很难准确地衡量代码的复杂情况，通过学习国际先进经验加上自我灵光一闪，设计了一套复杂度指标和算法。通过 Gödel 实现出来以后，发现不怎么优化就已经性能非常高了，很快就应用到了 10 几种语言，11+万个仓库当中去了。马上就对代码仓库整体的复杂度有了深入的了解。相比较以前需要自己解析代码，分析语法树，对接系统，不知道方便了多少。 类似需求：代码统计，代码度量，算法设计，学术研究。</description>
    </item>
    <item>
      <title>致谢</title>
      <link>/zh/contribution/%E8%87%B4%E8%B0%A2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/contribution/%E8%87%B4%E8%B0%A2/</guid>
      <description>CodeFuse-ai 文档主页基于docura构建！&#xA;ChatBot 项目基于langchain-chatchat和codebox-api!&#xA;&amp;hellip;&amp;hellip;&#xA;在此深深感谢他们的开源贡献！</description>
    </item>
    <item>
      <title>自定义 Retrieval 接入</title>
      <link>/zh/muagent/custom-retrieval-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-retrieval-zh/</guid>
      <description>基本介绍 Doc Retrieval 文档向量数据库是当前最主流的知识库构建方法，使用Text Embedding 模型对文档进行向量化并在向量数据库中存储。未来我们也会去支持基于知识图谱查询以及通过大模型自动抽取实体和关系的方式，来挖掘数据中多种复杂关系。&#xA;Code Retrieval LLM在代码生成、修复以及组件理解的任务上，会面临代码训练数据滞后、无法感知代码上下文依赖结构。以及在开发的过程中，对现有代码库和依赖包的理解、检索相关代码、查询元信息等会占用较长的时间。于是我们希望通过代码结构分析和代码检索生成来，以及为LLM提供知识体系外的代码。&#xA;Search Retrieval 除了现成的文档和代码知识库以及之外，在日常中实践中会去浏览大量网页内容获取更多的知识，帮助我们理解新兴的场景、业务、技术等，于是我们接入了duckduckgosearch这款开源的搜索工具，能够为LLM提供知识储备以外的内容。&#xA;Rertrieval 结构 class IMRertrieval:&#xD;def __init__(self,):&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;init your personal attributes&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;pass&#xD;def run(self, ):&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;execute interface, and can use init&amp;#39; attributes&#xD;&amp;#39;&amp;#39;&amp;#39;&#xD;pass&#xD;class BaseDocRetrieval(IMRertrieval):&#xD;def __init__(self, knowledge_base_name: str, search_top=5, score_threshold=1.0, embed_config: EmbedConfig=EmbedConfig(), kb_root_path: str=KB_ROOT_PATH):&#xD;self.knowledge_base_name = knowledge_base_name&#xD;self.search_top = search_top&#xD;self.score_threshold = score_threshold&#xD;self.embed_config = embed_config&#xD;self.kb_root_path = kb_root_path&#xD;def run(self, query: str, search_top=None, score_threshold=None, ):&#xD;docs = DocRetrieval.</description>
    </item>
    <item>
      <title>自定义 Tool 接入</title>
      <link>/zh/muagent/custom-tool-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/zh/muagent/custom-tool-zh/</guid>
      <description>基本介绍 在MuAgent中也支持Agent完成Tool的注册，通过Python注册模板BaseToolModel类，编写&#xA;Tool_nam Tool_descriptio ToolInputArgs ToolOutputArgs run 等相关属性和方法即可实现工具的快速接入，同时支持langchain Tool接口的直接使用。 例如像上述 XXRetrieval 的功能也可以注册为Tool，最终由LLM执行调用。&#xA;BaseTool 结构 from langchain.agents import Tool&#xD;from pydantic import BaseModel, Field&#xD;from typing import List, Dict&#xD;import json&#xD;class BaseToolModel:&#xD;name = &amp;#34;BaseToolModel&amp;#34;&#xD;description = &amp;#34;Tool Description&amp;#34;&#xD;class ToolInputArgs(BaseModel):&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;Input for MoveFileTool.&#xD;Tips:&#xD;default control Required, e.g. key1 is not Required/key2 is Required&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;key1: str = Field(default=None, description=&amp;#34;hello world!&amp;#34;)&#xD;key2: str = Field(..., description=&amp;#34;hello world!!&amp;#34;)&#xD;class ToolOutputArgs(BaseModel):&#xD;&amp;#34;&amp;#34;&amp;#34;&#xD;Input for MoveFileTool.</description>
    </item>
    <item>
      <title>最佳配置</title>
      <link>/docs/codefuse-modelcache-config-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-config-zh/</guid>
      <description>环境依赖 python版本: 3.8及以上 依赖包安装： pip install requirements.txt 服务启动 在启动服务前，应该进行如下环境配置： 安装关系数据库 mysql， 导入sql创建数据表，sql文件: reference_doc/create_table.sql 安装向量数据库milvus 在配置文件中添加数据库访问信息，配置文件为： modelcache/config/milvus_config.ini modelcache/config/mysql_config.ini 离线模型bin文件下载， 参考地址：https://huggingface.co/shibing624/text2vec-base-chinese/tree/main，并将下载的bin文件，放到 model/text2vec-base-chinese 文件夹中 通过flask4modelcache.py脚本启动后端服务。 </description>
    </item>
    <item>
      <title>最佳配置</title>
      <link>/docs/codefuse-modelcache-release-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-release-zh/</guid>
      <description> 时间 功能 版本号 20230430 完成GPTCache调研，开源流程在OpenAI接口上跑通，单节点形式 无 20230509 1、完成技术选型及上下游交互方案&#xA;2、重新开发数据库模块，替换SQLalchemy框架&#xA;3、重构llm_handler模块，兼容codegpt，适配codegpt模型参数 V0.1.0 20230519 1、根据环境动态选择codegpt服务模式&#xA;2、模型本地加载能力，以及预加载能力&#xA;3、增加本地路径依据环境动态加载能力 V0.1.1 20230522 1、架构优化，调整为类redis结构，解藕大模型调用&#xA;2、关系数据库由sqlite切换至OceanBase&#xA;3、向量数据库由faiss切换至milvus&#xA;4、模型数据隔离能力&#xA;5、增加核心模块adapter_query、adapter_insert V0.2.0 20230531 1、线上环境上线，动态感知能力&#xA;2、embedding模型评测及选型&#xA;3、增加预发环境及数据隔离能力&#xA;4、增加原始query字段透出能力 V0.2.1 20230607 1、优化关系数据库访问性能&#xA;2、优化环境和模型隔离能力 V0.2.2 20230630 1、在modelCache中增加大模型embedding层适配模块&#xA;2、增加采纳率统计能力 V0.2.3 20230730 1、增加缓存统计功能&#xA;2、增加数据删除功能接口&#xA;3、缓存一键清空能力上线&#xA;4、多轮会话能力研发，支持system指令和多轮对话 v0.3.0 20230830 1、增加异步处理能力，性能提升超20%&#xA;2、架构变更，解藕embedding推理和业务处理逻辑&#xA;3、黑名单过滤功能 V0.3.1 </description>
    </item>
  </channel>
</rss>
