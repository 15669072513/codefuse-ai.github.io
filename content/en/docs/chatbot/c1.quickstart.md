---
title: QuickStart
slug: QuickStart
description: ‰ªãÁªç‰∏ªË¶ÅÂäüËÉΩ
url: "docs/codefuse-chatbot-quickstart"
aliases:
- "/docs/codefuse-chatbot-quickstart"
---

<p align="left">
    <a href="/docs/quickstart-zh">‰∏≠Êñá</a>&nbsp ÔΩú &nbsp<a>English&nbsp </a>
</p>

## üöÄ Quick Start

To deploy private models, please install the NVIDIA driver by yourself. 
This project has been tested on Python 3.9.18 and CUDA 11.7 environments, as well as on Windows and macOS systems with x86 architecture.
For Docker installation, private LLM access, and related startup issues, see: [Start-detail...](/docs/start-detail)

### Preparation of Python environment

- It is recommended to use conda to manage the python environment (optional)
```bash
# Prepare conda environment
conda create --name Codefusegpt python=3.9
conda activate Codefusegpt
```

- Install related dependencies
```bash
cd Codefuse-ChatBot
pip install -r requirements.txt
```

### Basic Configuration

```bash
# Modify the basic configuration for service startup
cd configs
cp model_config.py.example model_config.py
cp server_config.py.example server_config.py

# model_config#11~12 If you need to use the OpenAI interface, the OpenAI interface key
os.environ["OPENAI_API_KEY"] = "sk-xxx"
# Replace with the api_base_url you need
os.environ["API_BASE_URL"] = "https://api.openai.com/v1"

# vi model_config#LLM_MODEL The language model you need to choose
LLM_MODEL = "gpt-3.5-turbo"
LLM_MODELs = ["gpt-3.5-turbo"]

# vi model_config#EMBEDDING_MODEL The private vector model you need to choose
EMBEDDING_ENGINE = 'model'
EMBEDDING_MODEL = "text2vec-base"

# Example of vector model access, modify model_config#embedding_model_dict
# If the model directory is:
model_dir: ~/codefuse-chatbot/embedding_models/shibing624/text2vec-base-chinese
# Configure as follows
"text2vec-base": "shibing624/text2vec-base-chinese"


# vi server_config#8~14, It's recommended to use a container to start the service to prevent environment conflicts when installing other dependencies using the codeInterpreter feature
DOCKER_SERVICE = True
# Whether to use a container sandbox
SANDBOX_DO_REMOTE = True
```

### Start the Service

By default, only webui related services are started, and fastchat is not started (optional).
```bash
# If you need to support the codellama-34b-int4 model, you need to patch fastchat
# cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
# Modify examples/llm_api.py#258 to kwargs={"gptq_wbits": 4},

# Start llm-service (optional)
python examples/llm_api.py
```

For more LLM access methods, see [Details...](/docs/fastchat)
```bash
# After completing the server_config.py configuration, you can start with one click
cd examples
python start.py
```