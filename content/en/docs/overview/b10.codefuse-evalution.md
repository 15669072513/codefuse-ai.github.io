---
title: "CodeFuseEval: Multi-tasking Evaluation Benchmark for Code Large Language Model"
description: 介绍主要功能
url: "/docs/codefuse-evalution"
aliases:
- "/docs/codefuse-evalution"
---


# CodeFuseEval: Multi-tasking Evaluation Benchmark for Code Large Language Model

<div align="center">
  <p>
    <a href="/docs/codefuse-evalution-zh" target="_blank">简体中文</a>｜
    <a href="https://modelscope.cn/datasets/codefuse-ai/CodeFuseEval/summary" target="_blank">CodeFuseEval on ModelScope</a>｜
    <a href="https://huggingface.co/datasets/codefuse-ai/CodeFuseEval" target="_blank">CodeFuseEval on Hugging Face</a>
  </p>
  
</div>

CodeFuseEval is a Code Generation benchmark that combines the multi-tasking scenarios of CodeFuse Model with the benchmarks of HumanEval-x and MBPP. This benchmark is designed to evaluate the performance of models in various multi-tasking tasks, including code completion, code generation from natural language, test case generation, cross-language code translation, and code generation from Chinese commands, among others.Continuously open, stay tuned !

<p>
    <img src="/images/codefuse-evalution/EnglishIntroduction.png" alt="English Introduction"/>
</p>
