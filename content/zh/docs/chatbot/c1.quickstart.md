---
title: 快速开始
slug: 快速开始
description: 介绍主要功能
url: "docs/快速开始"
aliases:
- "/docs/快速开始"
- "/docs/quickstart-zh"
---

<p align="left">
    <a>中文</a>&nbsp ｜ &nbsp<a href="/docs/quickstart">English&nbsp </a>
</p>


## 🚀 快速使用

如需使用私有化模型部署，请自行安装 nvidia 驱动程序，本项目已在 Python 3.9.18，CUDA 11.7 环境下，Windows、X86 架构的 macOS 系统中完成测试。

Docker安装、私有化LLM接入及相关启动问题见：[快速使用明细](/docs/start-detail-zh)

### python 环境准备

- 推荐采用 conda 对 python 环境进行管理（可选）
```bash
# 准备 conda 环境
conda create --name devopsgpt python=3.9
conda activate devopsgpt
```

- 安装相关依赖
```bash
cd codefuse-chatbot
pip install -r requirements.txt
```

### 基础配置

```bash
# 修改服务启动的基础配置
cd configs
cp model_config.py.example model_config.py
cp server_config.py.example server_config.py

# model_config#11~12 若需要使用openai接口，openai接口key
os.environ["OPENAI_API_KEY"] = "sk-xxx"
# 可自行替换自己需要的api_base_url
os.environ["API_BASE_URL"] = "https://api.openai.com/v1"

# vi model_config#LLM_MODEL 你需要选择的语言模型
LLM_MODEL = "gpt-3.5-turbo"
LLM_MODELs = ["gpt-3.5-turbo"]

# vi model_config#EMBEDDING_MODEL 你需要选择的私有化向量模型
EMBEDDING_ENGINE = 'model'
EMBEDDING_MODEL = "text2vec-base"

# 向量模型接入示例，修改 model_config#embedding_model_dict
# 若模型地址为：
model_dir: ~/codefuse-chatbot/embedding_models/shibing624/text2vec-base-chinese
# 配置如下
"text2vec-base": "shibing624/text2vec-base-chinese"

# vi server_config#8~14, 推荐采用容器启动服务，避免使用codeInterpreter功能时安装其它依赖导致环境冲突
DOCKER_SERVICE = True
# 是否采用容器沙箱
SANDBOX_DO_REMOTE = True
```

### 启动服务

默认只启动webui相关服务，未启动fastchat（可选）。
```bash
# 若需要支撑codellama-34b-int4模型，需要给fastchat打一个补丁
# cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
# examples/llm_api.py#258 修改为 kwargs={"gptq_wbits": 4},

# start llm-service（可选）
python examples/llm_api.py
```
更多LLM接入方法见[详情...](/docs/fastchat-zh)
<br>

```bash
# 完成server_config.py配置后，可一键启动
cd examples
python start.py
```
